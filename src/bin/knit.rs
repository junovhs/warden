use anyhow::Result;
use clap::{Parser, ValueEnum};
use colored::Colorize;
use std::fmt::Write;
use std::fs;
use std::path::Path;

use warden_core::config::{Config, GitMode};
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::tokens::Tokenizer;

#[derive(Debug, Clone, ValueEnum)]
enum OutputFormat {
    Text,
    Xml,
}

#[derive(Parser)]
#[command(name = "knit")]
#[command(about = "Stitches atomic files into a single context file.")]
#[allow(clippy::struct_excessive_bools)]
struct Cli {
    #[arg(long, short)]
    stdout: bool,
    #[arg(long, short)]
    verbose: bool,
    #[arg(long)]
    git_only: bool,
    #[arg(long)]
    no_git: bool,
    #[arg(long)]
    code_only: bool,
    /// Output format (Text for standard, Xml for Claude/LLMs)
    #[arg(long, value_enum, default_value_t = OutputFormat::Text)]
    format: OutputFormat,
}

fn main() -> Result<()> {
    let cli = Cli::parse();

    let mut config = Config::new();
    config.verbose = cli.verbose;
    config.code_only = cli.code_only;

    if cli.git_only {
        config.git_mode = GitMode::Yes;
    } else if cli.no_git {
        config.git_mode = GitMode::No;
    }

    config.load_local_config();
    config.validate()?;

    if !cli.stdout {
        println!("ðŸ§¶ Knitting repository...");
    }

    let enumerator = FileEnumerator::new(config.clone());
    let raw_files = enumerator.enumerate()?;

    let heuristic_filter = HeuristicFilter::new();
    let heuristics_files = heuristic_filter.filter(raw_files);

    let filter = FileFilter::new(config.clone())?;
    let target_files = filter.filter(heuristics_files);

    if cli.verbose {
        eprintln!("ðŸ“¦ Packing {} files...", target_files.len());
    }

    let mut full_context = String::with_capacity(100_000);

    match cli.format {
        OutputFormat::Text => pack_text(&target_files, &mut full_context),
        OutputFormat::Xml => pack_xml(&target_files, &mut full_context),
    }

    // Count tokens
    let token_count = Tokenizer::count(&full_context);

    if cli.stdout {
        print!("{full_context}");
        eprintln!(
            "\nðŸ“Š Context Size: {} tokens",
            token_count.to_string().yellow().bold()
        );
    } else {
        fs::write("context.txt", &full_context)?;
        println!("âœ… Generated 'context.txt'");
        println!(
            "ðŸ“Š Context Size: {} tokens",
            token_count.to_string().yellow().bold()
        );
    }

    Ok(())
}

fn normalize_path(path: &Path) -> String {
    path.to_string_lossy().replace('\\', "/")
}

fn pack_text(files: &[std::path::PathBuf], out: &mut String) {
    for path in files {
        let path_str = normalize_path(path);
        writeln!(
            out,
            "================================================================================"
        )
        .unwrap();
        writeln!(out, "FILE: {path_str}").unwrap();
        writeln!(
            out,
            "================================================================================"
        )
        .unwrap();

        match fs::read_to_string(path) {
            Ok(content) => {
                out.push_str(&content);
            }
            Err(e) => {
                writeln!(out, "<ERROR READING FILE: {e}>").unwrap();
            }
        }
        out.push_str("\n\n");
    }
}

fn pack_xml(files: &[std::path::PathBuf], out: &mut String) {
    writeln!(out, "<documents>").unwrap();
    for path in files {
        let path_str = normalize_path(path);
        writeln!(out, "  <document path=\"{path_str}\">").unwrap();

        match fs::read_to_string(path) {
            Ok(content) => {
                out.push_str(&content);
            }
            Err(e) => {
                writeln!(out, "    <!-- ERROR READING FILE: {e} -->").unwrap();
            }
        }
        writeln!(out, "\n  </document>").unwrap();
    }
    writeln!(out, "</documents>").unwrap();
}
