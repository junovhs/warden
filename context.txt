ğŸ›¡ï¸ SYSTEM MANDATE: THE WARDEN PROTOCOL
ROLE: High-Integrity Systems Architect (NASA/JPL Standard).
CONTEXT: You are coding inside a strict environment enforced by Warden.

THE 3 LAWS (Non-Negotiable):

1. LAW OF ATOMICITY
   - Files: MUST be < 2000 tokens.
   - Action: Split immediately if larger.

2. LAW OF COMPLEXITY
   - Cyclomatic Complexity: MUST be â‰¤ 5 per function.
   - Nesting Depth: MUST be â‰¤ 2 levels.
   - Function Arguments: MUST be â‰¤ 5 parameters.

3. LAW OF PARANOIA
   - Use Result<T, E> for I/O and fallible operations.
   - NO .unwrap() or .expect() calls.

OUTPUT FORMAT (MANDATORY):

When providing code files, use this exact format:

1. Declare ALL files first:

<delivery>
path/to/file1.rs
path/to/file2.rs [NEW]
</delivery>

2. Provide EACH file:

<file path='path/to/file1.rs'>
[complete file contents]
</file>

RULES:
- Every file in <delivery> MUST have a matching <file> block
- Do NOT use markdown code blocks - use <file> tags only
- Do NOT truncate files
- Paths must match exactly

The `warden apply` command will REJECT incomplete deliveries.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BEGIN CODEBASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

<file path=".gitignore">
# Rust build artifacts
/target/
/docs/
/Cargo.lock

# Logs and temp files
*.log
*.tmp
*.bak

# Generated AI pack and test fixtures
ai-pack/
tests/tmp/
tmp/
*.manifest.json

# Editor/OS files
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp
*.swo

# Git / OS cruft
*~
*.orig

# Node / frontend extras
node_modules/
dist/
build/
coverage/
.env

# Python extras
__pycache__/
.venv/
venv/

# Repomix or AI output
*.xml
*.jsonl

# Ignore everything inside /target/release except binaries we care about
!/target/release/saccade.exe
!/target/release/gauntlet.exe</file>

<file path="Cargo.toml">
[package]
name = "warden"
version = "0.4.0"
edition = "2021"

[lib]
name = "warden_core"
path = "src/lib.rs"

[[bin]]
name = "warden"
path = "src/bin/warden.rs"

[[bin]]
name = "knit"
path = "src/bin/knit.rs"

[dependencies]
anyhow = "1.0"
thiserror = "1.0"
regex = "1.10"
walkdir = "2.5"
clap = { version = "4.5", features = ["derive"] }
ignore = "0.4"
colored = "2.1"
rayon = "1.10"
once_cell = "1.19"
serde = { version = "1.0", features = ["derive"] }
toml = "0.8"
arboard = "3.4"

# THE BRAINS
tiktoken-rs = "0.5"

# UI / TUI
ratatui = "0.29"
crossterm = "0.28"

# Structural Parsing
tree-sitter = "0.20"
tree-sitter-rust = "0.20"
tree-sitter-python = "0.20"
tree-sitter-typescript = "0.20"
tree-sitter-javascript = "0.20"
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Spencer Nunamaker

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="README.md">
# ğŸ›¡ï¸ Warden Protocol

**Structural governance for AI-assisted development.**

> *"The rules are like the seat belts in a car: Initially, using them is perhaps a little uncomfortable, but after a while, it becomes second nature, and not using them is unimaginable."*  
> â€” Gerard J. Holzmann, NASA/JPL

Warden enforces **NASA "Power of 10" Rules** adapted for modern development. It's not a style linterâ€”it's an architectural MRI that keeps code AI-readable and human-verifiable.

**Languages:** Rust, TypeScript, JavaScript, Python

---

## The Problem

AI-generated code drifts. Functions bloat. Complexity creeps. Context windows overflow.

You paste 50KB into Claude, it generates a 400-line function with 6 levels of nesting, and now you're debugging something neither you nor the AI can reason about.

Warden stops this at the source.

---

## The 3 Laws

### 1. Law of Atomicity
Files must be **< 2000 tokens**.

Small files fit in context windows. Small files are verifiable. When a file grows too large, split it.

### 2. Law of Complexity  
- **Cyclomatic Complexity:** â‰¤ 10 per function (configurable down to 4 for "Spartan" mode)
- **Nesting Depth:** â‰¤ 4 levels (no arrow code)
- **Function Arguments:** â‰¤ 5 parameters (use structs)

If you can't read a function in one breath, it's too complex.

### 3. Law of Paranoia
- No `.unwrap()` or `.expect()` (Rust)
- Fallible operations must return `Result`

The type system is your ally. Don't lie to the compiler.

---

## Installation

```bash
cargo install --path .
```

This installs two binaries: `warden` and `knit`.

---

## Quick Start

```bash
cd your-project
warden              # Scan for violations (auto-creates warden.toml)
knit --prompt       # Generate context.txt for AI
```

That's it. No setup requiredâ€”Warden detects your project type and configures itself.

---

## The Workflow

Warden isn't just a linterâ€”it's a closed-loop system for AI development.

### 1. Generate Context

```bash
knit --prompt
```

Creates `context.txt` containing:
- Your codebase (filtered, deduplicated)
- The Warden Protocol system prompt
- Output format specification for AI responses
- Token count

### 2. Chat with AI

Drag `context.txt` into Claude/GPT/Gemini. Ask for changes.

The AI will respond with:

```xml
<delivery>
src/lib.rs
src/new_module.rs [NEW]
</delivery>

<file path="src/lib.rs">
// complete file contents
</file>

<file path="src/new_module.rs">
// complete file contents
</file>
```

### 3. Apply Changes

Copy the AI's entire response (Cmd+A, Cmd+C), then:

```bash
warden apply
```

This:
- Parses `<delivery>` manifest and `<file>` blocks
- Validates all declared files are provided
- **Rejects markdown code blocks** (``` or ~~~) â€” these corrupt source files
- Creates timestamped backups in `.warden_apply_backup/`
- Writes files atomically
- On failure: generates AI-friendly error message, copies to clipboard

### 4. Verify

```bash
warden
```

Runs structural analysis. If violations exist, exit code is non-zero.

For full verification including your language linter:

```bash
warden check
```

Runs whatever command is configured in `warden.toml` (e.g., `cargo clippy`, `biome check`).

### 5. Iterate

If `warden apply` fails, the error message is already in your clipboard. Paste it back to the AI, get corrected output, repeat.

---

## Commands

| Command | Description |
|---------|-------------|
| `warden` | Run structural scan (auto-creates config if missing) |
| `warden --ui` | Interactive TUI dashboard |
| `warden --init` | Create/regenerate `warden.toml` |
| `warden apply` | Apply AI response from clipboard |
| `warden apply --dry-run` | Validate without writing |
| `warden check` | Run configured check command |
| `warden fix` | Run configured fix command |
| `warden prompt` | Print system prompt |
| `warden prompt -c` | Copy system prompt to clipboard |
| `knit` | Generate context.txt |
| `knit --prompt` | Include system prompt in context |
| `knit --stdout` | Output to stdout instead of file |

---

## Configuration

Warden auto-generates `warden.toml` on first run, detecting your project type:

**Rust projects** get:
```toml
[commands]
check = "cargo clippy --all-targets -- -D warnings -D clippy::pedantic"
fix = "cargo fmt"
```

**Node/TypeScript projects** get:
```toml
[commands]
check = "npx @biomejs/biome check src/"
fix = "npx @biomejs/biome check --write src/"
```

**Python projects** get:
```toml
[commands]
check = "ruff check ."
fix = "ruff check --fix ."
```

### Full Configuration

```toml
[rules]
max_file_tokens = 2000
max_cyclomatic_complexity = 10
max_nesting_depth = 4
max_function_args = 5
max_function_words = 3
ignore_naming_on = ["tests", "spec"]

[commands]
check = "cargo clippy --all-targets -- -D warnings -D clippy::pedantic"
fix = "cargo fmt"
```

### Strict Mode (Spartan)

For maximum discipline:

```toml
[rules]
max_file_tokens = 1500
max_cyclomatic_complexity = 4
max_nesting_depth = 2
max_function_args = 4
```

---

## TUI Dashboard

```bash
warden --ui
```

| Key | Action |
|-----|--------|
| `j/k` | Navigate files |
| `s` | Cycle sort mode (name/size/errors) |
| `f` | Toggle error filter |
| `q` | Quit |

---

## Output Format (for AI)

When you include `--prompt` in your knit command, the AI is instructed to respond with:

```xml
<delivery>
path/to/file1.rs
path/to/file2.rs [NEW]
path/to/obsolete.rs [DELETE]
</delivery>

<file path="path/to/file1.rs">
[complete file contents - no truncation]
</file>

<file path="path/to/file2.rs">
[complete file contents]
</file>
```

**Rules:**
- Every file in `<delivery>` must have a matching `<file>` block (except `[DELETE]`)
- `[NEW]` marks files being created
- `[DELETE]` marks files to remove
- Files must be completeâ€”no `// ... rest of file` truncation
- **No markdown code blocks** â€” use `<file>` tags only
- Paths are relative to project root

---

## Safety Features

### Markdown Block Rejection

`warden apply` scans file contents for markdown code blocks (```, ~~~) and **rejects the entire apply** if found. This prevents a common AI failure mode where code blocks get embedded in source files.

### Atomic Backups

Before any write, Warden copies existing files to `.warden_apply_backup/TIMESTAMP/`. If something goes wrong, your original files are preserved.

### Validation-First

Files are validated before any disk writes. If the manifest declares 5 files but the AI only provided 4, nothing is written.

---

## Philosophy

Warden exists because AI-assisted development needs constraints, not suggestions.

The original Power of 10 rules were designed for life-critical systems where bugs kill people. We're not building flight software, but we are building systems that need to remain comprehensible as they evolve through hundreds of AI-human iterations.

Complexity is the enemy. Warden is the checkpoint.

---

## License

MIT
</file>

<file path="roadmap.md">
# Warden Protocol Roadmap

## Current State: v0.4.0

The core loop works. You can generate context, chat with AI, apply responses, and verify.

---

## v0.5.0 â€” Bulletproof Apply

**Theme:** If it applies, it's valid. If it's invalid, it rejects hard.

### Validation Hardening

- [ ] **Truncation detection (smart)**  
  Reject files that are obviously incomplete:
  - Unbalanced braces/brackets (language-aware: skip for Python)
  - Truncation markers: `// ...`, `/* ... */`, `// rest of file`, `// etc`, `// remaining code`
  - Unclosed strings, unclosed block comments
  - Files that end mid-statement (heuristic: ends with `{`, `,`, `(`, `=`)
  
  *Goal: Zero false positives. If Warden rejects it, it was definitely broken.*

- [ ] **Path safety validation**  
  Block dangerous paths before they touch disk:
  - `../` directory traversal
  - Absolute paths (`/etc/passwd`, `C:\Windows\...`)
  - Sensitive targets: `.git/`, `.env`, `.ssh/`, `id_rsa`, `.aws/`, `credentials`
  - Hidden files starting with `.` (configurable)
  
  *Enterprise-grade paranoia.*

- [ ] **Strict format enforcement**  
  If AI doesn't use `<file path="...">` tags, reject immediately with clear error message explaining the required format. No fallback parsing. No guessing. Garbage in = garbage out.

### Workflow Enhancement

- [ ] **Error injection in knit**  
  When `knit --prompt` runs:
  1. Run `warden` scan internally
  2. If violations exist, append them to context:
  ```
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  CURRENT VIOLATIONS (FIX THESE)
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  src/apply/validator.rs:42 [LAW OF COMPLEXITY]
    High Complexity: Score is 12 (Max: 10). Hard to test.
  
  src/lib.rs:156 [LAW OF ATOMICITY]  
    File is 2,341 tokens (Max: 2,000). Split it.
  ```
  
  *AI sees what's broken. AI fixes it. You don't have to explain.*

### Git Integration (Experimental)

- [ ] **`warden apply --commit`**  
  On successful apply:
  1. `git add .`
  2. Auto-generate commit message from `<delivery>` manifest
  3. Commit (no push by default)
  
  Example commit message:
  ```
  warden: update src/apply/validator.rs, add src/apply/safety.rs
  
  Applied via warden apply
  ```

- [ ] **`warden apply --commit --push`**  
  Same as above, but also pushes.

*Philosophy: If it passes validation, commit it. Use git as your undo. Atomic commits per apply.*

### Cut from v0.5

- ~~Backup system~~ â€” Use git. If you applied broken code, that's on you. `git checkout .` exists.
- ~~Markdown fallback parsing~~ â€” If AI can't follow format instructions, use a different AI.

---

## v0.6.0 â€” Intelligence

**Theme:** Understand the code, not just count it.

### Smarter Analysis

- [ ] **Function-level violation reporting**  
  Not just "file has violations" but:
  ```
  src/engine.rs
  
    fn process_batch() [Line 45]
    â”œâ”€ Complexity: 14 (max 10)
    â”œâ”€ Nesting depth: 5 (max 4)  
    â”œâ”€ Contributing factors:
    â”‚   â”œâ”€ 3 nested if statements (lines 52, 58, 61)
    â”‚   â”œâ”€ 2 match arms with complex guards (lines 67, 89)
    â”‚   â””â”€ while loop with break conditions (line 94)
    â””â”€ Suggestion: Extract inner match to separate function
  
    fn validate_input() [Line 142]
    â”œâ”€ Arity: 7 arguments (max 5)
    â””â”€ Suggestion: Group into ValidateOptions struct
  ```
  
  *Learn from the patterns. Understand WHY it's complex.*

- [ ] **Incremental scanning**  
  Only re-analyze changed files:
  - Track file mtimes in `.warden_cache`
  - Or use `git status` to find modified files
  - Full rescan on config change
  
  *Goal: As smart as rustc about what needs recompilation.*

### Smarter Context Generation

- [ ] **Dependency-aware knitting**  
  When file A imports file B:
  - Include B in context even if not explicitly requested
  - Order: dependencies before dependents
  - Show import graph in context header
  
  *AI sees the full picture without you manually selecting files.*

- [ ] **Import graph visualization**  
  `warden deps` or `warden deps src/main.rs`:
  ```
  src/main.rs
  â”œâ”€ src/config.rs
  â”‚  â””â”€ src/types.rs
  â”œâ”€ src/engine.rs
  â”‚  â”œâ”€ src/types.rs
  â”‚  â””â”€ src/analysis.rs
  â””â”€ src/tui/mod.rs
     â”œâ”€ src/tui/state.rs
     â””â”€ src/tui/view.rs
  ```

---

## v0.7.0 â€” Testing & Stability

**Theme:** Trust the tool.

- [ ] **Test suite**
  - Unit tests for each module
  - Integration tests: knit â†’ apply â†’ verify flow
  - Fixture files for each language (Rust, TS, Python)
  - Edge cases: malformed input, huge files, unicode

- [ ] **Performance benchmarks**
  - Scan time vs file count
  - Token counting speed
  - Memory usage on large codebases

- [ ] **CLI stability guarantee**
  - Document all flags and subcommands
  - Semantic versioning discipline
  - Deprecation warnings before removal

---

## v0.8.0 â€” Ecosystem

**Theme:** CI/CD and tooling integration.

- [ ] **JSON output**: `warden --format json`
- [ ] **SARIF output**: GitHub Code Scanning integration
- [ ] **Exit codes**: Documented, consistent, scriptable
- [ ] **Pre-commit hook**: `warden hook install`
- [ ] **GitHub Action**: Official action for PR checks

---

## v1.0.0 â€” Release

- [ ] Published to **crates.io**
- [ ] **Homebrew**: `brew install warden` (Mac/Linux package manager)
- [ ] **Scoop/Winget**: Windows package managers
- [ ] Complete documentation site
- [ ] Logo and branding

---

## v2.0.0 â€” Language Expansion

Way down the line:
- Go
- C/C++ (original Power of 10 target)
- Java/Kotlin

Each language needs: grammar, complexity patterns, naming rules, safety checks.

---

## Future / Speculative

### Metrics Dashboard
Track complexity trends over time. SQLite backend. Charts showing codebase health evolution.

### AI Provider Integration  
When money exists: direct Claude/GPT API calls. `warden chat` command. Self-contained loop without browser.

### Complexity Budget
Instead of per-function limits, allocate complexity budget per file that can be distributed across functions. Some functions can be complex if others are simple.

### Session Branches
`warden session start` creates timestamped branch. Each `warden apply --commit` adds to it. `warden session merge` squashes and merges to main.

---

## Not Doing

- **VS Code Extension** â€” IDE lock-in, maintenance burden
- **Undo/backup system** â€” Use git
- **Markdown fallback parsing** â€” Enforce format discipline
- **Watch mode** â€” Adds complexity, unclear benefit

---

## Principles

1. **Reject bad input, don't fix it**  
   Warden is a gatekeeper, not a fixer.

2. **Git is the undo system**  
   Don't reinvent version control.

3. **Explicit > Magic**  
   If AI doesn't follow the format, fail loudly.

4. **Learn from violations**  
   Error messages should teach, not just complain.
</file>

<file path="src/analysis.rs">
// src/analysis.rs
use crate::checks::{self, CheckContext};
use crate::config::RuleConfig;
use crate::types::Violation;
use anyhow::Result;
use tree_sitter::{Language, Parser, Query};

pub struct Analyzer {
    rust_naming: Query,
    rust_safety: Query,
    rust_complexity: Query,
    rust_banned: Query,
    js_naming: Query,
    js_safety: Query,
    js_complexity: Query,
    py_naming: Query,
    py_safety: Query,
    py_complexity: Query,
}

impl Default for Analyzer {
    fn default() -> Self {
        Self::new()
    }
}

impl Analyzer {
    #[must_use]
    pub fn new() -> Self {
        Self {
            rust_naming: q(
                tree_sitter_rust::language(),
                "(function_item name: (identifier) @name)",
            ),
            rust_safety: q(tree_sitter_rust::language(), r"(match_expression) @safe"),
            rust_complexity: q(
                tree_sitter_rust::language(),
                r#"
                (if_expression) @branch
                (match_arm) @branch
                (while_expression) @branch
                (for_expression) @branch
                (binary_expression operator: ["&&" "||"]) @branch
            "#,
            ),
            rust_banned: q(
                tree_sitter_rust::language(),
                r#"
                (call_expression function: (field_expression field: (field_identifier) @m (#eq? @m "unwrap"))) @banned
            "#,
            ),
            js_naming: q(
                tree_sitter_typescript::language_typescript(),
                r"
                (function_declaration name: (identifier) @name)
                (method_definition name: (property_identifier) @name)
                (variable_declarator name: (identifier) @name value: [(arrow_function) (function_expression)])
            ",
            ),
            js_safety: q(
                tree_sitter_typescript::language_typescript(),
                r"(try_statement) @safe",
            ),
            js_complexity: q(
                tree_sitter_typescript::language_typescript(),
                r#"
                (if_statement) @branch
                (for_statement) @branch
                (for_in_statement) @branch
                (while_statement) @branch
                (do_statement) @branch
                (switch_case) @branch
                (catch_clause) @branch
                (ternary_expression) @branch
                (binary_expression operator: ["&&" "||" "??"]) @branch
            "#,
            ),
            py_naming: q(
                tree_sitter_python::language(),
                "(function_definition name: (identifier) @name)",
            ),
            py_safety: q(tree_sitter_python::language(), r"(try_statement) @safe"),
            py_complexity: q(
                tree_sitter_python::language(),
                r"
                (if_statement) @branch
                (for_statement) @branch
                (while_statement) @branch
                (except_clause) @branch
                (boolean_operator) @branch
            ",
            ),
        }
    }

    #[must_use]
    pub fn analyze(
        &self,
        lang: &str,
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        let Some(queries) = self.select_language(lang) else {
            return vec![];
        };
        Self::run_analysis(queries, filename, content, config)
    }

    fn select_language(
        &self,
        lang: &str,
    ) -> Option<(Language, &Query, &Query, &Query, Option<&Query>)> {
        if lang == "rs" {
            return Some(self.queries_rust());
        }
        if matches!(lang, "js" | "jsx" | "ts" | "tsx") {
            return Some(self.queries_js());
        }
        if lang == "py" {
            return Some(self.queries_python());
        }
        None
    }

    fn queries_rust(&self) -> (Language, &Query, &Query, &Query, Option<&Query>) {
        (
            tree_sitter_rust::language(),
            &self.rust_naming,
            &self.rust_safety,
            &self.rust_complexity,
            Some(&self.rust_banned),
        )
    }

    fn queries_js(&self) -> (Language, &Query, &Query, &Query, Option<&Query>) {
        (
            tree_sitter_typescript::language_typescript(),
            &self.js_naming,
            &self.js_safety,
            &self.js_complexity,
            None,
        )
    }

    fn queries_python(&self) -> (Language, &Query, &Query, &Query, Option<&Query>) {
        (
            tree_sitter_python::language(),
            &self.py_naming,
            &self.py_safety,
            &self.py_complexity,
            None,
        )
    }

    fn run_analysis(
        (language, naming, safety, complexity, banned): (
            Language,
            &Query,
            &Query,
            &Query,
            Option<&Query>,
        ),
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        let mut parser_instance = Parser::new();
        let Ok(parser) = parser_instance.get_init(language) else {
            return vec![];
        };

        let Some(tree) = parser.parse(content, None) else {
            return vec![];
        };

        let mut violations = Vec::new();
        let ctx = CheckContext {
            root: tree.root_node(),
            source: content,
            filename,
            config,
        };

        checks::check_naming(&ctx, naming, &mut violations);
        checks::check_safety(&ctx, safety, &mut violations);
        checks::check_metrics(&ctx, complexity, &mut violations);

        if let Some(bq) = banned {
            let _ = checks::check_banned(&ctx, bq, &mut violations);
        }

        violations
    }
}

trait ParserInit {
    fn get_init(&mut self, lang: Language) -> Result<&mut Self>;
}

impl ParserInit for Parser {
    fn get_init(&mut self, lang: Language) -> Result<&mut Self> {
        self.set_language(lang)?;
        Ok(self)
    }
}

fn q(lang: Language, pattern: &str) -> Query {
    Query::new(lang, pattern).expect("Invalid Query")
}
</file>

<file path="src/apply/extractor.rs">
// src/apply/extractor.rs
use crate::apply::types::FileContent;
use anyhow::Result;
use regex::Regex;
use std::collections::HashMap;

/// Extracts file blocks from the raw response text.
///
/// # Errors
/// Returns error if regex compilation fails.
pub fn extract_files(response: &str) -> Result<HashMap<String, FileContent>> {
    let mut files = HashMap::new();
    let open_tag_re = Regex::new(r"(?i)<file\s+([^>]+)>")?;
    let path_attr_re = Regex::new(r#"(?i)path\s*=\s*(?:"([^"]*)"|'([^']*)'|([^>\s]+))"#)?;
    let close_tag_re = Regex::new(r"(?i)</file>")?;

    let mut current_pos = 0;

    while let Some(cap) = open_tag_re.find_at(response, current_pos) {
        current_pos = process_tag_match(response, cap, &path_attr_re, &close_tag_re, &mut files);
    }

    Ok(files)
}

fn process_tag_match(
    response: &str,
    cap: regex::Match,
    path_re: &Regex,
    close_re: &Regex,
    files: &mut HashMap<String, FileContent>,
) -> usize {
    let tag_end = cap.end();
    let attributes_str = cap.as_str();

    let Some(path) = extract_path(path_re, attributes_str) else {
        return tag_end;
    };

    if let Some(close_match) = close_re.find_at(response, tag_end) {
        let content_end = close_match.start();
        let raw_content = &response[tag_end..content_end];

        let clean_content = clean_file_content(raw_content);
        let line_count = clean_content.lines().count();

        files.insert(
            path,
            FileContent {
                content: clean_content,
                line_count,
            },
        );

        close_match.end()
    } else {
        // Stop parsing if no closing tag found
        response.len()
    }
}

fn extract_path(re: &Regex, attrs: &str) -> Option<String> {
    re.captures(attrs).and_then(|captures| {
        captures
            .get(1)
            .or_else(|| captures.get(2))
            .or_else(|| captures.get(3))
            .map(|m| m.as_str().to_string())
    })
}

fn clean_file_content(raw: &str) -> String {
    let trimmed = raw.trim();
    let lines: Vec<&str> = trimmed.lines().collect();

    if lines.is_empty() {
        return String::new();
    }

    let first_line = lines.first().unwrap_or(&"").trim();
    let last_line = lines.last().unwrap_or(&"").trim();

    let starts_with_fence = first_line.starts_with("```");
    let ends_with_fence = last_line.starts_with("```");

    if starts_with_fence && ends_with_fence {
        if lines.len() <= 2 {
            return String::new();
        }
        return lines[1..lines.len() - 1].join("\n");
    }

    trimmed.to_string()
}
</file>

<file path="src/apply/manifest.rs">
// src/apply/manifest.rs
use crate::apply::types::{ManifestEntry, Operation};
use anyhow::Result;
use regex::Regex;

/// Parses the delivery manifest block.
///
/// # Errors
/// Returns error if regex compilation fails.
pub fn parse_manifest(response: &str) -> Result<Option<Vec<ManifestEntry>>> {
    let Some((start, end)) = find_manifest_block(response)? else {
        return Ok(None);
    };

    let block = &response[start..end];
    let entries = parse_manifest_lines(block)?;

    Ok(Some(entries))
}

fn find_manifest_block(response: &str) -> Result<Option<(usize, usize)>> {
    let open_re = Regex::new(r"(?i)<delivery>")?;
    let close_re = Regex::new(r"(?i)</delivery>")?;

    let start_match = open_re.find(response);
    let end_match = close_re.find(response);

    match (start_match, end_match) {
        (Some(s), Some(e)) => Ok(Some((s.end(), e.start()))),
        _ => Ok(None),
    }
}

fn parse_manifest_lines(block: &str) -> Result<Vec<ManifestEntry>> {
    let list_marker_re = Regex::new(r"^\s*(?:[-*]|\d+\.)\s+")?;
    let mut entries = Vec::new();

    for line in block.lines() {
        if let Some(entry) = parse_manifest_line(line, &list_marker_re) {
            entries.push(entry);
        }
    }
    Ok(entries)
}

fn parse_manifest_line(line: &str, marker_re: &Regex) -> Option<ManifestEntry> {
    let trimmed = line.trim();
    if trimmed.is_empty() {
        return None;
    }

    let clean_line = marker_re.replace(trimmed, "");
    let clean_line_ref = clean_line.as_ref();

    if clean_line_ref.trim().is_empty() {
        return None;
    }

    let (path_raw, op) = parse_operation(clean_line_ref);
    let final_path = extract_clean_path(&path_raw);

    if final_path.is_empty() {
        None
    } else {
        Some(ManifestEntry {
            path: final_path,
            operation: op,
        })
    }
}

fn parse_operation(line: &str) -> (String, Operation) {
    let upper = line.to_uppercase();
    if upper.contains("[NEW]") {
        (
            line.replace("[NEW]", "").replace("[new]", ""),
            Operation::New,
        )
    } else if upper.contains("[DELETE]") {
        (
            line.replace("[DELETE]", "").replace("[delete]", ""),
            Operation::Delete,
        )
    } else {
        (line.to_string(), Operation::Update)
    }
}

fn extract_clean_path(raw: &str) -> String {
    raw.split_whitespace().next().unwrap_or(raw).to_string()
}
</file>

<file path="src/apply/messages.rs">
// src/apply/messages.rs
use crate::apply::types::ApplyOutcome;
use colored::Colorize;

pub fn print_outcome(outcome: &ApplyOutcome) {
    match outcome {
        ApplyOutcome::Success { written, backed_up } => print_success(written, *backed_up),
        ApplyOutcome::ValidationFailure {
            errors,
            missing,
            ai_message,
        } => {
            print_validation_errors(errors, missing);
            print_ai_feedback(ai_message);
        }
        ApplyOutcome::ParseError(e) => println!("{}: {e}", "âš ï¸  Parse Error".red()),
        ApplyOutcome::WriteError(e) => println!("{}: {e}", "ğŸ’¥ Write Error".red()),
    }
}

fn print_success(written: &[String], backed_up: bool) {
    println!("{}", "âœ… Apply successful!".green().bold());
    if backed_up {
        println!("   (Backup created in .warden_apply_backup/)");
    }
    println!();
    for file in written {
        println!("   {} {file}", "âœ“".green());
    }
    println!();
    println!("Run {} to verify.", "warden check".yellow());
}

fn print_validation_errors(errors: &[String], missing: &[String]) {
    println!("{}", "âŒ Validation Failed".red().bold());

    if !missing.is_empty() {
        println!(
            "{}",
            "\nMissing Files (Declared but not provided):".yellow()
        );
        for f in missing {
            println!("   - {f}");
        }
    }

    if !errors.is_empty() {
        println!("{}", "\nContent Errors:".yellow());
        for e in errors {
            println!("   - {e}");
        }
    }
}

fn print_ai_feedback(ai_message: &str) {
    println!();
    println!("{}", "ğŸ“‹ Paste this back to the AI:".cyan().bold());
    println!("{}", "â”€".repeat(60).black());
    println!("{ai_message}");
    println!("{}", "â”€".repeat(60).black());

    if crate::clipboard::copy_to_clipboard(ai_message).is_ok() {
        println!("{}", "âœ“ Copied to clipboard".green());
    }
}

#[must_use]
pub fn format_ai_rejection(missing: &[String], errors: &[String]) -> String {
    use std::fmt::Write;
    let mut msg = String::from("The previous output was rejected by the Warden Protocol.\n\n");

    if !missing.is_empty() {
        msg.push_str("MISSING FILES (Declared in <delivery> but not found in <file> blocks):\n");
        for f in missing {
            let _ = writeln!(msg, "- {f}");
        }
        msg.push('\n');
    }

    if !errors.is_empty() {
        msg.push_str("VALIDATION ERRORS:\n");
        for e in errors {
            let _ = writeln!(msg, "- {e}");
        }
        msg.push('\n');
    }

    msg.push_str(
        "Please provide the missing or corrected files using the <file path=\"...\"> format.",
    );
    msg
}
</file>

<file path="src/apply/mod.rs">
// src/apply/mod.rs
pub mod extractor;
pub mod manifest;
pub mod messages;
pub mod types;
pub mod validator;
pub mod writer;

use crate::clipboard;
use anyhow::{Context, Result};
use types::{ApplyOutcome, ExtractedFiles, Manifest};

/// Runs the apply command logic.
///
/// # Errors
/// Returns error if clipboard access fails or extraction fails.
pub fn run_apply(dry_run: bool) -> Result<ApplyOutcome> {
    let content = clipboard::read_clipboard().context("Failed to read clipboard")?;

    if content.trim().is_empty() {
        return Ok(ApplyOutcome::ParseError("Clipboard is empty".to_string()));
    }

    let validation = parse_and_validate(&content);

    match validation {
        ApplyOutcome::Success { .. } => {
            if dry_run {
                return Ok(ApplyOutcome::Success {
                    written: vec!["(Dry Run) Files verified".to_string()],
                    backed_up: false,
                });
            }
            writer::write_files(&extractor::extract_files(&content)?)
        }
        _ => Ok(validation),
    }
}

pub fn print_result(outcome: &ApplyOutcome) {
    messages::print_outcome(outcome);
}

fn parse_and_validate(content: &str) -> ApplyOutcome {
    let manifest = match parse_manifest_step(content) {
        Ok(m) => m,
        Err(e) => return ApplyOutcome::ParseError(e),
    };

    let extracted = match extract_files_step(content) {
        Ok(e) => e,
        Err(e) => return ApplyOutcome::ParseError(e),
    };

    validator::validate(&manifest, &extracted)
}

fn parse_manifest_step(content: &str) -> Result<Manifest, String> {
    match manifest::parse_manifest(content) {
        Ok(Some(m)) => Ok(m),
        Ok(None) => Ok(Vec::new()),
        Err(e) => Err(format!("Manifest Error: {e}")),
    }
}

fn extract_files_step(content: &str) -> Result<ExtractedFiles, String> {
    extractor::extract_files(content).map_err(|e| format!("Extraction Error: {e}"))
}
</file>

<file path="src/apply/types.rs">
// src/apply/types.rs
use std::collections::HashMap;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum Operation {
    Update,
    New,
    Delete,
}

#[derive(Debug, Clone)]
pub struct ManifestEntry {
    pub path: String,
    pub operation: Operation,
}

#[derive(Debug, Clone)]
pub struct FileContent {
    pub content: String,
    pub line_count: usize,
}

#[derive(Debug)]
pub enum ApplyOutcome {
    Success {
        written: Vec<String>,
        backed_up: bool,
    },
    ValidationFailure {
        errors: Vec<String>,
        missing: Vec<String>,
        ai_message: String,
    },
    ParseError(String),
    WriteError(String),
}

// The manifest is just a list of entries
pub type Manifest = Vec<ManifestEntry>;

// The extracted files are mapped by path
pub type ExtractedFiles = HashMap<String, FileContent>;
</file>

<file path="src/apply/validator.rs">
// src/apply/validator.rs
use crate::apply::messages;
use crate::apply::types::{ApplyOutcome, ExtractedFiles, Manifest, Operation};

const MARKDOWN_PATTERNS: &[&str] = &[
    "```", // Standard markdown code blocks
    "~~~", // Alternative markdown code blocks
    "```rust",
    "```python",
    "```javascript",
    "```typescript",
    "```html",
    "```css",
    "```json",
    "```yaml",
    "```toml",
    "```bash",
    "```sh",
    "```sql",
    "```xml",
];

#[must_use]
pub fn validate(manifest: &Manifest, extracted: &ExtractedFiles) -> ApplyOutcome {
    let missing = check_missing(manifest, extracted);
    let errors = check_content(extracted);

    if !missing.is_empty() || !errors.is_empty() {
        let ai_message = messages::format_ai_rejection(&missing, &errors);
        return ApplyOutcome::ValidationFailure {
            errors,
            missing,
            ai_message,
        };
    }

    let written = extracted.keys().cloned().collect();
    ApplyOutcome::Success {
        written,
        backed_up: true,
    }
}

fn check_missing(manifest: &Manifest, extracted: &ExtractedFiles) -> Vec<String> {
    let mut missing = Vec::new();
    for entry in manifest {
        if entry.operation != Operation::Delete && !extracted.contains_key(&entry.path) {
            missing.push(entry.path.clone());
        }
    }
    missing
}

fn check_content(extracted: &ExtractedFiles) -> Vec<String> {
    let mut errors = Vec::new();
    for (path, file) in extracted {
        check_single_file(path, &file.content, &mut errors);
    }
    errors
}

fn check_single_file(path: &str, content: &str, errors: &mut Vec<String>) {
    if content.trim().is_empty() {
        errors.push(format!("{path} is empty"));
        return;
    }

    if let Some(pattern) = detect_markdown_block(content) {
        errors.push(format!(
            "{path} contains markdown code block '{pattern}' - \
            AI output must use <file> tags, not markdown"
        ));
    }
}

fn detect_markdown_block(content: &str) -> Option<&'static str> {
    MARKDOWN_PATTERNS
        .iter()
        .find(|&&pattern| content.contains(pattern))
        .copied()
}
</file>

<file path="src/apply/writer.rs">
// src/apply/writer.rs
use crate::apply::types::{ApplyOutcome, ExtractedFiles};
use anyhow::{anyhow, Context, Result};
use std::fs;
use std::path::{Path, PathBuf};
use std::time::{SystemTime, UNIX_EPOCH};

const BACKUP_DIR: &str = ".warden_apply_backup";

/// Writes the extracted files to disk.
///
/// # Errors
/// Returns error if file system operations fail.
pub fn write_files(files: &ExtractedFiles) -> Result<ApplyOutcome> {
    let backup_path = create_backup(files)?;
    let mut written_paths = Vec::new();

    for (path_str, file_data) in files {
        write_single_file(path_str, &file_data.content)?;
        written_paths.push(path_str.clone());
    }

    Ok(ApplyOutcome::Success {
        written: written_paths,
        backed_up: backup_path.is_some(),
    })
}

fn write_single_file(path_str: &str, content: &str) -> Result<()> {
    let path = Path::new(path_str);
    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent)
            .map_err(|e| anyhow!("Failed to create directory {}: {e}", parent.display()))?;
    }
    fs::write(path, content).map_err(|e| anyhow!("Failed to write {path_str}: {e}"))?;
    Ok(())
}

/// Restores files from the latest backup.
///
/// # Errors
/// Returns error if no backup exists or restore fails.
pub fn restore_backup() -> Result<Vec<PathBuf>> {
    let backup_root = Path::new(BACKUP_DIR);
    if !backup_root.exists() {
        return Err(anyhow!("No backup directory found at {BACKUP_DIR}"));
    }

    let latest = find_latest_backup(backup_root)?;
    restore_files_from_backup(&latest)
}

fn find_latest_backup(root: &Path) -> Result<PathBuf> {
    let mut entries: Vec<_> = fs::read_dir(root)?
        .filter_map(std::result::Result::ok)
        .filter(|e| e.path().is_dir())
        .collect();

    entries.sort_by_key(|e| std::cmp::Reverse(e.file_name()));

    entries
        .first()
        .map(std::fs::DirEntry::path)
        .ok_or_else(|| anyhow!("No backups found in {BACKUP_DIR}"))
}

fn restore_files_from_backup(restore_source: &Path) -> Result<Vec<PathBuf>> {
    let mut restored_files = Vec::new();
    for entry in walkdir::WalkDir::new(restore_source) {
        let entry = entry?;
        if entry.file_type().is_file() {
            restored_files.push(restore_single_file_entry(&entry, restore_source)?);
        }
    }
    Ok(restored_files)
}

fn restore_single_file_entry(entry: &walkdir::DirEntry, restore_source: &Path) -> Result<PathBuf> {
    let rel_path = entry.path().strip_prefix(restore_source)?;
    if let Some(parent) = rel_path.parent() {
        fs::create_dir_all(parent)?;
    }
    fs::copy(entry.path(), rel_path)?;
    Ok(rel_path.to_path_buf())
}

fn create_backup(files: &ExtractedFiles) -> Result<Option<PathBuf>> {
    let files_to_backup: Vec<&String> = files.keys().filter(|p| Path::new(p).exists()).collect();

    if files_to_backup.is_empty() {
        return Ok(None);
    }

    let timestamp = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs();
    let backup_folder = Path::new(BACKUP_DIR).join(timestamp.to_string());

    fs::create_dir_all(&backup_folder).context("Failed to create backup directory")?;

    for path_str in files_to_backup {
        backup_single_file(path_str, &backup_folder)?;
    }

    Ok(Some(backup_folder))
}

fn backup_single_file(path_str: &str, backup_folder: &Path) -> Result<()> {
    let src = Path::new(path_str);
    let dest = backup_folder.join(path_str);

    if let Some(parent) = dest.parent() {
        fs::create_dir_all(parent)?;
    }

    fs::copy(src, dest).with_context(|| format!("Failed to backup {path_str}"))?;
    Ok(())
}
</file>

<file path="src/bin/knit.rs">
// src/bin/knit.rs
use anyhow::Result;
use clap::{Parser, ValueEnum};
use colored::Colorize;
use std::fmt::Write;
use std::fs;
use std::path::PathBuf;

use warden_core::config::{Config, GitMode};
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::prompt::PromptGenerator;
use warden_core::tokens::Tokenizer;

#[derive(Debug, Clone, ValueEnum)]
enum OutputFormat {
    Text,
    Xml,
}

#[derive(Parser)]
#[command(name = "knit")]
#[command(about = "Stitches atomic files into a single context file.")]
#[allow(clippy::struct_excessive_bools)]
struct Cli {
    #[arg(long, short)]
    stdout: bool,
    #[arg(long, short)]
    verbose: bool,
    #[arg(long)]
    git_only: bool,
    #[arg(long)]
    no_git: bool,
    #[arg(long)]
    code_only: bool,
    #[arg(long, short)]
    prompt: bool,
    #[arg(long, value_enum, default_value_t = OutputFormat::Text)]
    format: OutputFormat,
}

fn main() -> Result<()> {
    let cli = Cli::parse();
    let config = setup_config(&cli)?;

    if !cli.stdout {
        println!("ğŸ§¶ Knitting repository...");
    }

    let files = discover_files(&config, cli.verbose)?;
    let content = generate_content(&files, &cli, &config)?;
    let token_count = Tokenizer::count(&content);

    output_result(&content, token_count, cli.stdout)
}

fn setup_config(cli: &Cli) -> Result<Config> {
    let mut config = Config::new();
    config.verbose = cli.verbose;
    config.code_only = cli.code_only;
    config.git_mode = if cli.git_only {
        GitMode::Yes
    } else if cli.no_git {
        GitMode::No
    } else {
        GitMode::Auto
    };
    config.load_local_config();
    config.validate()?;
    Ok(config)
}

fn discover_files(config: &Config, verbose: bool) -> Result<Vec<PathBuf>> {
    let raw = FileEnumerator::new(config.clone()).enumerate()?;
    let h_files = HeuristicFilter::new().filter(raw);
    let t_files = FileFilter::new(config)?.filter(h_files);

    if verbose {
        eprintln!("ğŸ“¦ Packing {} files...", t_files.len());
    }
    Ok(t_files)
}

fn generate_content(files: &[PathBuf], cli: &Cli, config: &Config) -> Result<String> {
    let mut ctx = String::with_capacity(100_000);

    if cli.prompt {
        write_header(&mut ctx, config)?;
    }

    write_body(files, &mut ctx, &cli.format)?;

    if cli.prompt {
        write_footer(&mut ctx, config)?;
    }

    Ok(ctx)
}

fn write_body(files: &[PathBuf], ctx: &mut String, format: &OutputFormat) -> Result<()> {
    match format {
        OutputFormat::Text => pack_text(files, ctx),
        OutputFormat::Xml => pack_xml(files, ctx),
    }
}

fn write_header(ctx: &mut String, config: &Config) -> Result<()> {
    let gen = PromptGenerator::new(config.rules.clone());
    writeln!(ctx, "{}", gen.wrap_header()?)?;
    writeln!(ctx, "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nBEGIN CODEBASE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")?;
    Ok(())
}

fn write_footer(ctx: &mut String, config: &Config) -> Result<()> {
    let gen = PromptGenerator::new(config.rules.clone());
    writeln!(ctx, "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nEND CODEBASE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")?;
    writeln!(ctx, "{}", gen.generate_reminder()?)?;
    Ok(())
}

fn output_result(content: &str, tokens: usize, stdout: bool) -> Result<()> {
    let info = format!(
        "\nğŸ“Š Context Size: {} tokens",
        tokens.to_string().yellow().bold()
    );

    if stdout {
        print!("{content}");
        eprintln!("{info}");
    } else {
        fs::write("context.txt", content)?;
        println!("âœ… Generated 'context.txt'");
        println!("{info}");
    }
    Ok(())
}

fn pack_text(files: &[PathBuf], out: &mut String) -> Result<()> {
    for path in files {
        let p_str = path.to_string_lossy().replace('\\', "/");
        writeln!(out, "<file path=\"{p_str}\">")?;
        match fs::read_to_string(path) {
            Ok(c) => out.push_str(&c),
            Err(e) => writeln!(out, "<ERROR READING FILE: {e}>")?,
        }
        writeln!(out, "</file>\n")?;
    }
    Ok(())
}

fn pack_xml(files: &[PathBuf], out: &mut String) -> Result<()> {
    writeln!(out, "<documents>")?;
    for path in files {
        let p_str = path.to_string_lossy().replace('\\', "/");
        writeln!(out, "  <document path=\"{p_str}\"><![CDATA[")?;
        match fs::read_to_string(path) {
            Ok(c) => out.push_str(&c.replace("]]>", "]]]]><![CDATA[>")),
            Err(e) => writeln!(out, "ERROR READING FILE: {e}")?,
        }
        writeln!(out, "]]></document>")?;
    }
    writeln!(out, "</documents>")?;
    Ok(())
}
</file>

<file path="src/bin/warden.rs">
// src/bin/warden.rs
use anyhow::Result;
use clap::{Parser, Subcommand};
use colored::Colorize;
use std::fs;
use std::io;
use std::path::Path;
use std::process::{self, Command};

use warden_core::apply;
use warden_core::config::{Config, GitMode};
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::project;
use warden_core::prompt::PromptGenerator;
use warden_core::reporting;
use warden_core::rules::RuleEngine;
use warden_core::tui::state::App;
use warden_core::types::ScanReport;

#[derive(Parser)]
#[command(name = "warden")]
#[command(about = "Code quality guardian", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Option<Commands>,
    #[arg(long)]
    ui: bool,
    #[arg(long)]
    init: bool,
}

#[derive(Subcommand)]
enum Commands {
    Prompt {
        #[arg(long, short)]
        copy: bool,
    },
    Check,
    Fix,
    Apply {
        #[arg(long)]
        dry_run: bool,
    },
}

fn main() {
    if let Err(e) = run() {
        eprintln!("{} {e}", "error:".red().bold());
        process::exit(1);
    }
}

fn run() -> Result<()> {
    let cli = Cli::parse();

    if cli.init {
        return init_config();
    }

    ensure_config_exists();

    match cli.command {
        Some(Commands::Prompt { copy }) => handle_prompt(copy),
        Some(Commands::Check) => run_command("check"),
        Some(Commands::Fix) => run_command("fix"),
        Some(Commands::Apply { dry_run }) => apply::run(dry_run),
        None if cli.ui => run_tui(),
        None => run_scan(),
    }
}

fn ensure_config_exists() {
    if Path::new("warden.toml").exists() {
        return;
    }
    let content = project::generate_toml();
    if fs::write("warden.toml", &content).is_ok() {
        eprintln!("{}", "ğŸ“ Created warden.toml".dimmed());
    }
}

fn init_config() -> Result<()> {
    let content = project::generate_toml();
    fs::write("warden.toml", &content)?;
    println!("{}", "âœ“ Created warden.toml".green());
    Ok(())
}

fn handle_prompt(copy: bool) -> Result<()> {
    let prompt = PromptGenerator::generate();
    if copy {
        warden_core::clipboard::copy_to_clipboard(&prompt)?;
        println!("{}", "âœ“ Copied to clipboard".green());
    } else {
        println!("{prompt}");
    }
    Ok(())
}

fn run_command(name: &str) -> Result<()> {
    let mut config = Config::new();
    config.load_local_config();

    let Some(cmd_str) = config.commands.get(name) else {
        eprintln!(
            "{} No '{}' command configured in warden.toml",
            "error:".red(),
            name
        );
        process::exit(1);
    };

    println!("{} Running '{}': {}", "ğŸš€".green(), name, cmd_str.dimmed());

    let parts: Vec<&str> = cmd_str.split_whitespace().collect();
    let (prog, args) = parts.split_first().unwrap_or((&"", &[]));

    let status = Command::new(prog).args(args).status();

    match status {
        Ok(s) if s.success() => Ok(()),
        Ok(s) => {
            let code = s.code().unwrap_or(1);
            eprintln!("{} Command failed with exit code {code}", "âŒ".red());
            process::exit(code);
        }
        Err(e) => {
            handle_exec_error(&e, prog);
            process::exit(1);
        }
    }
}

fn handle_exec_error(e: &std::io::Error, prog: &str) {
    if e.kind() == io::ErrorKind::NotFound {
        eprintln!("{} Command not found: {prog}", "error:".red());
        eprintln!("  Check that the program is installed and in PATH");
    } else {
        eprintln!("{} Failed to execute: {e}", "error:".red());
    }
}

fn run_scan() -> Result<()> {
    let mut config = Config::new();
    config.load_local_config();

    let files = FileEnumerator::new(config.clone()).enumerate()?;
    let files = FileFilter::new(&config).filter(files);
    let files = HeuristicFilter::new(&config).filter(files);

    let engine = RuleEngine::new(config.rules.clone());
    let report = engine.analyze(&files)?;

    reporting::print_report(&report);

    if report.has_errors() {
        process::exit(1);
    }
    Ok(())
}

fn run_tui() -> Result<()> {
    let mut config = Config::new();
    config.load_local_config();

    let files = FileEnumerator::new(config.clone()).enumerate()?;
    let files = FileFilter::new(&config).filter(files);
    let files = HeuristicFilter::new(&config).filter(files);

    let engine = RuleEngine::new(config.rules.clone());
    let report = engine.analyze(&files)?;

    run_tui_with_report(report)
}

fn run_tui_with_report(report: ScanReport) -> Result<()> {
    use crossterm::{
        event::{DisableMouseCapture, EnableMouseCapture},
        execute,
        terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
    };
    use ratatui::backend::CrosstermBackend;
    use ratatui::Terminal;

    enable_raw_mode()?;
    let mut stdout = io::stdout();
    execute!(stdout, EnterAlternateScreen, EnableMouseCapture)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    let mut app = App::new(report);
    let res = app.run(&mut terminal);

    disable_raw_mode()?;
    execute!(
        terminal.backend_mut(),
        LeaveAlternateScreen,
        DisableMouseCapture
    )?;
    terminal.show_cursor()?;

    res
}
</file>

<file path="src/checks.rs">
// src/checks.rs
use crate::config::RuleConfig;
use crate::metrics;
use crate::types::Violation;
use anyhow::Result;
use tree_sitter::{Node, Query, QueryCursor, TreeCursor};

pub struct CheckContext<'a> {
    pub root: Node<'a>,
    pub source: &'a str,
    pub filename: &'a str,
    pub config: &'a RuleConfig,
}

/// Checks for naming violations.
pub fn check_naming(ctx: &CheckContext, query: &Query, out: &mut Vec<Violation>) {
    let mut cursor = QueryCursor::new();
    for m in cursor.matches(query, ctx.root, ctx.source.as_bytes()) {
        let node = m.captures[0].node;
        let name = node.utf8_text(ctx.source.as_bytes()).unwrap_or("?");

        if is_ignored(ctx.filename, &ctx.config.ignore_naming_on) {
            continue;
        }

        let word_count = count_words(name);
        if word_count > ctx.config.max_function_words {
            out.push(Violation {
                row: node.start_position().row,
                message: format!(
                    "Function '{name}' has {word_count} words (Max: {}). Is it doing too much?",
                    ctx.config.max_function_words
                ),
                law: "LAW OF BLUNTNESS",
            });
        }
    }
}

fn count_words(name: &str) -> usize {
    if name.contains('_') {
        name.split('_').count()
    } else {
        let caps = name.chars().filter(|c| c.is_uppercase()).count();
        if name.chars().next().is_some_and(char::is_uppercase) {
            caps
        } else {
            caps + 1
        }
    }
}

fn is_ignored(filename: &str, patterns: &[String]) -> bool {
    patterns.iter().any(|p| filename.contains(p))
}

/// Checks for safety violations.
pub fn check_safety(ctx: &CheckContext, _safety_query: &Query, out: &mut Vec<Violation>) {
    let _ = ctx;
    let _ = out;
}

/// Checks for complexity metrics.
pub fn check_metrics(ctx: &CheckContext, complexity_query: &Query, out: &mut Vec<Violation>) {
    traverse_nodes(ctx, |node| {
        if node.kind().contains("function") || node.kind().contains("method") {
            validate_arity(node, ctx.config.max_function_args, out);
            validate_depth(node, ctx.config.max_nesting_depth, out);
            validate_complexity(
                node,
                ctx.source,
                complexity_query,
                ctx.config.max_cyclomatic_complexity,
                out,
            );
        }
    });
}

fn validate_arity(node: Node, max: usize, out: &mut Vec<Violation>) {
    let args = metrics::count_arguments(node);
    if args > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!(
                "High Arity: Function takes {args} arguments (Max: {max}). Use a Struct."
            ),
            law: "LAW OF COMPLEXITY",
        });
    }
}

fn validate_depth(node: Node, max: usize, out: &mut Vec<Violation>) {
    let depth = metrics::calculate_max_depth(node);
    if depth > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!("Deep Nesting: Max depth is {depth} (Max: {max}). Extract logic."),
            law: "LAW OF COMPLEXITY",
        });
    }
}

fn validate_complexity(
    node: Node,
    source: &str,
    query: &Query,
    max: usize,
    out: &mut Vec<Violation>,
) {
    let score = metrics::calculate_complexity(node, source, query);
    if score > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!("High Complexity: Score is {score} (Max: {max}). Hard to test."),
            law: "LAW OF COMPLEXITY",
        });
    }
}

/// Checks for banned constructs.
/// # Errors
/// Returns `Ok`.
#[allow(clippy::unnecessary_wraps)]
pub fn check_banned(
    ctx: &CheckContext,
    banned_query: &Query,
    out: &mut Vec<Violation>,
) -> Result<()> {
    let mut cursor = QueryCursor::new();
    for m in cursor.matches(banned_query, ctx.root, ctx.source.as_bytes()) {
        let node = m.captures[0].node;
        out.push(Violation {
            row: node.start_position().row,
            message: "Explicit 'unwrap()' call detected. Use 'expect', 'unwrap_or', or '?'.".into(),
            law: "LAW OF PARANOIA",
        });
    }
    Ok(())
}

fn traverse_nodes<F>(ctx: &CheckContext, mut cb: F)
where
    F: FnMut(Node),
{
    let mut cursor = ctx.root.walk();
    loop {
        cb(cursor.node());
        if !step_cursor(&mut cursor) {
            break;
        }
    }
}

fn step_cursor(cursor: &mut TreeCursor) -> bool {
    if cursor.goto_first_child() {
        return true;
    }
    while !cursor.goto_next_sibling() {
        if !cursor.goto_parent() {
            return false;
        }
    }
    true
}
</file>

<file path="src/clipboard.rs">
// src/clipboard.rs
use anyhow::Result;
use std::process::Command;

/// Copies text to the system clipboard.
///
/// # Errors
/// Returns error if the system clipboard command fails.
pub fn copy_to_clipboard(text: &str) -> Result<()> {
    perform_copy(text)
}

/// Reads text from the system clipboard.
///
/// # Errors
/// Returns error if the system clipboard command fails.
pub fn read_clipboard() -> Result<String> {
    perform_read()
}

#[cfg(target_os = "macos")]
fn perform_copy(text: &str) -> Result<()> {
    use std::io::Write;
    let mut child = Command::new("pbcopy")
        .stdin(std::process::Stdio::piped())
        .spawn()?;
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(text.as_bytes())?;
    }
    child.wait()?;
    Ok(())
}

#[cfg(target_os = "macos")]
fn perform_read() -> Result<String> {
    let output = Command::new("pbpaste").output()?;
    Ok(String::from_utf8_lossy(&output.stdout).to_string())
}

#[cfg(target_os = "linux")]
fn perform_copy(text: &str) -> Result<()> {
    use std::io::Write;
    if let Ok(mut child) = Command::new("xclip")
        .args(["-selection", "clipboard", "-in"])
        .stdin(std::process::Stdio::piped())
        .spawn()
    {
        if let Some(mut stdin) = child.stdin.take() {
            stdin.write_all(text.as_bytes())?;
        }
        child.wait()?;
        return Ok(());
    }

    let mut child = Command::new("wl-copy")
        .stdin(std::process::Stdio::piped())
        .spawn()?;
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(text.as_bytes())?;
    }
    child.wait()?;
    Ok(())
}

#[cfg(target_os = "linux")]
fn perform_read() -> Result<String> {
    if let Ok(output) = Command::new("xclip")
        .args(["-selection", "clipboard", "-out"])
        .output()
    {
        return Ok(String::from_utf8_lossy(&output.stdout).to_string());
    }
    let output = Command::new("wl-paste").output()?;
    Ok(String::from_utf8_lossy(&output.stdout).to_string())
}

#[cfg(target_os = "windows")]
fn perform_copy(text: &str) -> Result<()> {
    // We use the arboard crate in the actual build, but if you want shell fallback:
    use std::io::Write;
    let mut child = Command::new("clip")
        .stdin(std::process::Stdio::piped())
        .spawn()?;
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(text.as_bytes())?;
    }
    child.wait()?;
    Ok(())
}

#[cfg(target_os = "windows")]
fn perform_read() -> Result<String> {
    // Powershell is slow but reliable without external deps
    let output = Command::new("powershell")
        .args(["-command", "Get-Clipboard"])
        .output()?;
    Ok(String::from_utf8_lossy(&output.stdout).to_string())
}
</file>

<file path="src/config.rs">
// src/config.rs
pub use crate::constants::{
    BIN_EXT_PATTERN, CODE_BARE_PATTERN, CODE_EXT_PATTERN, PRUNE_DIRS, SECRET_PATTERN,
};
use crate::error::Result;
use crate::project::{self, ProjectType};
use regex::Regex;
use serde::Deserialize;
use std::collections::HashMap;
use std::fs;
use std::path::Path;

#[derive(Debug, Clone, Deserialize)]
pub struct RuleConfig {
    #[serde(default = "default_max_tokens")]
    pub max_file_tokens: usize,
    #[serde(default = "default_max_complexity")]
    pub max_cyclomatic_complexity: usize,
    #[serde(default = "default_max_depth")]
    pub max_nesting_depth: usize,
    #[serde(default = "default_max_args")]
    pub max_function_args: usize,
    #[serde(default = "default_max_words")]
    pub max_function_words: usize,
    #[serde(default)]
    pub ignore_naming_on: Vec<String>,
}

impl Default for RuleConfig {
    fn default() -> Self {
        Self {
            max_file_tokens: default_max_tokens(),
            max_cyclomatic_complexity: default_max_complexity(),
            max_nesting_depth: default_max_depth(),
            max_function_args: default_max_args(),
            max_function_words: default_max_words(),
            ignore_naming_on: Vec::new(),
        }
    }
}

const fn default_max_tokens() -> usize {
    2000
}
const fn default_max_complexity() -> usize {
    5
}
const fn default_max_depth() -> usize {
    2
}
const fn default_max_args() -> usize {
    5
}
const fn default_max_words() -> usize {
    5
}

#[derive(Debug, Clone, Deserialize, Default)]
pub struct WardenToml {
    #[serde(default)]
    pub rules: RuleConfig,
    #[serde(default)]
    pub commands: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub enum GitMode {
    Auto,
    Yes,
    No,
}

#[derive(Debug, Clone)]
pub struct Config {
    pub git_mode: GitMode,
    pub include_patterns: Vec<Regex>,
    pub exclude_patterns: Vec<Regex>,
    pub code_only: bool,
    pub verbose: bool,
    pub rules: RuleConfig,
    pub commands: HashMap<String, String>,
}

impl Default for Config {
    fn default() -> Self {
        Self::new()
    }
}

impl Config {
    #[must_use]
    pub fn new() -> Self {
        Self {
            git_mode: GitMode::Auto,
            include_patterns: Vec::new(),
            exclude_patterns: Vec::new(),
            code_only: false,
            verbose: false,
            rules: RuleConfig::default(),
            commands: HashMap::new(),
        }
    }

    /// Validates configuration.
    /// # Errors
    /// Currently always returns Ok.
    pub fn validate(&self) -> Result<()> {
        Ok(())
    }

    pub fn load_local_config(&mut self) {
        self.load_ignore_file();
        self.load_toml_config();
        self.apply_project_defaults();
    }

    fn apply_project_defaults(&mut self) {
        if self.commands.contains_key("check") {
            return;
        }
        let defaults = project_defaults(ProjectType::detect());
        for (k, v) in defaults {
            self.commands.entry(k).or_insert(v);
        }
    }

    fn load_ignore_file(&mut self) {
        let Ok(content) = fs::read_to_string(".wardenignore") else {
            return;
        };
        for line in content.lines() {
            self.process_ignore_line(line);
        }
    }

    fn process_ignore_line(&mut self, line: &str) {
        let trimmed = line.trim();
        if trimmed.is_empty() || trimmed.starts_with('#') {
            return;
        }
        if let Ok(re) = Regex::new(trimmed) {
            self.exclude_patterns.push(re);
        }
    }

    fn load_toml_config(&mut self) {
        if !Path::new("warden.toml").exists() {
            return;
        }
        let Ok(content) = fs::read_to_string("warden.toml") else {
            return;
        };
        self.parse_toml(&content);
    }

    fn parse_toml(&mut self, content: &str) {
        let Ok(parsed) = toml::from_str::<WardenToml>(content) else {
            return;
        };
        self.rules = parsed.rules;
        self.commands = parsed.commands;
    }
}

fn project_defaults(project: ProjectType) -> HashMap<String, String> {
    let mut m = HashMap::new();
    match project {
        ProjectType::Rust => {
            m.insert(
                "check".into(),
                "cargo clippy --all-targets -- -D warnings -D clippy::pedantic".into(),
            );
            m.insert("fix".into(), "cargo fmt".into());
        }
        ProjectType::Node => {
            let npx = project::npx_cmd();
            m.insert("check".into(), format!("{npx} @biomejs/biome check src/"));
            m.insert(
                "fix".into(),
                format!("{npx} @biomejs/biome check --write src/"),
            );
        }
        ProjectType::Python => {
            m.insert("check".into(), "ruff check .".into());
            m.insert("fix".into(), "ruff check --fix .".into());
        }
        ProjectType::Unknown => {}
    }
    m
}
</file>

<file path="src/detection.rs">
// src/detection.rs
use crate::error::Result;
use std::collections::HashSet;
use std::fmt;
use std::path::Path;

#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy)]
pub enum BuildSystemType {
    Rust,
    Node,
    Python,
    Go,
    CMake,
    Conan,
}

impl fmt::Display for BuildSystemType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{self:?}")
    }
}

#[derive(Default)]
pub struct Detector;

impl Detector {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    /// Detects build systems.
    /// # Errors
    /// Returns `Ok`.
    pub fn detect_build_systems(
        &self,
        files: &[std::path::PathBuf],
    ) -> Result<Vec<BuildSystemType>> {
        let mut detected = HashSet::new();
        for file in files {
            check_file(file, &mut detected);
        }
        Ok(detected.into_iter().collect())
    }
}

fn check_file(path: &Path, set: &mut HashSet<BuildSystemType>) {
    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
        if check_cmake(path, set) {
            return;
        }
        check_common(name, set);
    }
}

fn check_cmake(path: &Path, set: &mut HashSet<BuildSystemType>) -> bool {
    if path
        .extension()
        .is_some_and(|e| e.eq_ignore_ascii_case("cmake"))
    {
        set.insert(BuildSystemType::CMake);
        return true;
    }
    false
}

const COMMON_CONFIGS: &[(&str, BuildSystemType)] = &[
    ("Cargo.toml", BuildSystemType::Rust),
    ("package.json", BuildSystemType::Node),
    ("requirements.txt", BuildSystemType::Python),
    ("pyproject.toml", BuildSystemType::Python),
    ("Pipfile", BuildSystemType::Python),
    ("go.mod", BuildSystemType::Go),
    ("CMakeLists.txt", BuildSystemType::CMake),
    ("conanfile.txt", BuildSystemType::Conan),
    ("conanfile.py", BuildSystemType::Conan),
];

fn check_common(name: &str, set: &mut HashSet<BuildSystemType>) {
    for (file, sys) in COMMON_CONFIGS {
        if name == *file {
            set.insert(*sys);
            return;
        }
    }
}
</file>

<file path="src/enumerate.rs">
// src/enumerate.rs
use crate::config::{Config, GitMode};
use crate::constants::should_prune;
use crate::error::{Result, WardenError};
use std::path::{Path, PathBuf};
use std::process::Command;
use walkdir::WalkDir;

pub struct FileEnumerator {
    config: Config,
}

impl FileEnumerator {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Enumerates files based on configuration.
    /// # Errors
    /// Returns error if git mode is Yes but not in a git repo.
    pub fn enumerate(&self) -> Result<Vec<PathBuf>> {
        match self.config.git_mode {
            GitMode::Yes => Self::enumerate_git_required(),
            GitMode::No => Ok(self.walk_filesystem()),
            GitMode::Auto => Ok(self.enumerate_auto()),
        }
    }

    fn enumerate_git_required() -> Result<Vec<PathBuf>> {
        if !in_git_repo() {
            return Err(WardenError::NotInGitRepo);
        }
        git_ls_files().map(filter_pruned)
    }

    fn enumerate_auto(&self) -> Vec<PathBuf> {
        if !in_git_repo() {
            return self.walk_filesystem();
        }
        git_ls_files().map_or_else(|_| self.walk_filesystem(), filter_pruned)
    }

    fn walk_filesystem(&self) -> Vec<PathBuf> {
        let walker = WalkDir::new(".")
            .follow_links(false)
            .into_iter()
            .filter_entry(|e| !should_prune(&e.file_name().to_string_lossy()));

        collect_files(walker, self.config.verbose)
    }
}

fn in_git_repo() -> bool {
    Command::new("git")
        .args(["rev-parse", "--is-inside-work-tree"])
        .output()
        .map(|o| o.status.success())
        .unwrap_or(false)
}

fn git_ls_files() -> Result<Vec<PathBuf>> {
    let out = Command::new("git")
        .args(["ls-files", "-z", "--exclude-standard", "."])
        .output()?;

    if !out.status.success() {
        return Err(WardenError::Other(format!(
            "git ls-files failed: {}",
            out.status
        )));
    }

    let paths = out
        .stdout
        .split(|&b| b == 0)
        .filter(|chunk| !chunk.is_empty())
        .map(|chunk| PathBuf::from(String::from_utf8_lossy(chunk).as_ref()))
        .collect();

    Ok(paths)
}

fn filter_pruned(paths: Vec<PathBuf>) -> Vec<PathBuf> {
    paths
        .into_iter()
        .filter(|p| !contains_pruned_component(p))
        .collect()
}

fn contains_pruned_component(path: &Path) -> bool {
    path.components()
        .filter_map(|c| c.as_os_str().to_str())
        .any(should_prune)
}

fn collect_files<I>(walker: I, verbose: bool) -> Vec<PathBuf>
where
    I: Iterator<Item = walkdir::Result<walkdir::DirEntry>>,
{
    let mut paths = Vec::new();
    let mut error_count = 0;

    for item in walker {
        match item {
            Ok(entry) if entry.file_type().is_file() => {
                let p = entry.path().strip_prefix(".").unwrap_or(entry.path());
                paths.push(p.to_path_buf());
            }
            Err(_) => error_count += 1,
            _ => {}
        }
    }

    if error_count > 0 && verbose {
        eprintln!("WARN: Encountered {error_count} errors during file walk");
    }

    paths
}
</file>

<file path="src/error.rs">
// src/error.rs
use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum WardenError {
    #[error("I/O error: {source} (path: {path})")]
    Io {
        source: std::io::Error,
        path: PathBuf,
    },

    #[error("Not inside a Git repository")]
    NotInGitRepo,

    #[error("Regex error: {0}")]
    Regex(#[from] regex::Error),

    #[error("Generic error: {0}")]
    Other(String),
}

pub type Result<T> = std::result::Result<T, WardenError>;

// Allow `?` on std::io::Error by converting to WardenError::Io with unknown path.
impl From<std::io::Error> for WardenError {
    fn from(source: std::io::Error) -> Self {
        WardenError::Io {
            source,
            path: PathBuf::from("<unknown>"),
        }
    }
}

// Gracefully convert WalkDir errors
impl From<walkdir::Error> for WardenError {
    fn from(e: walkdir::Error) -> Self {
        WardenError::Other(e.to_string())
    }
}
</file>

<file path="src/filter.rs">
// src/filter.rs
use crate::config::{Config, BIN_EXT_PATTERN, CODE_BARE_PATTERN, CODE_EXT_PATTERN, SECRET_PATTERN};
use crate::error::Result;
use regex::Regex;
use std::path::Path;

pub struct FileFilter {
    config: Config,
    bin_ext_re: Regex,
    secret_re: Regex,
    code_ext_re: Option<Regex>,
    code_bare_re: Option<Regex>,
}

impl FileFilter {
    /// Creates a new filter.
    /// # Errors
    /// Returns error on invalid regex.
    pub fn new(config: &Config) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            bin_ext_re: Regex::new(BIN_EXT_PATTERN)?,
            secret_re: Regex::new(SECRET_PATTERN)?,
            code_ext_re: if config.code_only {
                Some(Regex::new(CODE_EXT_PATTERN)?)
            } else {
                None
            },
            code_bare_re: if config.code_only {
                Some(Regex::new(CODE_BARE_PATTERN)?)
            } else {
                None
            },
        })
    }

    #[must_use]
    pub fn filter(&self, files: Vec<std::path::PathBuf>) -> Vec<std::path::PathBuf> {
        files.into_iter().filter(|p| self.should_keep(p)).collect()
    }

    fn should_keep(&self, path: &Path) -> bool {
        let s = path.to_string_lossy().replace('\\', "/");
        if self.is_ignored(&s) {
            return false;
        }
        if self.config.code_only && !self.is_code(&s) {
            return false;
        }
        self.is_included(&s)
    }

    fn is_ignored(&self, path: &str) -> bool {
        if self.secret_re.is_match(path) {
            return true;
        }
        if self.bin_ext_re.is_match(path) {
            return true;
        }
        if self
            .config
            .exclude_patterns
            .iter()
            .any(|p| p.is_match(path))
        {
            return true;
        }
        false
    }

    fn is_included(&self, path: &str) -> bool {
        self.config.include_patterns.is_empty()
            || self
                .config
                .include_patterns
                .iter()
                .any(|p| p.is_match(path))
    }

    fn is_code(&self, path: &str) -> bool {
        match (&self.code_ext_re, &self.code_bare_re) {
            (Some(ext), Some(bare)) => ext.is_match(path) || bare.is_match(path),
            _ => true,
        }
    }
}
</file>

<file path="src/heuristics.rs">
// warden:ignore
use crate::config::{CODE_BARE_PATTERN, CODE_EXT_PATTERN};
use regex::Regex;
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::sync::LazyLock;

// --- Configuration Constants for Heuristics ---
const MIN_TEXT_ENTROPY: f64 = 3.5;
const MAX_TEXT_ENTROPY: f64 = 5.5;

const BUILD_SYSTEM_PAMPS: &[&str] = &[
    "find_package",
    "add_executable",
    "target_link_libraries",
    "cmake_minimum_required",
    "project(",
    "add-apt-repository",
    "conanfile.py",
    "dependency",
    "require",
    "include",
    "import",
    "version",
    "dependencies",
];

// Pre-compiled regexes for known code files
static CODE_EXT_RE: LazyLock<Regex> = LazyLock::new(|| Regex::new(CODE_EXT_PATTERN).unwrap());
static CODE_BARE_RE: LazyLock<Regex> = LazyLock::new(|| Regex::new(CODE_BARE_PATTERN).unwrap());

pub struct HeuristicFilter;

impl HeuristicFilter {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    #[must_use]
    pub fn filter(&self, files: Vec<std::path::PathBuf>) -> Vec<std::path::PathBuf> {
        files
            .into_iter()
            .filter(|path| Self::should_keep(path))
            .collect()
    }

    fn should_keep(path: &Path) -> bool {
        let path_str = path.to_string_lossy();

        if CODE_EXT_RE.is_match(&path_str) || CODE_BARE_RE.is_match(&path_str) {
            return true;
        }

        if let Ok(entropy) = calculate_entropy(path) {
            if !(MIN_TEXT_ENTROPY..=MAX_TEXT_ENTROPY).contains(&entropy) {
                return false;
            }
        } else {
            return false;
        }

        if let Ok(content) = fs::read_to_string(path) {
            let lower_content = content.to_lowercase();
            for pamp in BUILD_SYSTEM_PAMPS {
                if lower_content.contains(pamp) {
                    return true;
                }
            }
        }

        true
    }
}

impl Default for HeuristicFilter {
    fn default() -> Self {
        Self::new()
    }
}

fn calculate_entropy(path: &Path) -> std::io::Result<f64> {
    let bytes = fs::read(path)?;
    if bytes.is_empty() {
        return Ok(0.0);
    }

    let mut freq_map = HashMap::new();
    for &byte in &bytes {
        *freq_map.entry(byte).or_insert(0) += 1;
    }

    // Suppress cast precision loss for 64-bit length; entropy approximation is fine.
    #[allow(clippy::cast_precision_loss)]
    let len = bytes.len() as f64;

    let entropy = freq_map.values().fold(0.0, |acc, &count| {
        let probability = f64::from(count) / len;
        acc - probability * probability.log2()
    });

    Ok(entropy)
}
</file>

<file path="src/lib.rs">
// src/lib.rs
pub mod analysis;
pub mod apply;
pub mod checks;
pub mod clipboard;
pub mod config;
pub mod constants;
pub mod detection;
pub mod enumerate;
pub mod error;
pub mod filter;
pub mod heuristics;
pub mod metrics;
pub mod project;
pub mod prompt;
pub mod reporting;
pub mod rules;
pub mod tokens;
pub mod tui;
pub mod types;
</file>

<file path="src/metrics.rs">
// warden:ignore
use tree_sitter::{Node, Query, QueryCursor};

/// Calculates the nesting depth of a node.
///
/// # Returns
/// The maximum depth of control structures within the node.
#[must_use]
pub fn calculate_max_depth(node: Node) -> usize {
    let mut max_depth = 0;
    let mut cursor = node.walk();

    // We start at 0 relative to function body
    for child in node.children(&mut cursor) {
        if child.kind().contains("block") || child.kind().contains("body") {
            max_depth = std::cmp::max(max_depth, walk_depth(child, 0));
        }
    }
    max_depth
}

fn walk_depth(node: Node, current: usize) -> usize {
    let mut max = current;
    let mut cursor = node.walk();

    for child in node.children(&mut cursor) {
        let kind = child.kind();
        // Uses matches! macro to reduce Cyclomatic Complexity score
        // (This replaces the massive if/else chain)
        if matches!(
            kind,
            "if_expression"
                | "match_expression"
                | "for_expression"
                | "while_expression"
                | "loop_expression"
                | "if_statement"
                | "for_statement"
                | "for_in_statement"
                | "while_statement"
                | "do_statement"
                | "switch_case"
                | "catch_clause"
                | "try_statement"
        ) {
            max = std::cmp::max(max, walk_depth(child, current + 1));
        } else {
            max = std::cmp::max(max, walk_depth(child, current));
        }
    }
    max
}

/// Calculates `McCabe` Cyclomatic Complexity.
#[must_use]
pub fn calculate_complexity(node: Node, source: &str, query: &Query) -> usize {
    let mut cursor = QueryCursor::new();
    // Base complexity is 1
    let mut complexity = 1;
    for _ in cursor.matches(query, node, source.as_bytes()) {
        complexity += 1;
    }
    complexity
}

/// Counts named arguments/parameters.
#[must_use]
pub fn count_arguments(node: Node) -> usize {
    let mut cursor = node.walk();
    for child in node.children(&mut cursor) {
        if child.kind().contains("parameter") || child.kind().contains("argument") {
            return child.named_child_count();
        }
    }
    0
}
</file>

<file path="src/prompt.rs">
// src/prompt.rs
use crate::config::RuleConfig;
use anyhow::Result;

pub struct PromptGenerator {
    config: RuleConfig,
}

impl PromptGenerator {
    #[must_use]
    pub fn new(config: RuleConfig) -> Self {
        Self { config }
    }

    /// Generates the full system prompt.
    ///
    /// # Errors
    /// This function does not currently error but returns Result for API consistency.
    pub fn generate(&self) -> Result<String> {
        Ok(self.build_system_prompt())
    }

    /// Wraps the header for context packs.
    ///
    /// # Errors
    /// This function does not currently error but returns Result for API consistency.
    pub fn wrap_header(&self) -> Result<String> {
        Ok(self.build_system_prompt())
    }

    /// Generates a short reminder prompt.
    ///
    /// # Errors
    /// This function does not currently error but returns Result for API consistency.
    pub fn generate_reminder(&self) -> Result<String> {
        Ok(self.build_reminder())
    }

    fn build_system_prompt(&self) -> String {
        let tokens = self.config.max_file_tokens;
        let complexity = self.config.max_cyclomatic_complexity;
        let depth = self.config.max_nesting_depth;
        let args = self.config.max_function_args;
        let output_format = build_output_format();

        format!(
            r"ğŸ›¡ï¸ SYSTEM MANDATE: THE WARDEN PROTOCOL
ROLE: High-Integrity Systems Architect (NASA/JPL Standard).
CONTEXT: You are coding inside a strict environment enforced by Warden.

THE 3 LAWS (Non-Negotiable):

1. LAW OF ATOMICITY
   - Files: MUST be < {tokens} tokens.
   - Action: Split immediately if larger.

2. LAW OF COMPLEXITY
   - Cyclomatic Complexity: MUST be â‰¤ {complexity} per function.
   - Nesting Depth: MUST be â‰¤ {depth} levels.
   - Function Arguments: MUST be â‰¤ {args} parameters.

3. LAW OF PARANOIA
   - Use Result<T, E> for I/O and fallible operations.
   - NO .unwrap() or .expect() calls.

{output_format}
"
        )
    }

    fn build_reminder(&self) -> String {
        let tokens = self.config.max_file_tokens;
        let complexity = self.config.max_cyclomatic_complexity;
        let depth = self.config.max_nesting_depth;
        let args = self.config.max_function_args;

        format!(
            r"WARDEN CONSTRAINTS:
â–¡ Files < {tokens} tokens
â–¡ Complexity â‰¤ {complexity}
â–¡ Nesting â‰¤ {depth}
â–¡ Args â‰¤ {args}
â–¡ No .unwrap()
â–¡ Use <delivery> + <file> tags"
        )
    }
}

fn build_output_format() -> &'static str {
    r"OUTPUT FORMAT (MANDATORY):

When providing code files, use this exact format:

1. Declare ALL files first:

<delivery>
path/to/file1.rs
path/to/file2.rs [NEW]
</delivery>

2. Provide EACH file:

<file path='path/to/file1.rs'>
[complete file contents]
</file>

RULES:
- Every file in <delivery> MUST have a matching <file> block
- Do NOT use markdown code blocks - use <file> tags only
- Do NOT truncate files
- Paths must match exactly

The `warden apply` command will REJECT incomplete deliveries."
}
</file>

<file path="src/reporting.rs">
// src/reporting.rs
use crate::types::{FileReport, ScanReport, Violation};
use anyhow::Result;
use colored::Colorize;

/// Prints the scan report to stdout.
///
/// # Errors
/// Returns Ok(()) normally.
pub fn print_report(report: &ScanReport) -> Result<()> {
    let failures = count_failures(report);

    // Filter and print only violating files
    report
        .files
        .iter()
        .filter(|f| !f.is_clean())
        .for_each(print_file_report);

    print_summary(report, failures);
    Ok(())
}

fn count_failures(report: &ScanReport) -> usize {
    report
        .files
        .iter()
        .filter(|f| !f.is_clean())
        .map(|f| f.violations.len())
        .sum()
}

fn print_file_report(file: &FileReport) {
    for v in &file.violations {
        print_violation(&file.path, v);
    }
}

fn print_violation(path: &std::path::Path, v: &Violation) {
    let filename = path.to_string_lossy();
    let line_num = v.row + 1;

    println!("{}: {}", "error".red().bold(), v.message.bold());
    println!("  {} {}:{}:1", "-->".blue(), filename, line_num);
    println!("   {}", "|".blue());
    println!(
        "   {} {}: Action required",
        "=".blue().bold(),
        v.law.white().bold()
    );
    println!();
}

fn print_summary(report: &ScanReport, failures: usize) {
    if failures > 0 {
        let msg = format!(
            "âŒ Warden found {failures} violations in {}ms.",
            report.duration_ms
        );
        println!("{}", msg.red().bold());
    } else {
        let msg = format!(
            "âœ… All Clear. Scanned {} tokens in {}ms.",
            report.total_tokens, report.duration_ms
        );
        println!("{}", msg.green().bold());
    }
}
</file>

<file path="src/rules.rs">
use crate::analysis::Analyzer;
use crate::config::Config;
use crate::tokens::Tokenizer;
use crate::types::{FileReport, ScanReport, Violation};
use rayon::prelude::*;
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::LazyLock;
use std::time::Instant;

static ANALYZER: LazyLock<Analyzer> = LazyLock::new(Analyzer::new);

pub struct RuleEngine {
    config: Config,
}

impl RuleEngine {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Scans a list of files and returns a structured report.
    ///
    /// # Errors
    ///
    /// Returns error if Rayon thread pool fails (unlikely).
    #[must_use]
    pub fn scan(&self, files: Vec<PathBuf>) -> ScanReport {
        let start = Instant::now();

        // Parallel scan
        let results: Vec<FileReport> = files
            .into_par_iter()
            .filter_map(|path| self.analyze_file(&path))
            .collect();

        let total_tokens = results.iter().map(|f| f.token_count).sum();
        let total_violations = results.iter().map(|f| f.violations.len()).sum();

        ScanReport {
            files: results,
            total_tokens,
            total_violations,
            duration_ms: start.elapsed().as_millis(),
        }
    }

    fn analyze_file(&self, path: &Path) -> Option<FileReport> {
        let content = fs::read_to_string(path).ok()?;

        if content.contains("// warden:ignore") || content.contains("# warden:ignore") {
            return None;
        }

        let filename = path.to_string_lossy();
        let mut violations = Vec::new();
        let token_count = Tokenizer::count(&content);

        // 1. Law of Atomicity
        if token_count > self.config.rules.max_file_tokens {
            violations.push(Violation {
                row: 0,
                message: format!(
                    "File size is {token_count} tokens (Limit: {})",
                    self.config.rules.max_file_tokens
                ),
                law: "LAW OF ATOMICITY",
            });
        }

        // 2. AST Analysis
        if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
            let mut analysis_violations =
                ANALYZER.analyze(ext, &filename, &content, &self.config.rules);
            violations.append(&mut analysis_violations);
        }

        Some(FileReport {
            path: path.to_path_buf(),
            token_count,
            complexity_score: 0, // Future: aggregate function complexity here
            violations,
        })
    }
}
</file>

<file path="src/tokens.rs">
use std::sync::LazyLock;
use tiktoken_rs::CoreBPE;

// We use cl100k_base (GPT-4/3.5 turbo encoding) as the standard
static BPE: LazyLock<CoreBPE> =
    LazyLock::new(|| tiktoken_rs::cl100k_base().expect("Failed to load cl100k_base dictionary"));

pub struct Tokenizer;

impl Tokenizer {
    #[must_use]
    pub fn count(text: &str) -> usize {
        // EncodeOrdinary is faster as it ignores special tokens, which is fine for code
        BPE.encode_ordinary(text).len()
    }

    /// Returns true if the file exceeds the token limit
    #[must_use]
    pub fn exceeds_limit(text: &str, limit: usize) -> bool {
        Self::count(text) > limit
    }
}
</file>

<file path="src/tui/mod.rs">
pub mod state;
pub mod view;
</file>

<file path="src/tui/state.rs">
// src/tui/state.rs
use crate::types::{FileReport, ScanReport};
use anyhow::Result;
use crossterm::event::{self, Event, KeyCode};
use std::time::Duration;

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum SortMode {
    Path,
    Tokens,
    Violations,
}

pub struct App {
    pub report: ScanReport,
    pub view_indices: Vec<usize>,
    pub selected_index: usize,
    pub running: bool,
    pub sort_mode: SortMode,
    pub only_violations: bool,
}

impl App {
    #[must_use]
    pub fn new(report: ScanReport) -> Self {
        let mut app = Self {
            report,
            view_indices: Vec::new(),
            selected_index: 0,
            running: true,
            sort_mode: SortMode::Path,
            only_violations: false,
        };
        app.update_view();
        app
    }

    fn update_view(&mut self) {
        let mut indices: Vec<usize> = self
            .report
            .files
            .iter()
            .enumerate()
            .filter(|(_, f)| !self.only_violations || !f.is_clean())
            .map(|(i, _)| i)
            .collect();

        self.sort_indices(&mut indices);
        self.view_indices = indices;
        self.clamp_selection();
    }

    fn sort_indices(&self, indices: &mut [usize]) {
        let files = &self.report.files;
        indices.sort_by(|&a, &b| {
            let f1 = &files[a];
            let f2 = &files[b];
            match self.sort_mode {
                SortMode::Path => f1.path.cmp(&f2.path),
                SortMode::Tokens => f2.token_count.cmp(&f1.token_count),
                SortMode::Violations => f2.violations.len().cmp(&f1.violations.len()),
            }
        });
    }

    fn clamp_selection(&mut self) {
        if self.view_indices.is_empty() {
            self.selected_index = 0;
        } else if self.selected_index >= self.view_indices.len() {
            self.selected_index = self.view_indices.len() - 1;
        }
    }

    /// Runs TUI loop.
    /// # Errors
    /// Returns error on IO failure.
    pub fn run<B: ratatui::backend::Backend>(
        &mut self,
        terminal: &mut ratatui::Terminal<B>,
    ) -> Result<()> {
        while self.running {
            terminal.draw(|f| crate::tui::view::draw(f, self))?;
            self.process_event()?;
        }
        Ok(())
    }

    fn process_event(&mut self) -> Result<()> {
        if event::poll(Duration::from_millis(100))? {
            if let Event::Key(key) = event::read()? {
                self.handle_input(key.code);
            }
        }
        Ok(())
    }

    fn handle_input(&mut self, code: KeyCode) {
        if self.handle_nav(code) {
            return;
        }
        if self.handle_quit(code) {
            return;
        }
        self.handle_toggles(code);
    }

    fn handle_nav(&mut self, code: KeyCode) -> bool {
        match code {
            KeyCode::Up | KeyCode::Char('k') => {
                self.move_up();
                true
            }
            KeyCode::Down | KeyCode::Char('j') => {
                self.move_down();
                true
            }
            _ => false,
        }
    }

    fn handle_quit(&mut self, code: KeyCode) -> bool {
        if matches!(code, KeyCode::Char('q') | KeyCode::Esc) {
            self.running = false;
            return true;
        }
        false
    }

    fn handle_toggles(&mut self, code: KeyCode) {
        match code {
            KeyCode::Char('s') => self.cycle_sort(),
            KeyCode::Char('f') => self.toggle_filter(),
            _ => {}
        }
    }

    fn move_up(&mut self) {
        if self.selected_index > 0 {
            self.selected_index -= 1;
        }
    }

    fn move_down(&mut self) {
        if !self.view_indices.is_empty() && self.selected_index < self.view_indices.len() - 1 {
            self.selected_index += 1;
        }
    }

    fn cycle_sort(&mut self) {
        self.sort_mode = match self.sort_mode {
            SortMode::Path => SortMode::Tokens,
            SortMode::Tokens => SortMode::Violations,
            SortMode::Violations => SortMode::Path,
        };
        self.update_view();
    }

    fn toggle_filter(&mut self) {
        self.only_violations = !self.only_violations;
        self.update_view();
    }

    #[must_use]
    pub fn get_selected_file(&self) -> Option<&FileReport> {
        if let Some(&real_index) = self.view_indices.get(self.selected_index) {
            self.report.files.get(real_index)
        } else {
            None
        }
    }
}
</file>

<file path="src/tui/view/components.rs">
// src/tui/view/components.rs
use crate::tui::state::App;
use crate::types::FileReport;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Gauge, List, ListItem, Paragraph};
use ratatui::Frame;

pub fn draw_file_list(f: &mut Frame, app: &App, area: Rect) {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" ğŸ“‚ File List ");
    let items = build_list_items(app);

    let list = List::new(items).block(block).highlight_style(
        Style::default()
            .bg(Color::DarkGray)
            .add_modifier(Modifier::BOLD),
    );

    let mut state = ratatui::widgets::ListState::default();
    state.select(Some(app.selected_index));
    f.render_stateful_widget(list, area, &mut state);
}

fn build_list_items(app: &App) -> Vec<ListItem<'_>> {
    app.view_indices
        .iter()
        .map(|&idx| {
            let file = &app.report.files[idx];
            create_list_item(file)
        })
        .collect()
}

fn create_list_item(file: &FileReport) -> ListItem<'_> {
    let name = file.path.to_string_lossy();
    let is_clean = file.is_clean();
    let (color, icon) = if !is_clean {
        (Color::Red, "!")
    } else if file.token_count > 1000 {
        (Color::Yellow, "âœ“")
    } else {
        (Color::Green, "âœ“")
    };

    let bars = (file.token_count / 200).clamp(0, 10);
    let bar_vis = "I".repeat(bars);

    let content = Line::from(vec![
        Span::styled(
            format!("{icon} "),
            Style::default().fg(color).add_modifier(Modifier::BOLD),
        ),
        Span::raw(format!("{name:<30} ")),
        Span::styled(
            format!("{bar_vis:<10}"),
            Style::default().fg(Color::DarkGray),
        ),
    ]);
    ListItem::new(content)
}

#[allow(clippy::cast_precision_loss)]
pub fn draw_inspector(f: &mut Frame, app: &App, area: Rect) {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" ğŸ•µï¸ Inspector ");
    let inner = block.inner(area);
    f.render_widget(block, area);

    if let Some(file) = app.get_selected_file() {
        let layout = Layout::default()
            .direction(Direction::Vertical)
            .constraints(
                [
                    Constraint::Length(2),
                    Constraint::Length(6),
                    Constraint::Min(5),
                ]
                .as_ref(),
            )
            .split(inner);

        draw_header(f, file, layout[0]);
        draw_stats(f, file, layout[1]);
        draw_issues(f, file, layout[2]);
    } else {
        f.render_widget(
            Paragraph::new("No file selected").alignment(Alignment::Center),
            inner,
        );
    }
}

fn draw_header(f: &mut Frame, file: &FileReport, area: Rect) {
    let header = Paragraph::new(Line::from(vec![
        Span::styled("TARGET: ", Style::default().fg(Color::DarkGray)),
        Span::styled(
            file.path.to_string_lossy(),
            Style::default().add_modifier(Modifier::BOLD),
        ),
    ]));
    f.render_widget(header, area);
}

#[allow(clippy::cast_precision_loss)]
fn draw_stats(f: &mut Frame, file: &FileReport, area: Rect) {
    let chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(50), Constraint::Percentage(50)].as_ref())
        .split(area);

    let t_ratio = (file.token_count as f64 / 2000.0).clamp(0.0, 1.0);
    let t_gauge = Gauge::default()
        .block(Block::default().borders(Borders::NONE).title("Size"))
        .gauge_style(Style::default().fg(if t_ratio > 0.8 {
            Color::Red
        } else {
            Color::Green
        }))
        .ratio(t_ratio)
        .label(format!("{} toks", file.token_count));
    f.render_widget(t_gauge, chunks[0]);

    let v_count = file.violations.len();
    let v_ratio = (v_count as f64 / 5.0).clamp(0.0, 1.0);
    let v_gauge = Gauge::default()
        .block(Block::default().borders(Borders::NONE).title("Issues"))
        .gauge_style(Style::default().fg(if v_count > 0 {
            Color::Red
        } else {
            Color::Green
        }))
        .ratio(v_ratio)
        .label(format!("{v_count} Found"));
    f.render_widget(v_gauge, chunks[1]);
}

fn draw_issues(f: &mut Frame, file: &FileReport, area: Rect) {
    if file.is_clean() {
        let p = Paragraph::new("âœ¨ Clean.")
            .style(Style::default().fg(Color::Green))
            .alignment(Alignment::Center);
        f.render_widget(p, area);
        return;
    }

    let items: Vec<ListItem> = file
        .violations
        .iter()
        .map(|v| {
            let header = Line::from(vec![
                Span::styled(
                    format!("[{}] ", v.law),
                    Style::default().fg(Color::Red).add_modifier(Modifier::BOLD),
                ),
                Span::raw(format!("Line {}", v.row + 1)),
            ]);
            let msg = Line::from(Span::styled(
                format!("  â””â”€ {}", v.message),
                Style::default().fg(Color::White),
            ));
            ListItem::new(vec![header, msg, Line::from("")])
        })
        .collect();

    let list = List::new(items).block(Block::default().borders(Borders::TOP).title(" Violations "));
    f.render_widget(list, area);
}
</file>

<file path="src/tui/view/layout.rs">
// src/tui/view/layout.rs
use crate::tui::state::{App, SortMode};
use crate::tui::view::components;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Paragraph};
use ratatui::Frame;

pub fn render_dashboard(f: &mut Frame, app: &App, area: Rect) {
    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints(
            [
                Constraint::Length(3),
                Constraint::Min(10),
                Constraint::Length(1),
            ]
            .as_ref(),
        )
        .split(area);

    draw_header(f, app, chunks[0]);
    draw_main(f, app, chunks[1]);
    draw_footer(f, chunks[2]);
}

#[allow(clippy::cast_precision_loss)]
fn draw_header(f: &mut Frame, app: &App, area: Rect) {
    let (clean_count, total) = count_stats(app);
    let health = if total > 0 {
        (clean_count as f64 / total as f64) * 100.0
    } else {
        100.0
    };

    let info = build_info_string(app, total);
    let line = build_header_line(health, &info);

    f.render_widget(
        Paragraph::new(line)
            .block(Block::default().borders(Borders::ALL))
            .alignment(Alignment::Center),
        area,
    );
}

fn count_stats(app: &App) -> (usize, usize) {
    (
        app.report.files.iter().filter(|f| f.is_clean()).count(),
        app.report.files.len(),
    )
}

fn get_health_color(health: f64) -> Color {
    if health > 90.0 {
        return Color::Green;
    }
    if health > 70.0 {
        return Color::Yellow;
    }
    Color::Red
}

fn build_info_string(app: &App, total: usize) -> String {
    let sort_str = get_sort_label(app.sort_mode);
    let filter_str = get_filter_label(app.only_violations);
    format!(" FILES: {total} | SORT: {sort_str}{filter_str} ")
}

fn get_sort_label(mode: SortMode) -> &'static str {
    match mode {
        SortMode::Path => "NAME",
        SortMode::Tokens => "SIZE",
        SortMode::Violations => "ERRORS",
    }
}

fn get_filter_label(active: bool) -> &'static str {
    if active {
        " | FILTER: ERRORS"
    } else {
        ""
    }
}

fn build_header_line(health: f64, info: &str) -> Line<'_> {
    Line::from(vec![
        Span::styled(
            " ğŸ›¡ï¸ WARDEN PROTOCOL ",
            Style::default()
                .fg(Color::Cyan)
                .add_modifier(Modifier::BOLD),
        ),
        Span::raw(" | "),
        Span::styled(
            format!("HEALTH: {health:.1}%"),
            Style::default().fg(get_health_color(health)),
        ),
        Span::raw(" |"),
        Span::raw(info),
    ])
}

fn draw_main(f: &mut Frame, app: &App, area: Rect) {
    let chunks = get_main_chunks(area);
    components::draw_file_list(f, app, chunks[0]);
    components::draw_inspector(f, app, chunks[1]);
}

fn get_main_chunks(area: Rect) -> std::rc::Rc<[Rect]> {
    Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(40), Constraint::Percentage(60)].as_ref())
        .split(area)
}

fn draw_footer(f: &mut Frame, area: Rect) {
    let text = " [s] Sort Mode | [f] Filter Errors | [j/k] Navigate | [q] Quit ";
    f.render_widget(
        Paragraph::new(text).style(Style::default().fg(Color::DarkGray).bg(Color::Black)),
        area,
    );
}
</file>

<file path="src/tui/view/mod.rs">
// src/tui/view/mod.rs
pub mod components;
pub mod layout;

use crate::tui::state::App;
use ratatui::Frame;

pub fn draw(f: &mut Frame, app: &App) {
    let area = f.area();
    layout::render_dashboard(f, app, area);
}
</file>

<file path="src/types.rs">
use std::path::PathBuf;

#[derive(Debug, Clone)]
pub struct Violation {
    pub row: usize,
    pub message: String,
    pub law: &'static str,
}

#[derive(Debug, Clone)]
pub struct FileReport {
    pub path: PathBuf,
    pub token_count: usize,
    pub complexity_score: usize,
    pub violations: Vec<Violation>,
}

impl FileReport {
    #[must_use]
    pub fn is_clean(&self) -> bool {
        self.violations.is_empty()
    }
}

#[derive(Debug, Clone, Default)]
pub struct ScanReport {
    pub files: Vec<FileReport>,
    pub total_tokens: usize,
    pub total_violations: usize,
    pub duration_ms: u128,
}
</file>

<file path="warden.toml">
# warden.toml
[rules]
max_file_tokens = 2000
max_cyclomatic_complexity = 5
max_nesting_depth = 2
max_function_args = 5
max_function_words = 5
ignore_naming_on = ["tests", "spec"]

[commands]
check = "cargo clippy --all-targets -- -D warnings -D clippy::pedantic"
fix = "cargo fmt"
</file>


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
END CODEBASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WARDEN CONSTRAINTS:
â–¡ Files < 2000 tokens
â–¡ Complexity â‰¤ 5
â–¡ Nesting â‰¤ 2
â–¡ Args â‰¤ 5
â–¡ No .unwrap()
â–¡ Use <delivery> + <file> tags
