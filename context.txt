ğŸ›¡ï¸ SYSTEM MANDATE: THE WARDEN PROTOCOL
ROLE: High-Integrity Systems Architect (NASA/JPL Standard).
CONTEXT: You are coding inside a strict environment enforced by Warden.

THE 3 LAWS (Non-Negotiable):

1. LAW OF ATOMICITY
   - Files: MUST be < 2000 tokens.
   - Action: Split immediately if larger.

2. LAW OF COMPLEXITY
   - Cyclomatic Complexity: MUST be â‰¤ 8 per function.
   - Nesting Depth: MUST be â‰¤ 3 levels.
   - Function Arguments: MUST be â‰¤ 5 parameters.

3. LAW OF PARANOIA
   - Use Result<T, E> for I/O and fallible operations.
   - NO .unwrap() or .expect() calls.

OUTPUT FORMAT (MANDATORY):

1. Explain the changes (Technical Plan) using NABLA PROTOCOL:
   - Must start with "GOAL:"
   - Must include "CHANGES:" list

âˆ‡âˆ‡âˆ‡ PLAN âˆ‡âˆ‡âˆ‡
GOAL: Refactor authentication module.
CHANGES:
1. Extract user validation to new file.
2. Update config parser.
âˆ†âˆ†âˆ†

2. Declare the plan (Manifest) using NABLA PROTOCOL:

âˆ‡âˆ‡âˆ‡ MANIFEST âˆ‡âˆ‡âˆ‡
path/to/file1.rs
path/to/file2.rs [NEW]
âˆ†âˆ†âˆ†

3. Provide EACH file using NABLA PROTOCOL:

âˆ‡âˆ‡âˆ‡ path/to/file1.rs âˆ‡âˆ‡âˆ‡
[file content]
âˆ†âˆ†âˆ†

RULES:
- Do NOT use markdown code blocks (e.g. triple backticks) to wrap the file. The âˆ‡âˆ‡âˆ‡ delimiters ARE the fence.
- You MAY use markdown inside the file content.
- Every file in the manifest MUST have a matching âˆ‡âˆ‡âˆ‡ block.
- Paths must match exactly.
- Do NOT truncate files (No "// ...").


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BEGIN CODEBASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸  ACTIVE VIOLATIONS (PRIORITY FIX REQUIRED)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FILE: DESIGN.md
LAW:  LAW OF ATOMICITY
LINE: 1
ERR:  File size is 6895 tokens (Limit: 2000)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FILE: ROADMAP.md
LAW:  LAW OF ATOMICITY
LINE: 1
ERR:  File size is 8654 tokens (Limit: 2000)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âˆ‡âˆ‡âˆ‡ .wardenignore âˆ‡âˆ‡âˆ‡
.warden_apply_backup
target
node_modules
.git

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ Cargo.toml âˆ‡âˆ‡âˆ‡
[package]
name = "warden"
version = "0.7.0"
edition = "2021"

[lib]
name = "warden_core"
path = "src/lib.rs"

[[bin]]
name = "warden"
path = "src/bin/warden.rs"

[dependencies]
anyhow = "1.0"
thiserror = "1.0"
regex = "1.10"
walkdir = "2.5"
clap = { version = "4.5", features = ["derive"] }
colored = "2.1"
rayon = "1.10"
serde = { version = "1.0", features = ["derive", "rc"] }
toml = "0.8"

# THE BRAINS
tiktoken-rs = "0.5"

# UI / TUI
ratatui = "0.29"
crossterm = "0.28"

# Structural Parsing
tree-sitter = "0.20"
tree-sitter-rust = "0.20"
tree-sitter-python = "0.20"
tree-sitter-typescript = "0.20"

[dev-dependencies]
tempfile = "3.10"
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ DESIGN.md âˆ‡âˆ‡âˆ‡
# Warden Design Document

> **Audience:** Developers (human or AI) working on or extending Warden.  
> **See also:** [README.md](README.md) for user guide, [ROADMAP.md](ROADMAP.md) for feature tracking.

---

## Table of Contents

1. [Vision & Philosophy](#vision--philosophy)
2. [Architecture Overview](#architecture-overview)
3. [The Three Laws](#the-three-laws)
4. [The Nabla Protocol](#the-nabla-protocol)
5. [Analysis Engine](#analysis-engine)
6. [Apply System](#apply-system)
7. [Pack & Context System](#pack--context-system)
8. [Smart Context (v0.8-0.9)](#smart-context-v08-09)
9. [Roadmap System](#roadmap-system)
10. [Security Model](#security-model)
11. [Key Decisions & Rationale](#key-decisions--rationale)
12. [Module Map](#module-map)
13. [Testing Philosophy](#testing-philosophy)
14. [Future Considerations](#future-considerations)

---

## Vision & Philosophy

### The Problem

AI coding assistants are powerful but unreliable. They:
- Generate files too large to review meaningfully
- Produce complex functions that can't be tested in isolation
- Truncate code with `// ...` or "rest of implementation"
- Escape markdown fences incorrectly, corrupting output
- Have no memory of project constraints between sessions

Developers end up manually reviewing every line, defeating the productivity gains.

### The Solution

**Warden is a gatekeeper, not a fixer.** It creates a feedback loop:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   warden pack â”€â”€â–º AI â”€â”€â–º warden apply â”€â”€â–º verify â”€â”€â–º commit    â”‚
â”‚        â–²                      â”‚                                 â”‚
â”‚        â”‚                      â–¼                                 â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€ rejection â—„â”€â”€ FAIL                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

When AI output violates constraints:
1. Warden rejects the entire response
2. Generates a structured error message
3. Copies it to clipboard for pasting back to AI
4. AI corrects and resubmits

**The AI learns the constraints through rejection, not instruction.**

### Core Principles

| # | Principle | Meaning |
|---|-----------|---------|
| 1 | **Every feature has a verified test** | No exceptions. The roadmap enforces this. |
| 2 | **Reject bad input, don't fix it** | Warden is a gatekeeper, not a linter with autofix. |
| 3 | **Git is the undo system** | Don't reinvent version control. Commit on success. |
| 4 | **Explicit > Magic** | Fail loudly on format violations. |
| 5 | **Containment over craftsmanship** | Constraints are safety, not style. |
| 6 | **Self-hosting** | Warden passes its own rules. |
| 7 | **Context is king** | Give AI exactly what it needs, nothing more. |
| 8 | **Graph over glob** | Understand structure, don't just pattern match. |
| 9 | **Errors are context** | Parse failures to understand scope. |

### What Warden Is NOT

- **Not a linter** â€” It doesn't suggest fixes, it rejects
- **Not an IDE plugin** â€” It's CLI-first, composable with any editor
- **Not AI-specific** â€” The constraints help human reviewers too
- **Not prescriptive about style** â€” It cares about size and complexity, not formatting

---

## Architecture Overview

```
src/
â”œâ”€â”€ analysis/          # The Three Laws enforcement (tree-sitter)
â”‚   â”œâ”€â”€ ast.rs         # Language-specific query compilation
â”‚   â”œâ”€â”€ checks.rs      # Violation detection logic
â”‚   â”œâ”€â”€ metrics.rs     # Complexity, depth, arity calculations
â”‚   â””â”€â”€ mod.rs         # RuleEngine orchestration
â”‚
â”œâ”€â”€ apply/             # AI response â†’ filesystem
â”‚   â”œâ”€â”€ extractor.rs   # Nabla format parsing
â”‚   â”œâ”€â”€ manifest.rs    # MANIFEST block parsing
â”‚   â”œâ”€â”€ validator.rs   # Path safety, truncation detection
â”‚   â”œâ”€â”€ writer.rs      # Atomic file writes with backup
â”‚   â”œâ”€â”€ verification.rs# Post-apply check commands
â”‚   â”œâ”€â”€ git.rs         # Commit and push automation
â”‚   â””â”€â”€ mod.rs         # Orchestration and flow control
â”‚
â”œâ”€â”€ graph/             # Dependency analysis (partially implemented)
â”‚   â”œâ”€â”€ imports.rs     # Import extraction per language
â”‚   â”œâ”€â”€ resolver.rs    # Import â†’ file path resolution
â”‚   â””â”€â”€ mod.rs
â”‚
â”œâ”€â”€ pack/              # Context generation for AI
â”‚   â””â”€â”€ mod.rs         # File discovery, skeleton, Nabla output
â”‚
â”œâ”€â”€ roadmap/           # Programmatic roadmap management
â”‚   â”œâ”€â”€ parser.rs      # Markdown â†’ structured data
â”‚   â”œâ”€â”€ commands.rs    # CHECK, ADD, UPDATE, etc.
â”‚   â”œâ”€â”€ audit/         # Test traceability verification
â”‚   â””â”€â”€ cli.rs         # Subcommand handlers
â”‚
â”œâ”€â”€ skeleton/          # Code compression (full â†’ signatures)
â”‚   â””â”€â”€ mod.rs         # Tree-sitter body removal
â”‚
â”œâ”€â”€ tui/               # Interactive dashboard
â”‚   â”œâ”€â”€ state.rs       # App state management
â”‚   â””â”€â”€ view/          # Ratatui rendering
â”‚
â”œâ”€â”€ config.rs          # warden.toml loading
â”œâ”€â”€ discovery.rs       # File enumeration (git + walk)
â”œâ”€â”€ tokens.rs          # tiktoken integration
â”œâ”€â”€ types.rs           # Shared types (Violation, FileReport, etc.)
â”œâ”€â”€ prompt.rs          # System prompt generation
â”œâ”€â”€ clipboard.rs       # Cross-platform clipboard
â””â”€â”€ lib.rs             # Public API (warden_core)
```

### Data Flow

```
User runs "warden pack"
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    discovery    â”‚â”€â”€â”€â”€â–ºâ”‚    analysis     â”‚â”€â”€â”€â”€â–ºâ”‚      pack       â”‚
â”‚   (find files)  â”‚     â”‚  (check rules)  â”‚     â”‚ (generate ctx)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                                 context.txt + prompt
                                                         â”‚
                                                    [TO AI]
                                                         â”‚
                                                         â–¼
                                                 AI response (Nabla)
                                                         â”‚
                                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    extractor    â”‚â”€â”€â”€â”€â–ºâ”‚    validator    â”‚â”€â”€â”€â”€â–ºâ”‚     writer      â”‚
â”‚ (parse Nabla)   â”‚     â”‚ (safety checks) â”‚     â”‚ (atomic write)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                 â”‚ verification  â”‚
                                                 â”‚ (cargo test)  â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â–¼                                         â–¼
                              [PASS: commit]                          [FAIL: reject]
                                    â”‚                                         â”‚
                                    â–¼                                         â–¼
                              git commit/push                      copy feedback to clipboard
```

---

## The Three Laws

Warden enforces structural constraints inspired by code review best practices. These are configurable but opinionated defaults.

### Law of Atomicity

**Files must be small enough to reason about.**

```toml
[rules]
max_file_tokens = 2000  # Default: ~500 lines of code
```

**Why:** A 5000-token file can't be meaningfully reviewed. AI-generated code especially tends toward monolithic files. Forcing small files creates natural modularity.

**Escape hatch:** `ignore_tokens_on = [".lock", ".md"]`

### Law of Complexity

**Functions must be simple enough to test.**

```toml
[rules]
max_cyclomatic_complexity = 8   # Branches per function
max_nesting_depth = 3           # if/for/while depth
max_function_args = 5           # Parameter count
max_function_words = 5          # Words in function name
```

**Why:** 
- High complexity = hard to test exhaustively
- Deep nesting = hard to follow control flow
- Many arguments = function doing too much
- Long names = unclear responsibility

**Implementation:** Tree-sitter queries count:
- Complexity: `if`, `match`, `for`, `while`, `&&`, `||`
- Depth: Nested `block` and `body` nodes
- Arity: Children of `parameters`/`arguments` nodes

### Law of Paranoia (Rust-specific)

**No panic paths in production code.**

```rust
// REJECTED
let value = thing.unwrap();
let other = thing.expect("msg");

// ALLOWED
let value = thing.unwrap_or(default);
let value = thing.unwrap_or_else(|| compute());
let value = thing?;
```

**Why:** `.unwrap()` and `.expect()` are fine for prototyping but represent silent panic paths. In production, explicit error handling is safer.

**Implementation:** Tree-sitter query matches `call_expression` where method is `unwrap` or `expect`.

### Law of Clarity (Naming)

**Function names should reveal intent.**
```toml
[rules]
max_function_words = 5   # Words in function name
```

**Why:** A function named `validate_user_input_and_send_email_notification_async` is doing too much. Short names force single responsibility.

**Implementation:** Tree-sitter extracts function names, then counts words by splitting on `_` (snake_case) or uppercase boundaries (CamelCase).

**Note:** In violation reports, this appears as `LAW OF BLUNTNESS` â€” a reminder that good names are blunt about what a function does.

---

## The Nabla Protocol

### Why Not Markdown Fences?

AI models frequently mess up markdown code fences:
- Nested fences get escaped wrong: ` ```rust ` inside ` ``` ` 
- Some models emit fences with wrong language tags
- Closing fences get matched incorrectly with earlier opens

The `âˆ‡âˆ‡âˆ‡` and `âˆ†âˆ†âˆ†` symbols:
- Never appear in normal code
- Unambiguous start/end delimiters
- Visually distinctive
- Don't require escape sequences

### Format Specification

```
âˆ‡âˆ‡âˆ‡ PLAN âˆ‡âˆ‡âˆ‡
GOAL: What you're doing
CHANGES:
1. First change
2. Second change
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ MANIFEST âˆ‡âˆ‡âˆ‡
src/file1.rs
src/file2.rs [NEW]
src/old.rs [DELETE]
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/file1.rs âˆ‡âˆ‡âˆ‡
// Complete file content
// No truncation allowed
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/file2.rs âˆ‡âˆ‡âˆ‡
// Another complete file
âˆ†âˆ†âˆ†
```

### Block Types

| Block | Purpose | Required |
|-------|---------|----------|
| `PLAN` | Human-readable summary for review | Recommended |
| `MANIFEST` | Declares all files being touched | Optional but validated |
| File paths | Actual file content | Required |

### Markers

| Marker | Meaning |
|--------|---------|
| `[NEW]` | File doesn't exist, will be created |
| `[DELETE]` | File will be removed |
| *(none)* | File exists, will be updated |

### The Contract

1. Every file in MANIFEST must have a corresponding Nabla block (unless DELETE)
2. File content must be **complete** â€” no `// ...` or "remaining code"
3. Paths must be relative, no traversal (`../`), no absolute paths
4. No touching sensitive files (`.env`, `.git/`, etc.)

---

## Adoption Tiers

Warden can be adopted incrementally:

### Tier 1: Structural Linting Only

Use Warden as a code quality scanner without any AI integration.
```bash
warden              # Scan for violations
warden check        # Run tests/linters
```

**What you get:** The Three Laws enforcement, complexity metrics, consistent standards across the team.

**Who it's for:** Teams that want code quality tooling but aren't using AI assistants yet.

### Tier 2: AI-Assisted Development

Add the pack/apply loop for AI coding sessions.
```bash
warden pack         # Generate context for AI
warden apply        # Validate and apply AI responses
```

**What you get:** Automatic rejection of oversized/complex/truncated AI output, backup system, auto-commit on success, structured rejection messages.

**Who it's for:** Individual developers or teams actively using AI coding assistants.

### Tier 3: Full Traceability

Add roadmap management and audit enforcement.
```bash
warden roadmap audit --strict   # Verify test coverage
```

**What you get:** Every feature tied to a test, programmatic progress tracking, unified apply (code + roadmap in one paste), CI enforcement of the test contract.

**Who it's for:** Teams that want rigorous test traceability and project management integration.

Teams can start at Tier 1 and graduate upward as trust in the workflow grows.

---

## Analysis Engine

### Tree-sitter Integration

Warden uses [tree-sitter](https://tree-sitter.github.io/) for structural code analysis. This provides:
- Language-agnostic AST access
- Incremental parsing (though we don't use it yet)
- Battle-tested grammars

### Supported Languages

| Language | Complexity | Skeleton | Notes |
|----------|:----------:|:--------:|-------|
| Rust | âœ… | âœ… | + `.unwrap()`/`.expect()` detection |
| TypeScript | âœ… | âœ… | Shared with JavaScript |
| JavaScript | âœ… | âœ… | ESM and CJS |
| Python | âœ… | âœ… | |
| Go | â€” | â€” | Project detection only |
| Others | â€” | â€” | Token counting only |

### Query Architecture

```rust
// src/analysis/ast.rs

// Each language has three queries:
struct LanguageQueries {
    language: Language,      // tree-sitter grammar
    naming: Query,           // Finds function/method names
    complexity: Query,       // Counts branching constructs
    banned: Option<Query>,   // Language-specific bans (Rust only)
}
```

Example complexity query (Rust):
```
(if_expression) @branch
(match_expression) @branch  
(for_expression) @branch
(while_expression) @branch
(binary_expression operator: "&&") @branch
(binary_expression operator: "||") @branch
```

### Analysis Flow

```rust
// src/analysis/mod.rs

pub struct RuleEngine { config: Config }

impl RuleEngine {
    pub fn scan(&self, files: Vec<PathBuf>) -> ScanReport {
        files.par_iter()                          // Parallel via rayon
            .filter_map(|path| self.analyze_file(path))
            .collect()
    }
    
    fn analyze_file(&self, path: &Path) -> Option<FileReport> {
        // 1. Check for warden:ignore
        // 2. Token count (Law of Atomicity)
        // 3. AST analysis (Law of Complexity, Paranoia)
        // 4. Return FileReport with violations
    }
}
```

---

## Apply System

### The Pipeline

```
Clipboard â”€â”€â–º Extract â”€â”€â–º Validate â”€â”€â–º Backup â”€â”€â–º Write â”€â”€â–º Verify â”€â”€â–º Commit
                â”‚            â”‚           â”‚          â”‚          â”‚          â”‚
                â”‚            â”‚           â”‚          â”‚          â”‚          â–¼
                â”‚            â”‚           â”‚          â”‚          â”‚     git commit/push
                â”‚            â”‚           â”‚          â”‚          â–¼
                â”‚            â”‚           â”‚          â”‚     Run check commands
                â”‚            â”‚           â”‚          â–¼
                â”‚            â”‚           â”‚     Write files atomically
                â”‚            â”‚           â–¼
                â”‚            â”‚     Backup existing files to .warden_apply_backup/
                â”‚            â–¼
                â”‚     Path safety, truncation detection, manifest validation
                â–¼
          Parse Nabla blocks, extract PLAN, MANIFEST, files
```

### Validation Rules

**Path Safety:**
- No `../` traversal
- No absolute paths (`/etc/passwd`, `C:\Windows`)
- No sensitive files (`.env`, `.ssh/`, `.aws/`, `.git/`)
- No hidden files (except `.gitignore`, `.wardenignore`)
- No overwriting `ROADMAP.md` (protected)

**Content Safety:**
- No truncation markers (`// ...`, `/* ... */`, `# ...`)
- No lazy phrases ("rest of implementation", "remaining code")
- No empty files
- Files must match MANIFEST declaration

### Backup System

Before any write:
```
.warden_apply_backup/
â””â”€â”€ 1699876543/           # Unix timestamp
    â””â”€â”€ src/
        â””â”€â”€ modified.rs   # Original content preserved
```

**Recovery:** If apply fails mid-write, original files are in backup.

### Verification

After successful writes, Warden runs configured check commands:

```toml
[commands]
check = [
    "cargo clippy --all-targets -- -D warnings",
    "cargo test"
]
```

- **All pass:** Auto-commit and push
- **Any fail:** Generate rejection message, copy to clipboard

### Escape Hatch (Planned)

For situations where strict rejection causes friction (AI failing repeatedly, deadline pressure), a future `warden apply --force` flag will:

1. Apply the code despite violations
2. Mark affected files with `// warden:quarantine`
3. Add them to a temporary ignore list
4. Report quarantined files in subsequent `warden` scans

This allows forward progress while maintaining visibility into technical debt. See ROADMAP.md v0.7.0 "Escape Hatches."

### Git Integration

On verification pass:
```rust
fn commit_and_push(message: &str) -> Result<()> {
    git add -A
    git commit -m "{prefix}{message}"
    git push
}
```

The commit message comes from the PLAN block's GOAL line.

---

## Pack & Context System

### The Problem

AI context windows are finite. You can't send your entire codebase for every request.

**Current solution:** Focus mode
```bash
warden pack src/apply/mod.rs
```
- Target file: full content
- All other files: skeletonized (signatures only)

### Skeleton System

Converts implementation to signatures:

**Before:**
```rust
pub fn validate_user(input: &UserInput) -> Result<User, ValidationError> {
    let email = input.email.trim();
    if email.is_empty() {
        return Err(ValidationError::EmptyEmail);
    }
    // ... 40 more lines
}
```

**After:**
```rust
pub fn validate_user(input: &UserInput) -> Result<User, ValidationError> { ... }
```

**Implementation:** Tree-sitter finds function bodies and replaces with `{ ... }` (Rust), `...` (Python), or `{ ... }` (JS/TS).

### Prompt Generation

Every `warden pack` output includes:
1. **Header:** System prompt with The Three Laws, current limits, Nabla instructions
2. **Violations:** Any existing rule violations (priority fix required)
3. **Files:** Codebase content in Nabla format
4. **Footer:** Constraint reminder

The AI receives not just code, but the rules it must follow.

---

## Smart Context (v0.8-0.9)

> **Status:** Designed, not yet implemented. See ROADMAP.md v0.8.0-v0.9.0.

### The Vision

Current flow:
```
You â”€â”€â–º [whole codebase] â”€â”€â–º AI â”€â”€â–º [fixes]
```

Future flow:
```
You â”€â”€â–º [minimal map] â”€â”€â–º AI â”€â”€â–º [context request] â”€â”€â–º You â”€â”€â–º [focused context] â”€â”€â–º AI â”€â”€â–º [fixes]
```

**The AI becomes a navigator, not just a consumer.**

### Warden Map

A lightweight structural overview (~50-100 lines):

```
WARDEN CODEBASE MAP
==================

src/
  analysis/     [4 files, 1.2k tokens]  â†’ Code quality checks
  apply/        [8 files, 2.8k tokens]  â†’ AI response parsing
  roadmap/      [9 files, 3.1k tokens]  â†’ Task tracking
  graph/        [3 files, 0.6k tokens]  â†’ Dependency extraction

CLUSTERS (auto-detected):
  [apply-system]    src/apply/* â†’ uses: types.rs, config.rs, roadmap/mod.rs
  [roadmap-system]  src/roadmap/* â†’ uses: types.rs
```

### Error-Driven Packing

For compiler errors, **skip AI reasoning entirely**:

```bash
cargo clippy 2>&1 | warden pack --from-errors
```

The compiler already tells you which files matter:
```
error[E0382]: use of moved value
  --> src/apply/writer.rs:45:9
```

Warden parses this, packs `src/apply/writer.rs` (and maybe its dependencies).

### Cluster Packing

```bash
warden pack --cluster apply-system
```

Packs all files in the cluster, in dependency order, with skeleton for boundary files.

### Trace Packing

```bash
warden pack --trace src/apply/mod.rs --depth 2
```

Walks the import graph 2 hops in both directions:
- What does `mod.rs` import? (dependencies)
- What imports `mod.rs`? (dependents)

Packs that subgraph, topologically sorted.

### Context Ordering

**Why it matters:** AI comprehension improves when dependencies come before dependents.

```
# BAD: Random order
src/apply/mod.rs        # Uses types.rs - but AI hasn't seen it yet
src/types.rs            # Too late!

# GOOD: Topological order  
src/types.rs            # Leaf node, no deps
src/apply/types.rs      # Uses types.rs (already seen)
src/apply/mod.rs        # Uses both (already seen)
```

### AI Context Protocol

Future: AI can request specific context:

```
CONTEXT_REQUEST:
  cluster: apply-system
  with_tests: true
```

Or in natural language:
> "I need the apply system files. Run `warden pack --cluster apply-system --with-tests`"

---

## Roadmap System

### Purpose

The roadmap isn't just documentationâ€”it's a **contract**:
- Every `[x]` feature has a `<!-- test: path::function -->` anchor
- `warden roadmap audit` verifies anchors resolve to real tests
- This enforces that "done" means "tested"

### Programmatic Updates

AI can update the roadmap via commands:

```
===ROADMAP===
CHECK "task-slug"
ADD "**New task**" AFTER "existing-task"
UPDATE "task-slug" "**New text**"
NOTE "section" "Additional info"
===ROADMAP===
```

### Unified Apply

When you run `warden apply`, it handles BOTH:
1. Code files (Nabla blocks)
2. Roadmap updates (`===ROADMAP===` block)

**One paste updates everything atomically.**

### Why Not Split Roadmap Into Its Own Tool?

We considered this. Arguments for splitting:
- Roadmap is useful beyond Warden users
- Cleaner separation of concerns

Arguments against (winning):
- **Unified apply is the killer feature** â€” one paste updates code AND progress
- Splitting requires two pastes, breaking flow
- The value is in the integration

**Decision:** Keep integrated. Extract `roadmap_core` as internal library if needed, but don't ship separately until users explicitly request it.

### Hardening (Post-Mortem)

During development, a batch of 110 roadmap commands failed catastrophically:
- 90+ commands failed due to cascading AFTER dependencies
- Slug mismatches caused silent failures
- Partial application left orphaned tasks

**Lessons learned:**
1. Validate ALL targets before executing ANY commands
2. Echo generated slugs so users know what to reference
3. All-or-nothing execution (atomic batch)
4. CHAIN command eliminates AFTER guessing for sequences

See ROADMAP.md v0.7.0 "Roadmap Hardening" for implementation tasks.

---

## Security Model

### Threat Model

**Attacker:** Malicious or confused AI generating dangerous file operations.

**Attack surface:**
- Path traversal (`../../../etc/passwd`)
- Sensitive file overwrite (`.env`, SSH keys)
- Code injection via truncation markers

### Defenses

| Threat | Defense |
|--------|---------|
| Path traversal | Block any path containing `..` |
| Absolute paths | Block paths starting with `/` or `C:\` |
| Sensitive files | Blocklist: `.env`, `.ssh/`, `.aws/`, `.gnupg/`, `id_rsa`, `credentials` |
| Hidden files | Block `.*` except `.gitignore`, `.wardenignore` |
| Backup overwrite | Block `.warden_apply_backup/` |
| Truncation | Detect `// ...`, `/* ... */`, `# ...`, lazy phrases |
| Empty files | Reject zero-content files |
| Protected files | Block `ROADMAP.md` overwrites (use roadmap commands instead) |

### Non-Goals

- Sandboxing execution (trust the user's environment)
- Network isolation (AI responses are text, not executable)
- Encryption (files are plaintext on disk anyway)

---

## Key Decisions & Rationale

### Why Rust?

- **Performance:** Parallel file analysis via rayon
- **Reliability:** No runtime crashes from null/undefined
- **Tree-sitter bindings:** First-class Rust support
- **Single binary:** Easy distribution, no dependencies
- **Dogfooding:** Warden enforces Rust best practices on itself

### Why Tree-sitter Over LSP?

- **No server overhead:** Parse on-demand, no background process
- **Language-agnostic queries:** Same query syntax for all languages
- **Incremental not needed:** We parse once per command, not on every keystroke
- **Simpler deployment:** No language server installation required

### Why CLI Over VS Code Extension?

- **Editor-agnostic:** Works with Vim, Emacs, VS Code, anything
- **Composable:** Pipes, scripts, CI integration
- **Maintainable:** One codebase, not per-editor plugins
- **AI-friendly:** Command-line is the universal interface

### Why Nabla Over Markdown?

- **Unambiguous:** No fence-escape issues
- **Distinctive:** `âˆ‡âˆ‡âˆ‡` never appears in code
- **Simple:** No language tags, just path and content
- **Parseable:** Regex-friendly delimiters

### Why Reject Instead of Fix?

- **Teaching:** AI learns constraints through failure
- **Safety:** Auto-fix could mask deeper problems
- **Simplicity:** Rejection logic is stateless
- **Trust:** User sees exactly what AI generated

### Why Git Integration?

- **Atomicity:** Commit represents "AI task complete"
- **Undo:** `git revert` is the recovery mechanism
- **History:** Track AI contributions over time
- **Workflow:** Push triggers CI, PR, deployment

---

## Module Map

### Core Libraries Used

| Crate | Purpose |
|-------|---------|
| `tree-sitter` | AST parsing |
| `tree-sitter-rust/python/typescript` | Language grammars |
| `tiktoken-rs` | Token counting (OpenAI tokenizer) |
| `clap` | CLI argument parsing |
| `serde` + `toml` | Configuration |
| `walkdir` | File system traversal |
| `rayon` | Parallel iteration |
| `regex` | Pattern matching |
| `colored` | Terminal output |
| `ratatui` + `crossterm` | TUI dashboard |
| `anyhow` + `thiserror` | Error handling |

### Internal Module Dependencies

```
lib.rs (warden_core)
    â”œâ”€â”€ analysis â”€â”€â–º config, types, tokens
    â”œâ”€â”€ apply â”€â”€â”€â”€â–º config, types, clipboard, roadmap
    â”œâ”€â”€ pack â”€â”€â”€â”€â”€â–º config, discovery, analysis, skeleton, prompt, clipboard
    â”œâ”€â”€ roadmap â”€â”€â–º clipboard
    â”œâ”€â”€ discovery â–º config
    â””â”€â”€ tui â”€â”€â”€â”€â”€â”€â–º analysis, types, config
```

---

## Testing Philosophy

### The Contract

From ROADMAP.md Philosophy:
> Every `[x]` feature MUST have a `<!-- test: path::function -->` reference

This is enforced by `warden roadmap audit --strict`.

### Test Organization

```
tests/
â”œâ”€â”€ unit_*.rs           # Pure function tests, no I/O
â”œâ”€â”€ integration_*.rs    # Multi-module tests, temp directories
â”œâ”€â”€ cli_*.rs            # Full command invocation tests
â””â”€â”€ security_*.rs       # Attack vector validation
```

### Naming Convention

Test functions should match feature slugs from ROADMAP.md:
```rust
// ROADMAP: - [x] **Block ../ traversal** <!-- test: tests/security_validation.rs::test_traversal_blocked -->

#[test]
fn test_traversal_blocked() {
    // ...
}
```

### What We Test

- **Happy paths:** Normal usage works
- **Rejection paths:** Invalid input is caught with correct error
- **Security:** Every blocked path type has explicit test
- **Edge cases:** Empty files, Unicode paths, deep nesting

### What We Don't Test

- Platform-specific clipboard (manual verification)
- Git operations in CI (mocked or skipped)
- TUI rendering (visual inspection)

---

## Future Considerations

### Language Additions

Adding a new language requires:
1. Add `tree-sitter-{lang}` dependency
2. Write complexity query (branching constructs)
3. Write naming query (function definitions)
4. Write skeleton cleaner (body replacement)
5. Add to language detection in `analysis/ast.rs`

Estimated effort: 2-4 hours per language.

### Performance

Current: ~1-2 seconds for medium codebase (1000 files).

If needed:
- Incremental analysis (cache unchanged files)
- Parallel tree-sitter parsing (currently sequential per file)
- Memory-mapped file reading

Not prioritized because current speed is acceptable.

### Distribution

Planned for v1.0.0:
- crates.io publication
- Homebrew formula (macOS)
- Scoop/Winget (Windows)
- AUR package (Arch Linux)
- GitHub Releases with prebuilt binaries

### What We're NOT Building

| Feature | Reason |
|---------|--------|
| VS Code Extension | IDE lock-in, maintenance burden |
| Watch mode | Complexity without clear benefit |
| Markdown fallback | Enforce format discipline |
| Auto-fix | Warden rejects, doesn't repair |
| LSP server | Overkill for our use case |
| Multi-repo | One project at a time |
| Cloud service | Local-first philosophy |

---

## Contributing

See ROADMAP.md for current priorities. The `ğŸ”„ CURRENT` version marker indicates active development.

Before submitting:
1. Run `warden` (must pass own rules)
2. Run `cargo clippy --all-targets -- -D warnings -D clippy::pedantic`
3. Run `cargo test`
4. Ensure new features have `<!-- test: -->` anchors in ROADMAP.md

---

*Last updated: 2025*

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ LICENSE âˆ‡âˆ‡âˆ‡
MIT License

Copyright (c) 2025 Spencer Nunamaker

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ README.md âˆ‡âˆ‡âˆ‡
# Warden

**A code quality gatekeeper for AI-assisted development**

Warden creates a feedback loop between your codebase and AI coding assistants. It packages your code with configurable quality constraints, validates AI responses, and only commits changes that pass your rules.

Instead of manually reviewing every AI-generated file, Warden automatically rejects malformed output and asks the AI to try again. When everything passes, it commits and pushes. You stay in flow.

```
warden pack â†’ AI â†’ warden apply â†’ âœ… Pass â†’ commit
                        â†“
                   âŒ Fail â†’ rejection copied â†’ paste back to AI â†’ retry
```

## Current Status

**Stable (v0.7.x):** The Three Laws, Nabla protocol, apply/reject loop, and roadmap system are fully implemented and tested.

**In Development (v0.8+):** Smart Context features (dependency graphs, error-driven packing, cluster isolation) are designed but not yet implemented. See [ROADMAP.md](ROADMAP.md).

For architecture, protocol specs, and design philosophy, see [DESIGN.md](DESIGN.md).

---

## Installation

```bash
git clone https://github.com/yourusername/warden.git
cd warden
cargo install --path .
```

Verify:
```bash
warden --version
```

### Clipboard Dependencies

Warden uses system clipboard for seamless copy/paste:

| Platform | Utility | Install |
|----------|---------|---------|
| macOS | `pbcopy` / `pbpaste` | Built-in |
| Linux (X11) | `xclip` | `apt install xclip` |
| Linux (Wayland) | `wl-copy` / `wl-paste` | `apt install wl-clipboard` |
| Windows | PowerShell | Built-in |

If clipboard isn't available, Warden displays file paths for manual copy.

---

## Quick Start

### 1. Initialize

```bash
cd your-project
warden --init
```

This creates `warden.toml` with sensible defaults for your project type.

### 2. Scan

```bash
warden
```

Reports any violations of the configured rules (file size, complexity, nesting, etc.).

### 3. Pack for AI

```bash
warden pack
```

Generates `context.txt` with your codebase and copies the file path to clipboard. Attach it to your AI conversation.

### 4. Apply AI Response

Copy the AI's response (in Nabla format), then:

```bash
warden apply
```

Warden validates the response, writes files, runs your checks, and commits if everything passes.

---

## The AI Loop (Example)

Here's how a typical AI coding session works with Warden:

**Step 1:** Pack your codebase
```bash
warden pack
# â†’ Generates context.txt, copies path to clipboard
# â†’ Attach to your AI conversation
```

**Step 2:** AI responds in Nabla format

```
âˆ‡âˆ‡âˆ‡ PLAN âˆ‡âˆ‡âˆ‡
GOAL: Add input validation to login handler
CHANGES:
1. Create validation module
2. Add email format check
3. Add password strength check
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ MANIFEST âˆ‡âˆ‡âˆ‡
src/auth/validate.rs [NEW]
src/auth/mod.rs
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/auth/validate.rs âˆ‡âˆ‡âˆ‡
use regex::Regex;

pub fn validate_email(email: &str) -> Result<(), &'static str> {
    let re = Regex::new(r"^[^@]+@[^@]+\.[^@]+$").unwrap();
    if re.is_match(email) {
        Ok(())
    } else {
        Err("Invalid email format")
    }
}

pub fn validate_password(password: &str) -> Result<(), &'static str> {
    if password.len() >= 8 {
        Ok(())
    } else {
        Err("Password must be at least 8 characters")
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/auth/mod.rs âˆ‡âˆ‡âˆ‡
pub mod validate;
pub mod login;
âˆ†âˆ†âˆ†
```

**Step 3:** Apply the response

```bash
warden apply
```

**If validation passes:** Files are written, tests run, changes committed and pushed.

**If validation fails:** Warden copies a rejection message to your clipboard:

```
The previous output was rejected by the Warden Protocol.

VALIDATION ERRORS:
- src/auth/validate.rs: Banned: '.unwrap()'. Use '?' or 'unwrap_or'.

Please provide corrected files using the NABLA PROTOCOL (âˆ‡âˆ‡âˆ‡ ... âˆ†âˆ†âˆ†).
```

**Step 4:** Paste the rejection back to the AI. It will fix the issue and retry.

The AI learns through rejection. After a few cycles, it internalizes the constraints.

---

## Configuration

Warden is configured via `warden.toml`:

```toml
[rules]
max_file_tokens = 2000              # Files must be reviewable
max_cyclomatic_complexity = 8       # Functions must be testable
max_nesting_depth = 3               # Logic must be followable
max_function_args = 5               # Functions must be focused

# Skip rules for certain files
ignore_tokens_on = [".md", ".lock", ".json"]
ignore_naming_on = ["tests", "spec"]

[commands]
check = [
    "cargo clippy --all-targets -- -D warnings",
    "cargo test"
]
fix = "cargo fmt"
```

### Language-Specific Examples

**Rust:**
```toml
[commands]
check = ["cargo clippy --all-targets -- -D warnings", "cargo test"]
fix = "cargo fmt"
```

**Node/TypeScript:**
```toml
[commands]
check = ["npm test", "npx eslint src/"]
fix = "npx prettier --write src/"
```

**Python:**
```toml
[commands]
check = ["pytest", "ruff check ."]
fix = "ruff check --fix ."
```

### Ignoring Files

**Project-wide:** Create `.wardenignore` (same syntax as `.gitignore`)
```
target/
node_modules/
*.generated.ts
```

**Per-file:** Add a comment at the top of the file
```rust
// warden:ignore
```
```python
# warden:ignore
```

---

## Commands Reference

| Command | Description |
|---------|-------------|
| `warden` | Scan codebase and report violations |
| `warden --init` | Launch configuration wizard |
| `warden --ui` | Interactive TUI dashboard |
| `warden pack` | Generate `context.txt` for AI |
| `warden pack --copy` | Copy content directly to clipboard |
| `warden pack --skeleton` | Compress all files to signatures only |
| `warden pack <file>` | Focus on one file, skeleton the rest |
| `warden apply` | Validate and apply AI response from clipboard |
| `warden check` | Run configured test/lint commands |
| `warden fix` | Run configured fix commands |
| `warden prompt` | Output the system prompt |

### Roadmap Commands

| Command | Description |
|---------|-------------|
| `warden roadmap init` | Create ROADMAP.md template |
| `warden roadmap show` | Display roadmap as tree |
| `warden roadmap tasks` | List all tasks |
| `warden roadmap tasks --pending` | List incomplete tasks |
| `warden roadmap audit` | Verify test anchors exist |
| `warden roadmap audit --strict` | Fail if any anchor is broken |

---

## Roadmap & Test Traceability

Warden includes a roadmap system that ties features to tests:

```markdown
- [x] **Token counting** <!-- test: tests/unit_tokens.rs::test_count_basic -->
- [ ] **Cluster packing**
```

The `<!-- test: ... -->` anchor creates a contract: "done" means "tested."

```bash
warden roadmap audit --strict   # Verify all completed tasks have passing tests
```

For maximum rigor, add audit to your check pipeline:

```toml
[commands]
check = [
    "cargo test",
    "warden roadmap audit --strict"
]
```

See [ROADMAP.md](ROADMAP.md) for the full feature contract.

---

## Language Support

| Language | Complexity Analysis | Skeleton Mode | Notes |
|----------|:-------------------:|:-------------:|-------|
| Rust | âœ… | âœ… | + `.unwrap()` detection |
| TypeScript | âœ… | âœ… | |
| JavaScript | âœ… | âœ… | |
| Python | âœ… | âœ… | |
| Go | â€” | â€” | Token counting only |
| Other | â€” | â€” | Token counting only |

---

## Adoption Tiers

Warden can be adopted incrementally:

### Tier 1: Structural Linting
Use Warden as a code quality scanner, no AI integration required.

```bash
warden              # Scan for violations
warden check        # Run tests/linters
```

### Tier 2: AI-Assisted Development
Add the pack/apply loop for AI coding sessions.

```bash
warden pack         # Generate context for AI
warden apply        # Validate and apply AI responses
```

### Tier 3: Full Traceability
Add roadmap management with audit enforcement.

```bash
warden roadmap audit --strict   # Every feature must have a test
```

Start at Tier 1, graduate upward as trust in the workflow grows.

---

## What's Coming

**v0.8 â€” Dependency Graph:** Import extraction, cluster detection, understanding which files relate to each other.

**v0.9 â€” Smart Context:** Error-driven packing (parse compiler errors â†’ pack relevant files), cluster packing (`--cluster apply-system`), trace packing (`--trace src/mod.rs --depth 2`).

**v0.10+ â€” Validation Hardening:** Markdown fence rejection, brace balancing, stricter content validation.

See [ROADMAP.md](ROADMAP.md) for the complete plan.

---

## License

MIT â€” See [LICENSE](LICENSE)

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ ROADMAP.md âˆ‡âˆ‡âˆ‡
# Warden Protocol Roadmap

## Philosophy

**Source of Truth:** This roadmap is the authoritative registry of all Warden features.

**The Contract:**
1. Every `[x]` feature MUST have a `<!-- test: path::function -->` reference
2. Every referenced test MUST exist and pass
3. `warden roadmap audit --strict` enforces this before any commit
4. Features without tests use `[no-test]` (docs, config, UI-only)

**Versioning:**
- v0.x.0 = Development milestones
- v1.0.0 = Production release

---

## How to Read This File

- **Users:** Skim version headlines and the Principles section.
- **Contributors:** Look at the `ğŸ”„ CURRENT` version's unchecked tasks.
- **AI Assistants:** Never rewrite this file directly. Emit `===ROADMAP===` command blocks only.

**Current Status (v0.7.x):** Warden fully supports the Three Laws, Nabla protocol, and basic roadmap management. Smart Context (dependency graphs, cluster packing) is planned for v0.8-0.9.

---

## v0.1.0 â€” Foundation âœ…

*Core infrastructure and project structure.*

### Token Counting
- [x] **Tokenizer initialization (cl100k_base)** <!-- test: tests/unit_tokens.rs::test_tokenizer_available -->
- [x] **Token count function** <!-- test: tests/unit_tokens.rs::test_count_basic -->
- [x] **Token limit check** <!-- test: tests/unit_tokens.rs::test_exceeds_limit -->
- [x] **Graceful fallback on init failure** <!-- test: tests/unit_tokens.rs::test_fallback_returns_zero -->

### Project Detection
- [x] **Rust project detection (Cargo.toml)** <!-- test: tests/unit_project.rs::test_detect_rust -->
- [x] **Node project detection (package.json)** <!-- test: tests/unit_project.rs::test_detect_node -->
- [x] **Python project detection** <!-- test: tests/unit_project.rs::test_detect_python -->
- [x] **Go project detection (go.mod)** <!-- test: tests/unit_project.rs::test_detect_go -->
- [x] **Unknown project fallback** <!-- test: tests/unit_project.rs::test_detect_unknown -->

### Configuration
- [x] **TOML config loading** <!-- test: tests/unit_config.rs::test_load_toml -->
- [x] **Default rule values** <!-- test: tests/unit_config.rs::test_defaults -->
- [x] **Command list parsing** <!-- test: tests/unit_config.rs::test_command_list -->
- [x] **.wardenignore loading** <!-- test: tests/unit_config.rs::test_wardenignore -->
- [x] **Auto-config generation** [no-test] *(side effect on first run)*

---

## v0.2.0 â€” The 3 Laws âœ…

*Structural analysis enforcement.*

### Law of Atomicity
- [x] **File token counting** <!-- test: tests/integration_core.rs::test_atomicity_clean_file_passes -->
- [x] **Token limit violation** <!-- test: tests/integration_core.rs::test_atomicity_large_file_fails -->
- [x] **Token exemption patterns** <!-- test: tests/unit_config.rs::test_ignore_tokens_on -->

### Law of Complexity â€” Cyclomatic
- [x] **Rust complexity query (if/match/for/while/&&/||)** <!-- test: tests/integration_core.rs::test_complexity_boundary_check -->
- [x] **Complexity violation detection** <!-- test: tests/integration_core.rs::test_complexity_construct_match -->
- [x] **JS/TS complexity query** <!-- test: tests/unit_analysis.rs::test_js_complexity -->
- [x] **Python complexity query** <!-- test: tests/unit_analysis.rs::test_python_complexity -->

### Law of Complexity â€” Nesting Depth
- [x] **Depth calculation (block/body traversal)** <!-- test: tests/integration_core.rs::test_nesting_boundary -->
- [x] **Deep nesting violation** <!-- test: tests/integration_core.rs::test_nesting_boundary -->

### Law of Complexity â€” Arity
- [x] **Parameter counting** <!-- test: tests/integration_core.rs::test_arity_boundary -->
- [x] **High arity violation** <!-- test: tests/integration_core.rs::test_arity_boundary -->

### Law of Complexity â€” Naming
- [x] **Snake_case word counting** <!-- test: tests/unit_analysis.rs::test_snake_case_words -->
- [x] **CamelCase word counting** <!-- test: tests/unit_analysis.rs::test_camel_case_words -->
- [x] **Naming ignore patterns** <!-- test: tests/unit_config.rs::test_ignore_naming_on -->

### Law of Paranoia (Rust)
- [x] **Banned call query (.unwrap/.expect)** <!-- test: tests/integration_core.rs::test_paranoia_unwrap_fails -->
- [x] **.expect() detection** <!-- test: tests/integration_core.rs::test_paranoia_expect_fails -->
- [x] **Safe alternatives allowed (.unwrap_or)** <!-- test: tests/integration_core.rs::test_paranoia_safe_alternatives_pass -->

### File Ignores
- [x] **warden:ignore (C-style //)** <!-- test: tests/integration_core.rs::test_warden_ignore_skips_file -->
- [x] **warden:ignore (Hash-style #)** <!-- test: tests/unit_analysis.rs::test_warden_ignore_hash -->
- [x] **warden:ignore (HTML-style)** <!-- test: tests/unit_analysis.rs::test_warden_ignore_html -->

---

## v0.3.0 â€” Apply System âœ…

*AI response parsing and file writing.*

### Nabla Format Extraction
- [x] **Header detection (âˆ‡âˆ‡âˆ‡ path âˆ‡âˆ‡âˆ‡)** <!-- test: tests/integration_apply.rs::test_extract_single_file -->
- [x] **Footer detection (âˆ†âˆ†âˆ†)** <!-- test: tests/integration_apply.rs::test_extract_single_file -->
- [x] **Path extraction from header** <!-- test: tests/integration_apply.rs::test_extract_single_file -->
- [x] **Content extraction** <!-- test: tests/integration_apply.rs::test_extract_single_file -->
- [x] **Multiple file extraction** <!-- test: tests/integration_apply.rs::test_extract_multiple_files -->
- [x] **MANIFEST block skipping** <!-- test: tests/integration_apply.rs::test_extract_skips_manifest -->
- [x] **PLAN block extraction** <!-- test: tests/integration_apply.rs::test_extract_plan -->
- [x] **Malformed block handling** <!-- test: tests/unit_extractor.rs::test_malformed_block_skipped -->

### Manifest Parsing
- [x] **Manifest block detection** <!-- test: tests/unit_manifest.rs::test_parse_manifest -->
- [x] **[NEW] marker detection** <!-- test: tests/unit_manifest.rs::test_new_marker -->
- [x] **[DELETE] marker detection** <!-- test: tests/unit_manifest.rs::test_delete_marker -->
- [x] **Default Update operation** <!-- test: tests/unit_manifest.rs::test_default_update -->

### File Writing
- [x] **Parent directory creation** <!-- test: tests/unit_writer.rs::test_creates_parent_dirs -->
- [x] **File content writing** <!-- test: tests/unit_writer.rs::test_writes_content -->
- [x] **Delete operation** <!-- test: tests/unit_writer.rs::test_delete_file -->
- [x] **Written files tracking** <!-- test: tests/unit_writer.rs::test_tracks_written -->

### Backup System
- [x] **Backup directory creation** <!-- test: tests/integration_backup.rs::test_backup_dir_created -->
- [x] **Timestamp subfolder** <!-- test: tests/integration_backup.rs::test_timestamp_folder -->
- [x] **Existing file backup** <!-- test: tests/integration_backup.rs::test_existing_backed_up -->
- [x] **New file skip (no backup needed)** <!-- test: tests/integration_backup.rs::test_new_file_no_backup -->
- [x] **Backup path structure preserved** <!-- test: tests/integration_backup.rs::test_path_structure -->

---

## v0.4.0 â€” Safety & Validation âœ…

*Path security and content validation.*

### Path Safety â€” Traversal
- [x] **Block ../ traversal** <!-- test: tests/integration_apply.rs::test_path_safety_blocks_traversal -->
- [x] **Block .. prefix** <!-- test: tests/security_validation.rs::test_traversal_blocked -->

### Path Safety â€” Absolute
- [x] **Block Unix absolute (/)** <!-- test: tests/integration_apply.rs::test_path_safety_blocks_absolute -->
- [x] **Block Windows absolute (C:)** <!-- test: tests/security_validation.rs::test_absolute_paths_blocked -->

### Path Safety â€” Sensitive
- [x] **Block .git/** <!-- test: tests/integration_apply.rs::test_path_safety_blocks_git -->
- [x] **Block .env** <!-- test: tests/security_validation.rs::test_sensitive_paths_blocked -->
- [x] **Block .ssh/** <!-- test: tests/security_validation.rs::test_sensitive_paths_blocked -->
- [x] **Block .aws/** <!-- test: tests/security_validation.rs::test_sensitive_paths_blocked -->
- [x] **Block .gnupg/** <!-- test: tests/unit_validator.rs::test_gnupg_blocked -->
- [x] **Block id_rsa** <!-- test: tests/unit_validator.rs::test_id_rsa_blocked -->
- [x] **Block credentials** <!-- test: tests/unit_validator.rs::test_credentials_blocked -->
- [x] **Block backup directory** <!-- test: tests/unit_validator.rs::test_backup_dir_blocked -->

### Path Safety â€” Hidden Files
- [x] **Block hidden files (.*)** <!-- test: tests/integration_apply.rs::test_path_safety_blocks_hidden -->
- [x] **Allow . and .. segments** <!-- test: tests/security_validation.rs::test_valid_paths_allowed -->

### Path Safety â€” Protected Files
- [x] **Block ROADMAP.md rewrite** <!-- test: tests/protection_roadmap.rs::test_roadmap_rewrite_is_blocked -->
- [x] **Case-insensitive protection** <!-- test: tests/protection_roadmap.rs::test_roadmap_rewrite_blocked_case_insensitive -->

### Truncation Detection
- [x] **Pattern: // ...** <!-- test: tests/integration_apply.rs::test_truncation_detects_ellipsis_comment -->
- [x] **Pattern: /* ... */** <!-- test: tests/unit_validator.rs::test_block_comment_ellipsis -->
- [x] **Pattern: # ...** <!-- test: tests/unit_validator.rs::test_hash_ellipsis -->
- [x] **Pattern: "rest of" phrases** <!-- test: tests/unit_validator.rs::test_lazy_phrase_rest_of -->
- [x] **Pattern: "remaining" phrases** <!-- test: tests/unit_validator.rs::test_lazy_phrase_remaining -->
- [x] **warden:ignore bypass** <!-- test: tests/integration_apply.rs::test_truncation_allows_warden_ignore -->
- [x] **Empty file rejection** <!-- test: tests/integration_apply.rs::test_truncation_detects_empty_file -->
- [x] **Line number in error** <!-- test: tests/unit_validator.rs::test_line_number_reported -->

### Valid Paths
- [x] **Normal paths accepted** <!-- test: tests/integration_apply.rs::test_path_safety_allows_valid -->
- [x] **Nested src paths accepted** <!-- test: tests/security_validation.rs::test_valid_paths_allowed -->

---

## v0.5.0 â€” Pack & Context âœ…

*Context generation for AI consumption.*

### Pack Core
- [x] **File discovery integration** <!-- test: tests/integration_pack.rs::test_nabla_delimiters_are_unique -->
- [x] **Nabla format output** <!-- test: tests/integration_pack.rs::test_nabla_format_structure -->
- [x] **Token count display** <!-- test: tests/unit_pack.rs::test_token_count_shown -->
- [x] **File write to context.txt** <!-- test: tests/unit_pack.rs::test_writes_context_file -->

### Pack Options
- [x] **--stdout output** <!-- test: tests/unit_pack.rs::test_stdout_option -->
- [x] **--copy to clipboard** <!-- test: tests/unit_pack.rs::test_copy_option -->
- [x] **--noprompt excludes header** <!-- test: tests/unit_pack.rs::test_noprompt -->
- [x] **--git-only mode** <!-- test: tests/unit_pack.rs::test_git_only -->
- [x] **--no-git mode** <!-- test: tests/unit_pack.rs::test_no_git -->
- [x] **--code-only mode** <!-- test: tests/unit_pack.rs::test_code_only -->
- [x] **--verbose progress** [no-test] *(output only)*

### Prompt Generation
- [x] **System prompt header** <!-- test: tests/integration_pack.rs::test_prompt_includes_laws -->
- [x] **Law of Atomicity in prompt** <!-- test: tests/integration_pack.rs::test_prompt_includes_limits -->
- [x] **Law of Complexity in prompt** <!-- test: tests/integration_pack.rs::test_prompt_includes_limits -->
- [x] **Nabla format instructions** <!-- test: tests/integration_pack.rs::test_prompt_includes_nabla_instructions -->
- [x] **Footer reminder** <!-- test: tests/integration_pack.rs::test_reminder_is_concise -->
- [x] **Violation injection** <!-- test: tests/unit_pack_violations.rs::test_violations_injected -->

### Skeleton System
- [x] **Rust body â†’ { ... }** <!-- test: tests/integration_skeleton.rs::test_clean_rust_basic -->
- [x] **Rust nested functions** <!-- test: tests/integration_skeleton.rs::test_clean_rust_nested -->
- [x] **Python body â†’ ...** <!-- test: tests/integration_skeleton.rs::test_clean_python -->
- [x] **TypeScript/JS body** <!-- test: tests/integration_skeleton.rs::test_clean_typescript -->
- [x] **Arrow function support** <!-- test: tests/integration_skeleton.rs::test_clean_typescript -->
- [x] **Unsupported passthrough** <!-- test: tests/integration_skeleton.rs::test_clean_unsupported_extension -->

### Focus Mode
- [x] **--skeleton all files** <!-- test: tests/integration_pack.rs::test_pack_skeleton_integration -->
- [x] **--target focus mode** <!-- test: tests/integration_pack.rs::test_smart_context_focus_mode -->
- [x] **Target full, rest skeleton** <!-- test: tests/integration_pack.rs::test_smart_context_focus_mode -->

### File Path Clipboard
- [x] **Copy file path for attachment** [no-test] *(platform-specific side effect)*

---

## v0.6.0 â€” Roadmap System âœ…

*Programmatic roadmap management.*

### Roadmap Parsing
- [x] **Title extraction (# Title)** <!-- test: tests/integration_roadmap.rs::test_parse_simple_roadmap -->
- [x] **Section heading detection** <!-- test: tests/integration_roadmap.rs::test_parse_simple_roadmap -->
- [x] **Task checkbox detection** <!-- test: tests/integration_roadmap.rs::test_parse_extracts_tasks -->
- [x] **Task status: pending** <!-- test: tests/integration_roadmap.rs::test_parse_extracts_tasks -->
- [x] **Task status: complete** <!-- test: tests/integration_roadmap.rs::test_parse_extracts_tasks -->
- [x] **Stats calculation** <!-- test: tests/integration_roadmap.rs::test_stats_are_correct -->
- [x] **Test anchor extraction** <!-- test: tests/unit_roadmap.rs::test_anchor_extraction -->
- [x] **Task path generation** <!-- test: tests/integration_roadmap.rs::test_find_task_by_path -->
- [x] **Compact state display** <!-- test: tests/integration_roadmap.rs::test_compact_state_format -->

### Slugification
- [x] **Lowercase conversion** <!-- test: tests/integration_roadmap.rs::test_slugify_basic -->
- [x] **Special char to dash** <!-- test: tests/integration_roadmap.rs::test_slugify_special_chars -->
- [x] **Number preservation** <!-- test: tests/integration_roadmap.rs::test_slugify_preserves_numbers -->

### Command Parsing
- [x] **===ROADMAP=== block detection** <!-- test: tests/integration_roadmap.rs::test_parse_extracts_from_larger_text -->
- [x] **CHECK command** <!-- test: tests/integration_roadmap.rs::test_parse_check_command -->
- [ ] **UNCHECK command** <!-- test: tests/integration_roadmap.rs::test_parse_uncheck_command -->
- [x] **ADD command** <!-- test: tests/integration_roadmap.rs::test_parse_multiple_commands -->
- [x] **ADD with AFTER** <!-- test: tests/integration_roadmap.rs::test_parse_add_with_after -->
- [x] **UPDATE command** <!-- test: tests/unit_roadmap.rs::test_update_command -->
- [x] **NOTE command** <!-- test: tests/unit_roadmap.rs::test_note_command -->
- [x] **MOVE command** <!-- test: tests/unit_roadmap.rs::test_move_command -->
- [x] **Comment skipping** <!-- test: tests/integration_roadmap.rs::test_parse_ignores_comments -->
- [x] **Summary generation** <!-- test: tests/integration_roadmap.rs::test_summary_format -->

### Roadmap CLI
- [x] **roadmap init** <!-- test: tests/cli_roadmap.rs::test_init_creates_file -->
- [x] **roadmap prompt** <!-- test: tests/cli_roadmap.rs::test_prompt_generates -->
- [x] **roadmap apply** <!-- test: tests/cli_roadmap.rs::test_apply_from_clipboard -->
- [x] **roadmap show** <!-- test: tests/cli_roadmap.rs::test_show_tree -->
- [x] **roadmap tasks** <!-- test: tests/cli_roadmap.rs::test_tasks_list -->
- [x] **roadmap tasks --pending** <!-- test: tests/cli_roadmap.rs::test_tasks_pending_filter -->
- [x] **roadmap tasks --complete** <!-- test: tests/cli_roadmap.rs::test_tasks_complete_filter -->
- [x] **roadmap audit** <!-- test: tests/cli_roadmap.rs::test_audit_runs -->

### Unified Apply
- [x] **Detect ===ROADMAP=== in apply** <!-- test: tests/integration_apply.rs::test_unified_apply_roadmap -->
- [x] **Apply roadmap + files together** <!-- test: tests/integration_apply.rs::test_unified_apply_combined -->

---

## v0.7.0 â€” Test Traceability ğŸ”„ CURRENT

*Enforce the contract: every feature has verified tests.*

### Parser Hardening
- [ ] **Empty task ID filtering** <!-- test: tests/unit_parser.rs::test_empty_id_skipped -->
- [ ] **Task ID collision detection** <!-- test: tests/unit_parser.rs::test_id_collision_resolved -->
- [ ] **Anchor-based task matching** <!-- test: tests/unit_parser.rs::test_anchor_id_extraction -->
- [ ] **Smart UPDATE inference (vs DELETE+ADD)** <!-- test: tests/unit_diff.rs::test_text_change_is_update -->

### Audit System
- [x] **Scan completed tasks** <!-- test: tests/integration_audit.rs::test_scans_completed_only -->
- [x] **[no-test] skip** <!-- test: tests/integration_audit.rs::test_no_test_skipped -->
- [x] **Explicit anchor verification** <!-- test: tests/integration_audit.rs::test_explicit_anchor_verified -->
- [x] **Missing test file detection** <!-- test: tests/integration_audit.rs::test_missing_file_detected -->
- [ ] **Missing test function detection**
- [ ] **Test execution verification (cargo test)**
- [ ] **Exit code 1 on any failure**
- [ ] **--strict mode (all must pass)**

### Self-Hosting
- [ ] **Warden passes own rules** <!-- test: tests/integration_self_host.rs::test_warden_passes_own_rules -->

### Test Naming Convention
- [ ] **Feature ID â†’ test function mapping**
- [ ] **Audit validates naming convention**

### Roadmap Hardening
- [ ] **SECTION command (create version headers)** <!-- test: tests/unit_roadmap_cmd.rs::test_section_command -->
- [ ] **SUBSECTION command (create ### headers)** <!-- test: tests/unit_roadmap_cmd.rs::test_subsection_command -->
- [ ] **CHAIN command (sequential adds)** <!-- test: tests/unit_roadmap_cmd.rs::test_chain_command -->
- [ ] **AFTER PREVIOUS keyword** <!-- test: tests/unit_roadmap_cmd.rs::test_after_previous -->
- [ ] **AFTER TEXT "exact" match** <!-- test: tests/unit_roadmap_cmd.rs::test_after_text_exact -->
- [ ] **AFTER LINE N match** <!-- test: tests/unit_roadmap_cmd.rs::test_after_line_number -->
- [ ] **IN "section/subsection" location** <!-- test: tests/unit_roadmap_cmd.rs::test_in_location -->
- [ ] **Slug echo on ADD (show generated slug)** <!-- test: tests/unit_roadmap_cmd.rs::test_slug_echo -->
- [ ] **Pre-validation: all AFTER targets exist** <!-- test: tests/unit_roadmap_validate.rs::test_after_target_exists -->
- [ ] **Pre-validation: no slug collisions** <!-- test: tests/unit_roadmap_validate.rs::test_slug_collision -->
- [ ] **Pre-validation: no circular AFTER chains** <!-- test: tests/unit_roadmap_validate.rs::test_circular_detection -->
- [ ] **Batch dependency resolution (topological sort)** <!-- test: tests/unit_roadmap_validate.rs::test_batch_topo_sort -->
- [ ] **Fuzzy match suggestions on AFTER miss** <!-- test: tests/unit_roadmap_validate.rs::test_fuzzy_suggest -->
- [ ] **Dry-run mode (--dry-run flag)** <!-- test: tests/cli_roadmap.rs::test_apply_dry_run -->
- [ ] **Atomic file write (temp â†’ rename)** <!-- test: tests/unit_roadmap_write.rs::test_atomic_write -->
- [ ] **Backup creation (.md.bak)** <!-- test: tests/unit_roadmap_write.rs::test_backup_created -->
- [ ] **All-or-nothing execution (rollback on error)** <!-- test: tests/integration_roadmap.rs::test_rollback_on_error -->
- [ ] **Verbose plan output** <!-- test: tests/cli_roadmap.rs::test_verbose_plan -->

### Escape Hatches
- [ ] **warden apply --force flag** <!-- test: tests/cli_apply.rs::test_force_flag -->
- [ ] **Quarantine mode (// warden:quarantine marker)** <!-- test: tests/integration_apply.rs::test_quarantine_marker -->
- [ ] **Quarantine report in warden scan** <!-- test: tests/integration_core.rs::test_quarantine_report -->

---

## v0.8.0 â€” Dependency Graph âš¡ HIGH PRIORITY

> **Why this matters:** Efficient context management is what makes AI assistance scale. Understanding file relationships enables surgical context packing.

*Build the import graph for smart context generation.*

### Import Extraction â€” Rust
- [ ] **Rust use declaration extraction** <!-- test: tests/unit_graph.rs::test_rust_use_extraction -->
- [ ] **Rust mod declaration extraction** <!-- test: tests/unit_graph.rs::test_rust_mod_extraction -->
- [ ] **Rust re-export handling (pub use)** <!-- test: tests/unit_graph.rs::test_rust_reexport -->
- [ ] **Rust crate:: path resolution** <!-- test: tests/unit_graph.rs::test_rust_crate_path -->
- [ ] **Rust self:: path resolution** <!-- test: tests/unit_graph.rs::test_rust_self_path -->
- [ ] **Rust super:: path resolution** <!-- test: tests/unit_graph.rs::test_rust_super_path -->
- [ ] **Rust mod.rs index resolution** <!-- test: tests/unit_graph.rs::test_rust_mod_index -->

### Import Extraction â€” Python
- [ ] **Python import statement extraction** <!-- test: tests/unit_graph.rs::test_python_import -->
- [ ] **Python from...import extraction** <!-- test: tests/unit_graph.rs::test_python_from_import -->
- [ ] **Python relative import handling (.module)** <!-- test: tests/unit_graph.rs::test_python_relative -->
- [ ] **Python __init__.py resolution** <!-- test: tests/unit_graph.rs::test_python_init -->

### Import Extraction â€” TypeScript/JavaScript
- [ ] **TypeScript import extraction** <!-- test: tests/unit_graph.rs::test_ts_import -->
- [ ] **TypeScript require() extraction** <!-- test: tests/unit_graph.rs::test_ts_require -->
- [ ] **TypeScript re-export extraction** <!-- test: tests/unit_graph.rs::test_ts_reexport -->
- [ ] **JavaScript ESM/CJS detection** <!-- test: tests/unit_graph.rs::test_js_module_type -->
- [ ] **TypeScript index.ts resolution** <!-- test: tests/unit_graph.rs::test_ts_index -->
- [ ] **TypeScript path alias resolution (tsconfig)** <!-- test: tests/unit_graph.rs::test_ts_path_alias -->

### Import Resolution â€” General
- [ ] **Relative path resolution (./)** <!-- test: tests/unit_resolver.rs::test_relative_path -->
- [ ] **Parent path resolution (../)** <!-- test: tests/unit_resolver.rs::test_parent_path -->
- [ ] **External dependency detection (skip)** <!-- test: tests/unit_resolver.rs::test_external_skipped -->

### Graph Construction
- [ ] **Graph node creation (file â†’ vertex)** <!-- test: tests/unit_graph_build.rs::test_node_creation -->
- [ ] **Graph edge creation (import â†’ directed edge)** <!-- test: tests/unit_graph_build.rs::test_edge_creation -->
- [ ] **Reverse index construction (importers)** <!-- test: tests/unit_graph_build.rs::test_reverse_index -->
- [ ] **Cycle detection (tarjan or DFS)** <!-- test: tests/unit_graph_build.rs::test_cycle_detection -->
- [ ] **Hub file detection (high in-degree)** <!-- test: tests/unit_graph_build.rs::test_hub_detection -->
- [ ] **Leaf file detection (zero out-degree)** <!-- test: tests/unit_graph_build.rs::test_leaf_detection -->
- [ ] **Orphan file detection (zero in-degree)** <!-- test: tests/unit_graph_build.rs::test_orphan_detection -->
- [ ] **Graph serialization (debug output)** <!-- test: tests/unit_graph_build.rs::test_serialization -->

### Graph Traversal
- [ ] **BFS traversal from entry point** <!-- test: tests/unit_graph_walk.rs::test_bfs -->
- [ ] **DFS traversal from entry point** <!-- test: tests/unit_graph_walk.rs::test_dfs -->
- [ ] **Depth-limited traversal (--depth N)** <!-- test: tests/unit_graph_walk.rs::test_depth_limit -->
- [ ] **Forward walk (dependencies)** <!-- test: tests/unit_graph_walk.rs::test_forward -->
- [ ] **Reverse walk (dependents)** <!-- test: tests/unit_graph_walk.rs::test_reverse -->
- [ ] **Bidirectional walk (both)** <!-- test: tests/unit_graph_walk.rs::test_bidirectional -->
- [ ] **Subgraph extraction** <!-- test: tests/unit_graph_walk.rs::test_subgraph -->
- [ ] **Topological sort for output ordering** <!-- test: tests/unit_graph_walk.rs::test_topo_sort -->

### Cluster Detection
- [ ] **Directory-based cluster inference** <!-- test: tests/unit_cluster.rs::test_dir_cluster -->
- [ ] **Strongly connected component detection** <!-- test: tests/unit_cluster.rs::test_scc -->
- [ ] **Cluster boundary identification** <!-- test: tests/unit_cluster.rs::test_boundary -->
- [ ] **Cross-cluster edge detection** <!-- test: tests/unit_cluster.rs::test_cross_cluster -->
- [ ] **Cluster size metrics (files, tokens)** <!-- test: tests/unit_cluster.rs::test_metrics -->
- [ ] **warden.toml [clusters] definition** <!-- test: tests/unit_cluster.rs::test_toml_clusters -->
- [ ] **// warden:cluster(name) annotation** <!-- test: tests/unit_cluster.rs::test_annotation -->

---

## v0.9.0 â€” Smart Context âš¡ HIGH PRIORITY

> **Ship incrementally:** Start with `--from-errors` (highest ROI), then add cluster/trace packing.

*Give AI exactly what it needs, nothing more.*

### Warden Map Command
- [ ] **warden map basic output** <!-- test: tests/cli_map.rs::test_map_basic -->
- [ ] **Directory tree with file counts** <!-- test: tests/cli_map.rs::test_map_tree -->
- [ ] **Cluster summary display** <!-- test: tests/cli_map.rs::test_map_clusters -->
- [ ] **--deps flag (show dependency arrows)** <!-- test: tests/cli_map.rs::test_map_deps -->
- [ ] **--stats flag (token counts per cluster)** <!-- test: tests/cli_map.rs::test_map_stats -->
- [ ] **--json flag (machine-readable map)** <!-- test: tests/cli_map.rs::test_map_json -->
- [ ] **Module description extraction (//! or docstring)** <!-- test: tests/cli_map.rs::test_map_docs -->
- [ ] **Entry point detection (main.rs, lib.rs, index.ts)** <!-- test: tests/cli_map.rs::test_entry_point -->

### Error-Driven Packing
- [ ] **warden pack --from-errors flag** <!-- test: tests/integration_error_pack.rs::test_from_errors_flag -->
- [ ] **Cargo/rustc error parsing** <!-- test: tests/unit_error_parse.rs::test_cargo_errors -->
- [ ] **Clippy warning parsing** <!-- test: tests/unit_error_parse.rs::test_clippy_warnings -->
- [ ] **TypeScript/tsc error parsing** <!-- test: tests/unit_error_parse.rs::test_tsc_errors -->
- [ ] **Python traceback parsing** <!-- test: tests/unit_error_parse.rs::test_python_traceback -->
- [ ] **ESLint output parsing** <!-- test: tests/unit_error_parse.rs::test_eslint_output -->
- [ ] **File path extraction from errors** <!-- test: tests/unit_error_parse.rs::test_path_extraction -->
- [ ] **Line number extraction from errors** <!-- test: tests/unit_error_parse.rs::test_line_extraction -->
- [ ] **Unique file deduplication** <!-- test: tests/unit_error_parse.rs::test_dedup -->
- [ ] **Auto-include test files for src errors** <!-- test: tests/integration_error_pack.rs::test_auto_tests -->
- [ ] **Piped input support (cargo clippy 2>&1 |)** <!-- test: tests/integration_error_pack.rs::test_piped -->
- [ ] **--from-clipboard-errors flag** <!-- test: tests/integration_error_pack.rs::test_clipboard_errors -->

### Cluster Packing
- [ ] **warden pack --cluster NAME flag** <!-- test: tests/integration_cluster_pack.rs::test_cluster_flag -->
- [ ] **Cluster resolution by name** <!-- test: tests/integration_cluster_pack.rs::test_by_name -->
- [ ] **Cluster resolution by directory path** <!-- test: tests/integration_cluster_pack.rs::test_by_dir -->
- [ ] **--with-tests flag (include test files)** <!-- test: tests/integration_cluster_pack.rs::test_with_tests -->
- [ ] **--with-boundary flag (skeleton boundary files)** <!-- test: tests/integration_cluster_pack.rs::test_with_boundary -->
- [ ] **--no-boundary flag (exclude boundary files)** <!-- test: tests/integration_cluster_pack.rs::test_no_boundary -->
- [ ] **Multiple cluster inclusion (--cluster a --cluster b)** <!-- test: tests/integration_cluster_pack.rs::test_multi_cluster -->

### Trace Packing
- [ ] **warden pack --trace PATH flag** <!-- test: tests/integration_trace_pack.rs::test_trace_flag -->
- [ ] **--depth N limit for trace** <!-- test: tests/integration_trace_pack.rs::test_depth_limit -->
- [ ] **--forward flag (dependencies only)** <!-- test: tests/integration_trace_pack.rs::test_forward -->
- [ ] **--reverse flag (dependents only)** <!-- test: tests/integration_trace_pack.rs::test_reverse -->
- [ ] **Default: bidirectional trace** <!-- test: tests/integration_trace_pack.rs::test_bidirectional -->
- [ ] **Trace + skeleton hybrid output** <!-- test: tests/integration_trace_pack.rs::test_hybrid -->
- [ ] **Multiple trace roots (--trace a --trace b)** <!-- test: tests/integration_trace_pack.rs::test_multi_trace -->

### Context Ordering
- [ ] **Dependency-first ordering (topological)** <!-- test: tests/unit_ordering.rs::test_topo_order -->
- [ ] **Leaf files appear first** <!-- test: tests/unit_ordering.rs::test_leaves_first -->
- [ ] **Target/focus file appears last** <!-- test: tests/unit_ordering.rs::test_target_last -->
- [ ] **Circular dependency handling (break arbitrarily)** <!-- test: tests/unit_ordering.rs::test_cycle_break -->
- [ ] **Shared dependency hoisting** <!-- test: tests/unit_ordering.rs::test_hoisting -->

### AI Context Protocol
- [ ] **CONTEXT_REQUEST format specification** [no-test] *(documentation)*
- [ ] **AI can emit cluster requests** <!-- test: tests/integration_fulfill.rs::test_cluster_request -->
- [ ] **AI can emit trace requests** <!-- test: tests/integration_fulfill.rs::test_trace_request -->
- [ ] **AI can emit file requests** <!-- test: tests/integration_fulfill.rs::test_file_request -->
- [ ] **warden fulfill command (parse AI request)** <!-- test: tests/integration_fulfill.rs::test_fulfill_command -->
- [ ] **Request validation (cluster/file exists)** <!-- test: tests/integration_fulfill.rs::test_validation -->

---

## v0.10.0 â€” Validation Hardening

*Catch more AI failure modes.*

### Markdown Rejection
- [ ] **Block triple backticks (```)** <!-- test: tests/integration_apply.rs::test_rejects_markdown_fences -->
- [ ] **Block tilde fences (~~~)** <!-- test: tests/integration_apply.rs::test_rejects_tilde_fences -->
- [ ] **Markdown fence rejection rationale** [no-test] *(documentation: AI escape issues)*

### Brace Balancing
- [ ] **Detect unbalanced {** <!-- test: tests/integration_apply.rs::test_detects_unbalanced_open_brace -->
- [ ] **Detect unbalanced }** <!-- test: tests/integration_apply.rs::test_detects_unbalanced_close_brace -->
- [ ] **Detect unbalanced [** <!-- test: tests/integration_apply.rs::test_detects_unbalanced_bracket -->
- [ ] **Detect unbalanced (** <!-- test: tests/integration_apply.rs::test_detects_unbalanced_paren -->
- [ ] **Brace balance algorithm selection** [no-test] *(design decision)*
- [ ] **String literal exclusion from brace count** <!-- test: tests/unit_brace.rs::test_string_exclusion -->
- [ ] **Comment exclusion from brace count** <!-- test: tests/unit_brace.rs::test_comment_exclusion -->

---

## v0.11.0 â€” CI/CD Integration

*Machine-readable output and automation.*

### Output Formats
- [ ] **--format json** <!-- test: tests/cli_format.rs::test_json_output -->
- [ ] **SARIF output for GitHub** <!-- test: tests/cli_format.rs::test_sarif_output -->

### Git Hooks
- [ ] **warden hook install** <!-- test: tests/cli_hooks.rs::test_hook_install -->
- [ ] **Pre-commit hook script** <!-- test: tests/cli_hooks.rs::test_precommit_runs -->

### Exit Codes
- [ ] **Exit 0 on clean** <!-- test: tests/cli_exit.rs::test_exit_0_clean -->
- [ ] **Exit 1 on violations** <!-- test: tests/cli_exit.rs::test_exit_1_violations -->
- [ ] **Exit 2 on error** <!-- test: tests/cli_exit.rs::test_exit_2_error -->

### CI Templates
- [ ] **GitHub Actions workflow template** <!-- test: tests/cli_ci.rs::test_github_template -->
- [ ] **GitLab CI template** <!-- test: tests/cli_ci.rs::test_gitlab_template -->
- [ ] **warden init --ci flag (generate workflow)** <!-- test: tests/cli_ci.rs::test_init_ci -->
- [ ] **Fail-fast vs report-all modes** <!-- test: tests/cli_ci.rs::test_fail_modes -->
- [ ] **Annotation output for GitHub PR comments** <!-- test: tests/cli_ci.rs::test_annotations -->

---

## v0.12.0 â€” Graph Visualization

*See your codebase structure.*

### Visualization Formats
- [ ] **warden graph command** <!-- test: tests/cli_graph.rs::test_graph_command -->
- [ ] **DOT format export (Graphviz)** <!-- test: tests/cli_graph.rs::test_dot_export -->
- [ ] **Mermaid format export** <!-- test: tests/cli_graph.rs::test_mermaid_export -->
- [ ] **--cluster-only flag (show clusters, not files)** <!-- test: tests/cli_graph.rs::test_cluster_only -->
- [ ] **--highlight PATH flag (color specific subgraph)** <!-- test: tests/cli_graph.rs::test_highlight -->
- [ ] **Interactive HTML export (D3.js)** <!-- test: tests/cli_graph.rs::test_html_export -->
- [ ] **Terminal ASCII graph (small projects)** <!-- test: tests/cli_graph.rs::test_ascii_graph -->

---

## v0.13.0 â€” Legacy Adoption

*Make Warden adoptable in existing codebases.*

### Baseline System
- [ ] **warden baseline command (snapshot current violations)** <!-- test: tests/cli_baseline.rs::test_baseline_creation -->
- [ ] **Baseline file format (.warden-baseline.json)** <!-- test: tests/unit_baseline.rs::test_baseline_format -->
- [ ] **Baseline comparison mode (only report new violations)** <!-- test: tests/integration_baseline.rs::test_baseline_comparison -->
- [ ] **--baseline flag for warden scan** <!-- test: tests/cli_baseline.rs::test_baseline_flag -->
- [ ] **Auto-generate warden:ignore for existing violations** <!-- test: tests/cli_baseline.rs::test_auto_ignore -->
- [ ] **Gradual tightening guide** [no-test] *(documentation)*

---

## v1.0.0 â€” Release

*Production-ready distribution.*

### Distribution
- [ ] **Published to crates.io** [no-test]
- [ ] **Homebrew formula** [no-test]
- [ ] **Scoop/Winget packages** [no-test]

### Documentation
- [ ] **Documentation site** [no-test]
- [ ] **Logo and branding** [no-test]
- [ ] **README finalized** [no-test]
- [ ] **CHANGELOG.md generation** [no-test]
- [ ] **CONTRIBUTING.md guide** [no-test]
- [ ] **Security policy (SECURITY.md)** [no-test]

### Polish
- [ ] **License audit (dependency licenses)** <!-- test: tests/release.rs::test_license_audit -->
- [ ] **Binary size optimization** [no-test]
- [ ] **Startup time benchmarking** [no-test]
- [ ] **Cross-compilation CI (linux/mac/windows)** [no-test]

---

## Principles

1. **Every [x] feature has a verified test** â€” No exceptions (except [no-test])
2. **Reject bad input, don't fix it** â€” Warden is a gatekeeper
3. **Git is the undo system** â€” Don't reinvent version control
4. **Explicit > Magic** â€” Fail loudly on format violations
5. **Containment over craftsmanship** â€” Constraints are safety, not style
6. **Self-hosting** â€” Warden passes its own rules
7. **Context is king** â€” Give AI exactly what it needs, nothing more
8. **Graph over glob** â€” Understand structure, don't just pattern match
9. **Errors are context** â€” Parse failures to understand scope

---

## Not Doing

- **VS Code Extension** â€” IDE lock-in, maintenance burden
- **Watch mode** â€” Complexity without clear benefit
- **Markdown fallback parsing** â€” Enforce format discipline
- **"Smart" fixing** â€” Warden rejects, doesn't repair
- **Full LSP implementation** â€” Use tree-sitter queries, not language servers
- **Multi-repo support** â€” One project at a time
- **Incremental graph updates** â€” Rebuild on each run (fast enough)

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ package.json âˆ‡âˆ‡âˆ‡
{}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/analysis/ast.rs âˆ‡âˆ‡âˆ‡
// src/analysis/ast.rs
use super::checks::{self, CheckContext};
use crate::config::RuleConfig;
use crate::types::Violation;
use tree_sitter::{Language, Parser, Query};

pub struct Analyzer {
    rust_naming: Query,
    rust_complexity: Query,
    rust_banned: Query,
    js_naming: Query,
    js_complexity: Query,
    py_naming: Query,
    py_complexity: Query,
}

impl Default for Analyzer {
    fn default() -> Self {
        Self::new()
    }
}

impl Analyzer {
    #[must_use]
    pub fn new() -> Self {
        Self {
            rust_naming: compile_query(
                tree_sitter_rust::language(),
                "(function_item name: (identifier) @name)",
            ),
            rust_complexity: compile_query(
                tree_sitter_rust::language(),
                r#"
                (if_expression) @branch
                (match_arm) @branch
                (while_expression) @branch
                (for_expression) @branch
                (binary_expression operator: ["&&" "||"]) @branch
            "#,
            ),
            rust_banned: compile_query(
                tree_sitter_rust::language(),
                r"(call_expression function: (field_expression field: (field_identifier) @method)) @call",
            ),
            js_naming: compile_query(
                tree_sitter_typescript::language_typescript(),
                r"
                (function_declaration name: (identifier) @name)
                (method_definition name: (property_identifier) @name)
                (variable_declarator name: (identifier) @name value: [(arrow_function) (function_expression)])
            ",
            ),
            js_complexity: compile_query(
                tree_sitter_typescript::language_typescript(),
                r#"
                (if_statement) @branch
                (for_statement) @branch
                (for_in_statement) @branch
                (while_statement) @branch
                (do_statement) @branch
                (switch_case) @branch
                (catch_clause) @branch
                (ternary_expression) @branch
                (binary_expression operator: ["&&" "||" "??"]) @branch
            "#,
            ),
            py_naming: compile_query(
                tree_sitter_python::language(),
                "(function_definition name: (identifier) @name)",
            ),
            py_complexity: compile_query(
                tree_sitter_python::language(),
                r"
                (if_statement) @branch
                (for_statement) @branch
                (while_statement) @branch
                (except_clause) @branch
                (boolean_operator) @branch
            ",
            ),
        }
    }

    #[must_use]
    pub fn analyze(
        &self,
        lang: &str,
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        let Some(queries) = self.select_language(lang) else {
            return vec![];
        };
        Self::run_analysis(&queries, filename, content, config)
    }

    fn select_language(&self, lang: &str) -> Option<LanguageQueries<'_>> {
        match lang {
            "rs" => Some(self.queries_rust()),
            "js" | "jsx" | "ts" | "tsx" => Some(self.queries_js()),
            "py" => Some(self.queries_python()),
            _ => None,
        }
    }

    fn queries_rust(&self) -> LanguageQueries<'_> {
        LanguageQueries {
            language: tree_sitter_rust::language(),
            naming: &self.rust_naming,
            complexity: &self.rust_complexity,
            banned: Some(&self.rust_banned),
        }
    }

    fn queries_js(&self) -> LanguageQueries<'_> {
        LanguageQueries {
            language: tree_sitter_typescript::language_typescript(),
            naming: &self.js_naming,
            complexity: &self.js_complexity,
            banned: None,
        }
    }

    fn queries_python(&self) -> LanguageQueries<'_> {
        LanguageQueries {
            language: tree_sitter_python::language(),
            naming: &self.py_naming,
            complexity: &self.py_complexity,
            banned: None,
        }
    }

    fn run_analysis(
        queries: &LanguageQueries<'_>,
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        let mut parser = Parser::new();
        if parser.set_language(queries.language).is_err() {
            return vec![];
        }

        let Some(tree) = parser.parse(content, None) else {
            return vec![];
        };

        let mut violations = Vec::new();
        let ctx = CheckContext {
            root: tree.root_node(),
            source: content,
            filename,
            config,
        };

        checks::check_naming(&ctx, queries.naming, &mut violations);
        checks::check_metrics(&ctx, queries.complexity, &mut violations);

        if let Some(banned) = queries.banned {
            checks::check_banned(&ctx, banned, &mut violations);
        }

        violations
    }
}

struct LanguageQueries<'a> {
    language: Language,
    naming: &'a Query,
    complexity: &'a Query,
    banned: Option<&'a Query>,
}

fn compile_query(lang: Language, pattern: &str) -> Query {
    match Query::new(lang, pattern) {
        Ok(q) => q,
        Err(e) => panic!("Invalid tree-sitter query pattern: {e}"),
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/analysis/checks.rs âˆ‡âˆ‡âˆ‡
// src/analysis/checks.rs
use super::metrics;
use crate::config::RuleConfig;
use crate::types::Violation;
use tree_sitter::{Node, Query, QueryCursor, QueryMatch, TreeCursor};

pub struct CheckContext<'a> {
    pub root: Node<'a>,
    pub source: &'a str,
    pub filename: &'a str,
    pub config: &'a RuleConfig,
}

/// Checks for naming violations (function name word count).
pub fn check_naming(ctx: &CheckContext, query: &Query, out: &mut Vec<Violation>) {
    if is_ignored(ctx.filename, &ctx.config.ignore_naming_on) {
        return;
    }

    let mut cursor = QueryCursor::new();
    for m in cursor.matches(query, ctx.root, ctx.source.as_bytes()) {
        let node = m.captures[0].node;
        let name = node.utf8_text(ctx.source.as_bytes()).unwrap_or("?");
        let word_count = count_words(name);

        if word_count > ctx.config.max_function_words {
            out.push(Violation {
                row: node.start_position().row,
                message: format!(
                    "Function '{name}' has {word_count} words (Max: {}). Is it doing too much?",
                    ctx.config.max_function_words
                ),
                law: "LAW OF BLUNTNESS",
            });
        }
    }
}

fn count_words(name: &str) -> usize {
    if name.contains('_') {
        name.split('_').count()
    } else {
        let caps = name.chars().filter(|c| c.is_uppercase()).count();
        if name.chars().next().is_some_and(char::is_uppercase) {
            caps
        } else {
            caps + 1
        }
    }
}

fn is_ignored(filename: &str, patterns: &[String]) -> bool {
    patterns.iter().any(|p| filename.contains(p))
}

/// Checks for complexity metrics (arity, depth, cyclomatic complexity).
pub fn check_metrics(ctx: &CheckContext, complexity_query: &Query, out: &mut Vec<Violation>) {
    traverse_nodes(ctx, |node| {
        let kind = node.kind();
        if kind.contains("function") || kind.contains("method") {
            validate_arity(node, ctx.config.max_function_args, out);
            validate_depth(node, ctx.config.max_nesting_depth, out);
            validate_complexity(
                node,
                ctx.source,
                complexity_query,
                ctx.config.max_cyclomatic_complexity,
                out,
            );
        }
    });
}

fn validate_arity(node: Node, max: usize, out: &mut Vec<Violation>) {
    let args = metrics::count_arguments(node);
    if args > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!(
                "High Arity: Function takes {args} arguments (Max: {max}). Use a Struct."
            ),
            law: "LAW OF COMPLEXITY",
        });
    }
}

fn validate_depth(node: Node, max: usize, out: &mut Vec<Violation>) {
    let depth = metrics::calculate_max_depth(node);
    if depth > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!("Deep Nesting: Max depth is {depth} (Max: {max}). Extract logic."),
            law: "LAW OF COMPLEXITY",
        });
    }
}

fn validate_complexity(
    node: Node,
    source: &str,
    query: &Query,
    max: usize,
    out: &mut Vec<Violation>,
) {
    let score = metrics::calculate_complexity(node, source, query);
    if score > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!("High Complexity: Score is {score} (Max: {max}). Hard to test."),
            law: "LAW OF COMPLEXITY",
        });
    }
}

/// Checks for banned constructs (`.unwrap()` and `.expect()` calls).
pub fn check_banned(ctx: &CheckContext, banned_query: &Query, out: &mut Vec<Violation>) {
    let mut cursor = QueryCursor::new();
    let names = banned_query.capture_names();

    for m in cursor.matches(banned_query, ctx.root, ctx.source.as_bytes()) {
        process_banned_match(&m, names, ctx, out);
    }
}

fn process_banned_match(
    m: &QueryMatch,
    names: &[String],
    ctx: &CheckContext,
    out: &mut Vec<Violation>,
) {
    let mut method_name: Option<&str> = None;
    let mut row = 0;

    for cap in m.captures {
        let capture_name = &names[cap.index as usize];

        if capture_name == "method" {
            method_name = cap.node.utf8_text(ctx.source.as_bytes()).ok();
        }
        if capture_name == "call" {
            row = cap.node.start_position().row;
        }
    }

    if let Some(name) = method_name {
        if name == "unwrap" || name == "expect" {
            out.push(Violation {
                row,
                message: format!("Banned: '.{name}()'. Use '?' or 'unwrap_or'."),
                law: "LAW OF PARANOIA",
            });
        }
    }
}

fn traverse_nodes<F>(ctx: &CheckContext, mut cb: F)
where
    F: FnMut(Node),
{
    let mut cursor = ctx.root.walk();
    loop {
        cb(cursor.node());
        if !advance_cursor(&mut cursor) {
            break;
        }
    }
}

fn advance_cursor(cursor: &mut TreeCursor) -> bool {
    if cursor.goto_first_child() {
        return true;
    }
    while !cursor.goto_next_sibling() {
        if !cursor.goto_parent() {
            return false;
        }
    }
    true
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/analysis/metrics.rs âˆ‡âˆ‡âˆ‡
// src/analysis/metrics.rs
use tree_sitter::{Node, Query, QueryCursor};

/// Calculates the nesting depth of a node.
#[must_use]
pub fn calculate_max_depth(node: Node) -> usize {
    let mut max_depth = 0;
    let mut cursor = node.walk();

    for child in node.children(&mut cursor) {
        if child.kind().contains("block") || child.kind().contains("body") {
            max_depth = std::cmp::max(max_depth, walk_depth(child, 0));
        }
    }
    max_depth
}

fn walk_depth(node: Node, current: usize) -> usize {
    let mut max = current;
    let mut cursor = node.walk();

    for child in node.children(&mut cursor) {
        let kind = child.kind();
        if matches!(
            kind,
            "if_expression"
                | "match_expression"
                | "for_expression"
                | "while_expression"
                | "loop_expression"
                | "if_statement"
                | "for_statement"
                | "for_in_statement"
                | "while_statement"
                | "do_statement"
                | "switch_case"
                | "catch_clause"
                | "try_statement"
                | "closure_expression" // Rust closures
                | "arrow_function" // JS/TS
                | "function_expression" // JS/TS
                | "lambda" // Python
        ) {
            max = std::cmp::max(max, walk_depth(child, current + 1));
        } else {
            max = std::cmp::max(max, walk_depth(child, current));
        }
    }
    max
}

/// Calculates `McCabe` Cyclomatic Complexity.
#[must_use]
pub fn calculate_complexity(node: Node, source: &str, query: &Query) -> usize {
    let mut cursor = QueryCursor::new();
    let mut complexity = 1;
    for _ in cursor.matches(query, node, source.as_bytes()) {
        complexity += 1;
    }
    complexity
}

/// Counts named arguments/parameters.
#[must_use]
pub fn count_arguments(node: Node) -> usize {
    let mut cursor = node.walk();
    for child in node.children(&mut cursor) {
        if child.kind().contains("parameter") || child.kind().contains("argument") {
            return child.named_child_count();
        }
    }
    0
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/analysis/mod.rs âˆ‡âˆ‡âˆ‡
// src/analysis/mod.rs
pub mod ast;
pub mod checks;
pub mod metrics;

use crate::config::Config;
use crate::tokens::Tokenizer;
use crate::types::{FileReport, ScanReport, Violation};
use ast::Analyzer;
use rayon::prelude::*;
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::LazyLock;
use std::time::Instant;

static ANALYZER: LazyLock<Analyzer> = LazyLock::new(Analyzer::new);

pub struct RuleEngine {
    config: Config,
}

impl RuleEngine {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Scans a list of files and returns a structured report.
    #[must_use]
    pub fn scan(&self, files: Vec<PathBuf>) -> ScanReport {
        let start = Instant::now();

        let results: Vec<FileReport> = files
            .into_par_iter()
            .filter_map(|path| self.analyze_file(&path))
            .collect();

        let total_tokens = results.iter().map(|f| f.token_count).sum();
        let total_violations = results.iter().map(|f| f.violations.len()).sum();

        ScanReport {
            files: results,
            total_tokens,
            total_violations,
            duration_ms: start.elapsed().as_millis(),
        }
    }

    fn analyze_file(&self, path: &Path) -> Option<FileReport> {
        let content = fs::read_to_string(path).ok()?;

        // Support C-style, Hash-style, and HTML-style (Markdown) ignores
        if content.contains("// warden:ignore")
            || content.contains("# warden:ignore")
            || content.contains("<!-- warden:ignore -->")
        {
            return None;
        }

        let filename = path.to_string_lossy();
        let token_count = Tokenizer::count(&content);
        let mut violations = Vec::new();

        // 1. Law of Atomicity (checked unless exempted)
        if !self.is_exempt_from_tokens(&filename) && token_count > self.config.rules.max_file_tokens
        {
            violations.push(Violation {
                row: 0,
                message: format!(
                    "File size is {token_count} tokens (Limit: {})",
                    self.config.rules.max_file_tokens
                ),
                law: "LAW OF ATOMICITY",
            });
        }

        // 2. AST Analysis (complexity, nesting, arity, banned calls)
        if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
            let mut ast_violations = ANALYZER.analyze(ext, &filename, &content, &self.config.rules);
            violations.append(&mut ast_violations);
        }

        Some(FileReport {
            path: path.to_path_buf(),
            token_count,
            complexity_score: 0,
            violations,
        })
    }

    fn is_exempt_from_tokens(&self, filename: &str) -> bool {
        self.config
            .rules
            .ignore_tokens_on
            .iter()
            .any(|pattern| filename.contains(pattern))
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/extractor.rs âˆ‡âˆ‡âˆ‡
// src/apply/extractor.rs
use crate::apply::types::FileContent;
use anyhow::Result;
use regex::Regex;
use std::collections::HashMap;

/// Extracts the optional PLAN block.
#[must_use]
pub fn extract_plan(response: &str) -> Option<String> {
    let open_re = Regex::new(r"(?m)^âˆ‡âˆ‡âˆ‡\s*PLAN\s*âˆ‡âˆ‡âˆ‡\s*$").ok()?;
    let close_re = Regex::new(r"(?m)^âˆ†âˆ†âˆ†\s*$").ok()?;

    let start_match = open_re.find(response)?;
    let end_match = close_re.find_at(response, start_match.end())?;

    let content = &response[start_match.end()..end_match.start()];
    Some(content.trim().to_string())
}

/// Extracts file blocks using the Robust Delimiter Protocol (Nabla Format).
///
/// Format:
/// âˆ‡âˆ‡âˆ‡ path/to/file.rs âˆ‡âˆ‡âˆ‡
/// [content]
/// âˆ†âˆ†âˆ†
///
/// # Errors
/// Returns error if regex compilation fails.
pub fn extract_files(response: &str) -> Result<HashMap<String, FileContent>> {
    let mut files = HashMap::new();
    let header_re = Regex::new(r"(?m)^âˆ‡âˆ‡âˆ‡\s*(.+?)\s*âˆ‡âˆ‡âˆ‡\s*$")?;
    let footer_re = Regex::new(r"(?m)^âˆ†âˆ†âˆ†\s*$")?;

    let mut current_pos = 0;

    while let Some(header_match) = header_re.find_at(response, current_pos) {
        current_pos = process_block(response, header_match, &footer_re, &mut files);
    }

    Ok(files)
}

fn process_block(
    response: &str,
    header_match: regex::Match,
    footer_re: &Regex,
    files: &mut HashMap<String, FileContent>,
) -> usize {
    let raw_path = header_match.as_str().replace('âˆ‡', "").trim().to_string();

    // Skip MANIFEST and PLAN blocks (don't write them to disk)
    if raw_path == "MANIFEST" || raw_path == "PLAN" {
        return skip_block(response, header_match.end(), footer_re);
    }

    let content_start = header_match.end();

    if let Some(footer_match) = footer_re.find_at(response, content_start) {
        let content_end = footer_match.start();
        let raw_content = &response[content_start..content_end];
        let clean_content = clean_nabla_content(raw_content);
        let line_count = clean_content.lines().count();

        files.insert(
            raw_path,
            FileContent {
                content: clean_content,
                line_count,
            },
        );
        footer_match.end()
    } else {
        // Malformed/Truncated block, skip head
        content_start
    }
}

fn skip_block(response: &str, start_pos: usize, footer_re: &Regex) -> usize {
    if let Some(footer_match) = footer_re.find_at(response, start_pos) {
        footer_match.end()
    } else {
        start_pos
    }
}

fn clean_nabla_content(raw: &str) -> String {
    // We want to remove the single leading newline that usually follows the header
    // and the single trailing newline before the footer, but keep everything else.
    let content = raw.trim_matches('\n');
    content.to_string()
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/manifest.rs âˆ‡âˆ‡âˆ‡
// src/apply/manifest.rs
use crate::apply::types::{ManifestEntry, Operation};
use anyhow::Result;
use regex::Regex;

/// Parses the delivery manifest block.
/// Supports both Legacy XML and Nabla Protocol.
///
/// # Errors
/// Returns error if regex compilation fails.
pub fn parse_manifest(response: &str) -> Result<Option<Vec<ManifestEntry>>> {
    if let Some((start, end)) = find_nabla_manifest(response)? {
        let block = &response[start..end];
        let entries = parse_manifest_lines(block)?;
        return Ok(Some(entries));
    }

    if let Some((start, end)) = find_legacy_manifest(response)? {
        let block = &response[start..end];
        let entries = parse_manifest_lines(block)?;
        return Ok(Some(entries));
    }

    Ok(None)
}

fn find_nabla_manifest(response: &str) -> Result<Option<(usize, usize)>> {
    // âˆ‡âˆ‡âˆ‡ MANIFEST âˆ‡âˆ‡âˆ‡
    let open_re = Regex::new(r"âˆ‡âˆ‡âˆ‡\s*MANIFEST\s*âˆ‡âˆ‡âˆ‡")?;
    // âˆ†âˆ†âˆ†
    let close_re = Regex::new(r"âˆ†âˆ†âˆ†")?;

    let Some(start_match) = open_re.find(response) else {
        return Ok(None);
    };
    
    // Search for closer AFTER the opener
    let Some(end_match) = close_re.find_at(response, start_match.end()) else {
        return Ok(None);
    };

    Ok(Some((start_match.end(), end_match.start())))
}

fn find_legacy_manifest(response: &str) -> Result<Option<(usize, usize)>> {
    let open_re = Regex::new(r"(?i)<delivery>")?;
    let close_re = Regex::new(r"(?i)</delivery>")?;

    let start_match = open_re.find(response);
    let end_match = close_re.find(response);

    match (start_match, end_match) {
        (Some(s), Some(e)) => Ok(Some((s.end(), e.start()))),
        _ => Ok(None),
    }
}

fn parse_manifest_lines(block: &str) -> Result<Vec<ManifestEntry>> {
    let list_marker_re = Regex::new(r"^\s*(?:[-*]|\d+\.)\s+")?;
    let mut entries = Vec::new();

    for line in block.lines() {
        if let Some(entry) = parse_manifest_line(line, &list_marker_re) {
            entries.push(entry);
        }
    }
    Ok(entries)
}

fn parse_manifest_line(line: &str, marker_re: &Regex) -> Option<ManifestEntry> {
    let trimmed = line.trim();
    if trimmed.is_empty() {
        return None;
    }

    let clean_line = marker_re.replace(trimmed, "");
    let clean_line_ref = clean_line.as_ref();

    if clean_line_ref.trim().is_empty() {
        return None;
    }

    let (path_raw, op) = parse_operation(clean_line_ref);
    let final_path = extract_clean_path(&path_raw);

    if final_path.is_empty() {
        None
    } else {
        Some(ManifestEntry {
            path: final_path,
            operation: op,
        })
    }
}

fn parse_operation(line: &str) -> (String, Operation) {
    let upper = line.to_uppercase();
    if upper.contains("[NEW]") {
        (
            line.replace("[NEW]", "").replace("[new]", ""),
            Operation::New,
        )
    } else if upper.contains("[DELETE]") {
        (
            line.replace("[DELETE]", "").replace("[delete]", ""),
            Operation::Delete,
        )
    } else {
        (line.to_string(), Operation::Update)
    }
}

fn extract_clean_path(raw: &str) -> String {
    raw.split_whitespace().next().unwrap_or(raw).to_string()
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/messages.rs âˆ‡âˆ‡âˆ‡
// src/apply/messages.rs
use crate::apply::types::ApplyOutcome;
use colored::Colorize;

pub fn print_outcome(outcome: &ApplyOutcome) {
    match outcome {
        ApplyOutcome::Success {
            written,
            deleted,
            roadmap_results,
            backed_up,
        } => print_success(written, deleted, roadmap_results, *backed_up),
        ApplyOutcome::ValidationFailure {
            errors,
            missing,
            ai_message,
        } => {
            print_validation_errors(errors, missing);
            print_ai_feedback(ai_message);
        }
        ApplyOutcome::ParseError(e) => println!("{}: {e}", "âš ï¸  Parse Error".red()),
        ApplyOutcome::WriteError(e) => println!("{}: {e}", "ğŸ’¥ Write Error".red()),
    }
}

fn print_success(written: &[String], deleted: &[String], roadmap: &[String], backed_up: bool) {
    println!("{}", "âœ… Apply successful!".green().bold());
    if backed_up {
        println!("   (Backup created in .warden_apply_backup/)");
    }
    println!();
    
    for file in written {
        println!("   {} {file}", "âœ“".green());
    }
    for file in deleted {
        println!("   {} {file}", "âœ—".red());
    }
    
    if !roadmap.is_empty() {
        println!("{}", "\n   Roadmap Updates:".cyan());
        for msg in roadmap {
             println!("   {msg}");
        }
    }
    
    println!();
    println!("Run {} to verify.", "warden check".yellow());
}

fn print_validation_errors(errors: &[String], missing: &[String]) {
    println!("{}", "âŒ Validation Failed".red().bold());

    if !missing.is_empty() {
        println!(
            "{}",
            "\nMissing Files (Declared but not provided):".yellow()
        );
        for f in missing {
            println!("   - {f}");
        }
    }

    if !errors.is_empty() {
        println!("{}", "\nContent Errors:".yellow());
        for e in errors {
            println!("   - {e}");
        }
    }
}

pub fn print_ai_feedback(ai_message: &str) {
    println!();
    println!("{}", "ğŸ“‹ Paste this back to the AI:".cyan().bold());
    println!("{}", "â”€".repeat(60).black());
    println!("{ai_message}");
    println!("{}", "â”€".repeat(60).black());

    if crate::clipboard::copy_to_clipboard(ai_message).is_ok() {
        println!("{}", "âœ“ Copied to clipboard".green());
    }
}

#[must_use]
pub fn format_ai_rejection(missing: &[String], errors: &[String]) -> String {
    use std::fmt::Write;
    let mut msg = String::from("The previous output was rejected by the Warden Protocol.\n\n");

    if !missing.is_empty() {
        msg.push_str("MISSING FILES (Declared in MANIFEST but not found in Nabla blocks):\n");
        for f in missing {
            let _ = writeln!(msg, "- {f}");
        }
        msg.push('\n');
    }

    if !errors.is_empty() {
        msg.push_str("VALIDATION ERRORS:\n");
        let mut hint_dogfood = false;
        for e in errors {
            let _ = writeln!(msg, "- {e}");
            if e.contains("truncation marker") || e.contains("Banned") {
                hint_dogfood = true;
            }
        }
        msg.push('\n');

        if hint_dogfood {
            msg.push_str("TIP: If you are actively 'dogfooding' (testing failure cases) or intentionally using banned patterns, verify the content matches the rules or use '// warden:ignore' on the file/line to bypass.\n\n");
        }
    }

    msg.push_str(
        "Please provide the missing or corrected files using the NABLA PROTOCOL (âˆ‡âˆ‡âˆ‡ ... âˆ†âˆ†âˆ†).",
    );
    msg
}

#[must_use]
pub fn format_verification_failure(output: &str) -> String {
    format!(
        "The changes were applied, but post-application verification failed.\n\nFAILURE LOG:\n{}\n\nPlease fix the implementation so that checks pass.",
        output.trim()
    )
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/mod.rs âˆ‡âˆ‡âˆ‡
// src/apply/mod.rs
pub mod extractor;
pub mod git;
pub mod manifest;
pub mod messages;
pub mod types;
pub mod validator;
pub mod verification;
pub mod writer;

use crate::clipboard;
use crate::roadmap;
use anyhow::{Context, Result};
use colored::Colorize;
use std::io::{self, Write};
use std::path::Path;
use types::{ApplyContext, ApplyOutcome, ExtractedFiles, Manifest};

const INTENT_FILE: &str = ".warden_intent";

/// Runs the apply command logic.
///
/// # Errors
/// Returns error if clipboard access fails.
pub fn run_apply(ctx: &ApplyContext) -> Result<ApplyOutcome> {
    let content = clipboard::read_clipboard().context("Failed to read clipboard")?;
    process_input(&content, ctx)
}

pub fn print_result(outcome: &ApplyOutcome) {
    messages::print_outcome(outcome);
}

/// Processes input content directly.
///
/// # Errors
/// Returns error if extraction, write, or git operations fail.
pub fn process_input(content: &str, ctx: &ApplyContext) -> Result<ApplyOutcome> {
    if content.trim().is_empty() {
        return Ok(ApplyOutcome::ParseError("Clipboard/Input is empty".to_string()));
    }

    let plan_opt = extractor::extract_plan(content);

    if !ensure_consent(plan_opt.as_deref(), ctx)? {
        return Ok(ApplyOutcome::ParseError("Operation cancelled by user.".to_string()));
    }

    let validation = validate_payload(content);
    if !matches!(validation, ApplyOutcome::Success { .. }) {
        // Validation failed immediately (bad format/safety)
        // We do NOT persist intent here because the user likely needs to reprompt entirely.
        return Ok(validation);
    }

    apply_and_verify(content, ctx, plan_opt.as_deref())
}

fn ensure_consent(plan: Option<&str>, ctx: &ApplyContext) -> Result<bool> {
    let Some(p) = plan else {
        if ctx.force || ctx.dry_run {
            return Ok(true);
        }
        println!("{}", "âš ï¸  No PLAN block found. Please ALWAYS include a plan block.".yellow());
        return confirm("Apply these changes without a plan?");
    };

    println!("{}", "ğŸ“‹ PROPOSED PLAN:".cyan().bold());
    println!("{}", "â”€".repeat(50).dimmed());
    println!("{}", p.trim());
    println!("{}", "â”€".repeat(50).dimmed());

    if ctx.force || ctx.dry_run {
        return Ok(true);
    }

    validate_plan_structure(p);
    confirm("Apply these changes?")
}

fn validate_payload(content: &str) -> ApplyOutcome {
    let manifest = match parse_manifest_step(content) {
        Ok(m) => m,
        Err(e) => return ApplyOutcome::ParseError(e),
    };

    let extracted = match extract_files_step(content) {
        Ok(e) => e,
        Err(e) => return ApplyOutcome::ParseError(e),
    };

    validator::validate(&manifest, &extracted)
}

fn apply_and_verify(content: &str, ctx: &ApplyContext, plan: Option<&str>) -> Result<ApplyOutcome> {
    let extracted = extractor::extract_files(content)?;
    let manifest = manifest::parse_manifest(content)?.unwrap_or_default();

    if ctx.dry_run {
        return Ok(ApplyOutcome::Success {
            written: vec!["(Dry Run) Files verified".to_string()],
            deleted: vec![],
            roadmap_results: vec![],
            backed_up: false,
        });
    }

    let mut outcome = writer::write_files(&manifest, &extracted, None)?;

    // Handle roadmap updates
    let roadmap_path = Path::new("ROADMAP.md");
    let mut roadmap_results = Vec::new();
    if roadmap_path.exists() {
        match roadmap::handle_input(roadmap_path, content) {
            Ok(results) => roadmap_results = results,
            Err(e) => eprintln!("{} Roadmap update failed: {e}", "âš ï¸".yellow()),
        }
    }
    if let ApplyOutcome::Success { roadmap_results: ref mut rr, .. } = outcome {
        rr.append(&mut roadmap_results);
    }

    verify_and_commit(&outcome, ctx, plan)?;
    Ok(outcome)
}

fn verify_and_commit(outcome: &ApplyOutcome, ctx: &ApplyContext, plan: Option<&str>) -> Result<()> {
    if !matches!(outcome, ApplyOutcome::Success { .. }) {
        return Ok(());
    }
    
    if !has_changes(outcome) {
         println!("{}", "No changes detected.".yellow());
         return Ok(());
    }

    let (success, log) = verification::verify_application(ctx)?;
    
    if success {
        handle_success(plan);
    } else {
        let msg = messages::format_verification_failure(&log);
        handle_failure(plan, &msg);
    }
    Ok(())
}

fn has_changes(outcome: &ApplyOutcome) -> bool {
    if let ApplyOutcome::Success { written, deleted, roadmap_results, .. } = outcome {
        !written.is_empty() || !deleted.is_empty() || !roadmap_results.is_empty()
    } else {
        false
    }
}

fn handle_success(plan: Option<&str>) {
    println!("{}", "\nâœ¨ Verification Passed. Committing & Pushing...".green().bold());
    let message = construct_commit_message(plan);
    if let Err(e) = git::commit_and_push(&message) {
        eprintln!("{} Git operation failed: {e}", "âš ï¸".yellow());
    } else {
        clear_intent();
    }
}

fn handle_failure(plan: Option<&str>, failure_log: &str) {
    println!("{}", "\nâŒ Verification Failed. Changes applied but NOT committed.".red().bold());
    println!("Fix the issues manually and then commit.");
    
    // Auto-copy failure log
    messages::print_ai_feedback(failure_log);

    if let Some(p) = plan {
         save_intent(p);
    }
}

fn save_intent(plan: &str) {
    // Only save if no intent exists (preserve the original goal)
    if !Path::new(INTENT_FILE).exists() {
        let clean = plan.replace("GOAL:", "").trim().to_string();
        // Ignore errors silently (best effort)
        let _ = std::fs::write(INTENT_FILE, clean);
    }
}

fn clear_intent() {
    let _ = std::fs::remove_file(INTENT_FILE);
}

fn construct_commit_message(current_plan: Option<&str>) -> String {
    let current = current_plan.unwrap_or("Automated update").replace("GOAL:", "").trim().to_string();
    
    if let Ok(stored) = std::fs::read_to_string(INTENT_FILE) {
        let stored = stored.trim();
        if !stored.is_empty() && stored != current {
            return format!("{stored}\n\nFollow-up: {current}");
        }
    }
    current
}

fn validate_plan_structure(plan: &str) {
    if !plan.contains("GOAL:") || !plan.contains("CHANGES:") {
        println!("{}", "âš ï¸  Plan is unstructured (missing GOAL/CHANGES).".yellow());
    }
}

fn confirm(prompt: &str) -> Result<bool> {
    print!("{prompt} [y/N] ");
    io::stdout().flush()?;
    let mut input = String::new();
    io::stdin().read_line(&mut input)?;
    Ok(input.trim().eq_ignore_ascii_case("y"))
}

fn parse_manifest_step(content: &str) -> Result<Manifest, String> {
    match manifest::parse_manifest(content) {
        Ok(Some(m)) => Ok(m),
        Ok(None) => Ok(Vec::new()),
        Err(e) => Err(format!("Manifest Error: {e}")),
    }
}

fn extract_files_step(content: &str) -> Result<ExtractedFiles, String> {
    extractor::extract_files(content).map_err(|e| format!("Extraction Error: {e}"))
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/types.rs âˆ‡âˆ‡âˆ‡
// src/apply/types.rs
use crate::config::Config;
use std::collections::HashMap;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum Operation {
    Update,
    New,
    Delete,
}

#[derive(Debug, Clone)]
pub struct ManifestEntry {
    pub path: String,
    pub operation: Operation,
}

#[derive(Debug, Clone)]
pub struct FileContent {
    pub content: String,
    pub line_count: usize,
}

#[derive(Debug)]
pub enum ApplyOutcome {
    Success {
        written: Vec<String>,
        deleted: Vec<String>,
        roadmap_results: Vec<String>, // Added field
        backed_up: bool,
    },
    ValidationFailure {
        errors: Vec<String>,
        missing: Vec<String>,
        ai_message: String,
    },
    ParseError(String),
    WriteError(String),
}

/// Context for the apply operation.
/// Connects project config with runtime flags.
pub struct ApplyContext<'a> {
    pub config: &'a Config,
    pub force: bool,   // Skips interactive confirmation (for tests/automation)
    pub dry_run: bool, // Skips disk writes (for tests)
}

impl<'a> ApplyContext<'a> {
    #[must_use]
    pub fn new(config: &'a Config) -> Self {
        Self {
            config,
            force: false,
            dry_run: false,
        }
    }
}

// The manifest is just a list of entries
pub type Manifest = Vec<ManifestEntry>;

// The extracted files are mapped by path
pub type ExtractedFiles = HashMap<String, FileContent>;
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/validator.rs âˆ‡âˆ‡âˆ‡
// src/apply/validator.rs
use crate::apply::messages;
use crate::apply::types::{ApplyOutcome, ExtractedFiles, Manifest, Operation};
use crate::roadmap::{diff, Roadmap, Command, MovePosition};
use regex::Regex;
use std::path::Path;
use std::sync::LazyLock;
use std::fmt::Write;

const SENSITIVE_PATHS: &[&str] = &[
    ".git/",
    ".env",
    ".ssh/",
    ".aws/",
    ".gnupg/",
    "id_rsa",
    "id_ed25519",
    "credentials",
    ".warden_apply_backup/",
];

const ALLOWED_DOTFILES: &[&str] = &[
    ".gitignore",
    ".wardenignore",
    ".warden_intent",
];

static LAZY_MARKERS: LazyLock<Vec<Regex>> = LazyLock::new(|| {
    [
        r"^\s*//\s*\.{3,}\s*$",
        r"^\s*/\*\s*\.{3,}\s*\*/\s*$",
        r"(?i)^\s*//.*(rest of|remaining|existing|implement|logic here).*$",
        r"^\s*#\s*\.{3,}\s*$",
    ]
    .iter()
    .filter_map(|pattern| match Regex::new(pattern) {
        Ok(re) => Some(re),
        Err(e) => {
            eprintln!("Warning: Invalid lazy marker pattern '{pattern}': {e}");
            None
        }
    })
    .collect()
});

#[must_use]
pub fn validate(manifest: &Manifest, extracted: &ExtractedFiles) -> ApplyOutcome {
    let mut errors = Vec::new();
    
    // Check path safety and handle ROADMAP.md specifically
    for path in extracted.keys() {
         if path.eq_ignore_ascii_case("ROADMAP.md") {
             if let Some(outcome) = handle_roadmap_rewrite(path, &extracted[path].content) {
                 return outcome;
             }
             // If we couldn't infer commands or load the file, default to standard block
             errors.push(
                "PROTECTED: ROADMAP.md is managed programmatically. Use 'warden roadmap apply' commands instead of rewriting the file.".to_string(),
             );
         } else {
             validate_single_path(path, &mut errors);
         }
    }

    if !errors.is_empty() {
        let ai_message = messages::format_ai_rejection(&[], &errors);
        return ApplyOutcome::ValidationFailure {
            errors,
            missing: Vec::new(),
            ai_message,
        };
    }

    let missing = check_missing(manifest, extracted);
    let content_errors = check_content(extracted);

    if !missing.is_empty() || !content_errors.is_empty() {
        let ai_message = messages::format_ai_rejection(&missing, &content_errors);
        return ApplyOutcome::ValidationFailure {
            errors: content_errors,
            missing,
            ai_message,
        };
    }

    let written = extracted.keys().cloned().collect();
    let deleted = manifest
        .iter()
        .filter(|e| e.operation == Operation::Delete)
        .map(|e| e.path.clone())
        .collect();

    ApplyOutcome::Success {
        written,
        deleted,
        roadmap_results: Vec::new(),
        backed_up: true,
    }
}

/// Attempts to diff the incoming ROADMAP.md with the existing one // warden:ignore
/// and return a specific rejection message with proposed commands.
fn handle_roadmap_rewrite(path: &str, incoming_content: &str) -> Option<ApplyOutcome> {
    let local_path = Path::new(path);
    if !local_path.exists() {
        return None;
    }

    let Ok(current) = Roadmap::from_file(local_path) else {
        return None;
    };
    
    let incoming = Roadmap::parse(incoming_content);
    let commands = diff::diff(&current, &incoming);
    
    if commands.is_empty() {
        return None;
    }

    let mut msg = String::new();
    let _ = writeln!(msg, "The Warden Protocol blocked a direct rewrite of ROADMAP.md.\n");
    let _ = writeln!(msg, "However, I inferred your intent. Please use these commands instead:\n");
    let _ = writeln!(msg, "âˆ‡âˆ‡âˆ‡ ROADMAP âˆ‡âˆ‡âˆ‡");
    let _ = writeln!(msg, "===ROADMAP===");
    
    for cmd in commands {
        match cmd {
            Command::Check { path } => { let _ = writeln!(msg, "CHECK {path}"); },
            Command::Uncheck { path } => { let _ = writeln!(msg, "UNCHECK {path}"); },
            Command::Update { path, text } => { let _ = writeln!(msg, "UPDATE {path} \"{text}\""); },
            Command::Add { parent, text, .. } => { let _ = writeln!(msg, "ADD {parent} \"{text}\""); },
            Command::Delete { path } => { let _ = writeln!(msg, "DELETE {path}"); },
            Command::AddSection { heading } => { let _ = writeln!(msg, "SECTION \"{heading}\""); },
            Command::Move { path, position } => {
                match position {
                    MovePosition::After(t) => { let _ = writeln!(msg, "MOVE {path} AFTER {t}"); },
                    MovePosition::Before(t) => { let _ = writeln!(msg, "MOVE {path} BEFORE {t}"); },
                    MovePosition::EndOfSection(s) => { let _ = writeln!(msg, "MOVE {path} TO {s}"); },
                }
            },
            _ => {}
        }
    }
    
    let _ = writeln!(msg, "===END===");
    let _ = writeln!(msg, "âˆ†âˆ†âˆ†");

    Some(ApplyOutcome::ValidationFailure {
        errors: vec!["Roadmap rewrite converted to commands".to_string()],
        missing: vec![],
        ai_message: msg,
    })
}

fn validate_single_path(path: &str, errors: &mut Vec<String>) {
    if path.eq_ignore_ascii_case("ROADMAP.md") {
        // Handled in main loop
        return;
    }

    if has_traversal(path) {
        errors.push(format!(
            "SECURITY: path contains directory traversal: {path}"
        ));
        return;
    }

    if is_absolute_path(path) {
        errors.push(format!("SECURITY: absolute path not allowed: {path}"));
        return;
    }

    if is_sensitive_path(path) {
        errors.push(format!("SECURITY: sensitive path blocked: {path}"));
        return;
    }

    if is_hidden_file(path) {
        errors.push(format!("SECURITY: hidden file not allowed: {path}"));
    }
}

fn has_traversal(path: &str) -> bool {
    path.contains("../") || path.starts_with("..")
}

fn is_absolute_path(path: &str) -> bool {
    if path.starts_with('/') {
        return true;
    }
    let bytes = path.as_bytes();
    bytes.len() >= 2 && bytes[0].is_ascii_alphabetic() && bytes[1] == b':'
}

fn is_sensitive_path(path: &str) -> bool {
    let lower = path.to_lowercase();
    SENSITIVE_PATHS.iter().any(|s| lower.contains(s))
}

fn is_hidden_file(path: &str) -> bool {
    if ALLOWED_DOTFILES.iter().any(|&f| path.ends_with(f)) {
        return false;
    }

    path.split('/')
        .filter(|s| !s.is_empty())
        .any(|seg| seg.starts_with('.') && seg != "." && seg != "..")
}

fn check_missing(manifest: &Manifest, extracted: &ExtractedFiles) -> Vec<String> {
    manifest
        .iter()
        .filter(|entry| entry.operation != Operation::Delete)
        .filter(|entry| !extracted.contains_key(&entry.path))
        .map(|entry| entry.path.clone())
        .collect()
}

fn check_content(extracted: &ExtractedFiles) -> Vec<String> {
    let mut errors = Vec::new();
    for (path, file) in extracted {
        check_single_file(path, &file.content, &mut errors);
    }
    errors
}

fn check_single_file(path: &str, content: &str, errors: &mut Vec<String>) {
    if content.trim().is_empty() {
        errors.push(format!("{path} is empty"));
        return;
    }
    check_lazy_truncation(path, content, errors);
}

fn check_lazy_truncation(path: &str, content: &str, errors: &mut Vec<String>) {
    for (line_num, line) in content.lines().enumerate() {
        if line.contains("warden:ignore") {
            continue;
        }

        for regex in LAZY_MARKERS.iter() {
            if regex.is_match(line) {
                errors.push(format!(
                    "{path}:{}: Detected lazy truncation marker: '{}'. Full file required.",
                    line_num + 1,
                    line.trim()
                ));
            }
        }
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/verification.rs âˆ‡âˆ‡âˆ‡
// src/apply/verification.rs
use crate::apply::types::ApplyContext;
use anyhow::Result;
use colored::Colorize;
use std::fmt::Write as FmtWrite;
use std::process::Command;

/// Runs configured checks and Warden scan to verify application.
/// Returns `(success, log_output)`.
///
/// # Errors
/// Returns error if command execution fails.
pub fn verify_application(ctx: &ApplyContext) -> Result<(bool, String)> {
    println!("{}", "\nğŸ” Verifying changes...".blue().bold());
    let mut log_buffer = String::new();

    if let Some(commands) = ctx.config.commands.get("check") {
        for cmd in commands {
            let (success, output) = run_check_command(cmd)?;
            let _ = writeln!(log_buffer, "> {cmd}\n{output}");

            if !success {
                return Ok((false, log_buffer));
            }
        }
    }

    println!("Running structural scan...");
    let (success, output) = run_warden_check()?;
    let _ = writeln!(log_buffer, "> warden scan\n{output}");

    Ok((success, log_buffer))
}

fn run_check_command(cmd: &str) -> Result<(bool, String)> {
    println!("Running check: {}", cmd.dimmed());
    let parts: Vec<&str> = cmd.split_whitespace().collect();
    let Some((prog, args)) = parts.split_first() else {
        return Ok((true, String::new()));
    };

    let output = Command::new(prog).args(args).output()?;
    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    print!("{stdout}");
    eprint!("{stderr}");

    let combined = format!("{stdout}\n{stderr}");
    Ok((output.status.success(), combined))
}

fn run_warden_check() -> Result<(bool, String)> {
    let output = Command::new("warden").output()?;
    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    print!("{stdout}");
    eprint!("{stderr}");

    let combined = format!("{stdout}\n{stderr}");
    Ok((output.status.success(), combined))
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/apply/writer.rs âˆ‡âˆ‡âˆ‡
// src/apply/writer.rs
use crate::apply::types::{ApplyOutcome, ExtractedFiles, Manifest, Operation};
use anyhow::{anyhow, Context, Result};
use std::fs;
use std::path::{Path, PathBuf};
use std::time::{SystemTime, UNIX_EPOCH};

const BACKUP_DIR: &str = ".warden_apply_backup";

/// Writes changes (updates, new files, deletes) to disk.
///
/// # Errors
/// Returns error if file system operations fail.
pub fn write_files(
    manifest: &Manifest,
    files: &ExtractedFiles,
    root: Option<&Path>,
) -> Result<ApplyOutcome> {
    let backup_path = create_backup(manifest, root)?;
    let mut written = Vec::new();
    let mut deleted = Vec::new();

    for entry in manifest {
        match entry.operation {
            Operation::Delete => {
                delete_file(&entry.path, root)?;
                deleted.push(entry.path.clone());
            }
            Operation::Update | Operation::New => {
                if let Some(file_data) = files.get(&entry.path) {
                    write_single_file(&entry.path, &file_data.content, root)?;
                    written.push(entry.path.clone());
                }
            }
        }
    }

    Ok(ApplyOutcome::Success {
        written,
        deleted,
        roadmap_results: Vec::new(),
        backed_up: backup_path.is_some(),
    })
}

fn delete_file(path_str: &str, root: Option<&Path>) -> Result<()> {
    let path = resolve_path(path_str, root);
    if path.exists() {
        fs::remove_file(&path).with_context(|| format!("Failed to delete {}", path.display()))?;
    }
    Ok(())
}

fn write_single_file(path_str: &str, content: &str, root: Option<&Path>) -> Result<()> {
    let path = resolve_path(path_str, root);

    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent)
            .map_err(|e| anyhow!("Failed to create directory {}: {e}", parent.display()))?;
    }
    fs::write(&path, content).map_err(|e| anyhow!("Failed to write {}: {e}", path.display()))?;
    Ok(())
}

fn resolve_path(path_str: &str, root: Option<&Path>) -> PathBuf {
    match root {
        Some(r) => r.join(path_str),
        None => PathBuf::from(path_str),
    }
}

fn create_backup(manifest: &Manifest, root: Option<&Path>) -> Result<Option<PathBuf>> {
    let targets: Vec<&String> = manifest
        .iter()
        .map(|e| &e.path)
        .filter(|p| resolve_path(p, root).exists())
        .collect();

    if targets.is_empty() {
        return Ok(None);
    }

    let timestamp = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs();
    let root_path = root.map_or_else(|| PathBuf::from("."), Path::to_path_buf);
    let backup_folder = root_path.join(BACKUP_DIR).join(timestamp.to_string());

    fs::create_dir_all(&backup_folder).context("Failed to create backup directory")?;

    for path_str in targets {
        backup_single_file(path_str, &backup_folder, root)?;
    }

    Ok(Some(backup_folder))
}

fn backup_single_file(path_str: &str, backup_folder: &Path, root: Option<&Path>) -> Result<()> {
    let src = resolve_path(path_str, root);
    let dest = backup_folder.join(path_str);

    if let Some(parent) = dest.parent() {
        fs::create_dir_all(parent)?;
    }

    fs::copy(&src, &dest).with_context(|| format!("Failed to backup {}", src.display()))?;
    Ok(())
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/bin/warden.rs âˆ‡âˆ‡âˆ‡
use anyhow::Result;
use clap::{Parser, Subcommand};
use colored::Colorize;
use std::fs;
use std::io;
use std::path::{Path, PathBuf};
use std::process::{self, Command};

use warden_core::analysis::RuleEngine;
use warden_core::apply;
use warden_core::apply::types::ApplyContext;
use warden_core::config::Config;
use warden_core::discovery;
use warden_core::pack::{self, OutputFormat, PackOptions};
use warden_core::prompt::PromptGenerator;
use warden_core::reporting;
use warden_core::roadmap::cli::{handle_command, RoadmapCommand};
use warden_core::tui::state::App;
use warden_core::types::ScanReport;
use warden_core::wizard;

#[derive(Parser)]
#[command(name = "warden")]
#[command(version)]
#[command(about = "Code quality guardian", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Option<Commands>,
    #[arg(long)]
    ui: bool,
    #[arg(long)]
    init: bool,
}

#[derive(Subcommand)]
enum Commands {
    Prompt {
        #[arg(long, short)]
        copy: bool,
    },
    Check,
    Fix,
    Apply,
    /// Open the interactive configuration editor
    Config,
    #[command(subcommand)]
    Roadmap(RoadmapCommand),
    Pack {
        #[arg(long, short)]
        stdout: bool,
        #[arg(long, short)]
        copy: bool,
        /// Skip including the system prompt (prompt is included by default)
        #[arg(long)]
        noprompt: bool,
        #[arg(long, value_enum, default_value_t = OutputFormat::Text)]
        format: OutputFormat,
        /// Force skeletonization of all files (overrides focus target)
        #[arg(long)]
        skeleton: bool,
        #[arg(long)]
        git_only: bool,
        #[arg(long)]
        no_git: bool,
        #[arg(long)]
        code_only: bool,
        #[arg(long, short)]
        verbose: bool,
        /// Focus on a specific file (others will be skeletonized)
        #[arg(value_name = "TARGET")]
        target: Option<PathBuf>,
    },
}

fn main() {
    if let Err(e) = run() {
        eprintln!("{} {e}", "error:".red().bold());
        process::exit(1);
    }
}

fn run() -> Result<()> {
    let cli = Cli::parse();

    if cli.init {
        return wizard::run();
    }

    ensure_config_exists();
    dispatch_command(&cli)
}

fn dispatch_command(cli: &Cli) -> Result<()> {
    match &cli.command {
        Some(cmd) => dispatch_subcommand(cmd),
        None => dispatch_default(cli.ui),
    }
}

fn dispatch_subcommand(cmd: &Commands) -> Result<()> {
    match cmd {
        Commands::Prompt { copy } => handle_prompt(*copy),
        Commands::Check => run_command("check"),
        Commands::Fix => run_command("fix"),
        Commands::Apply => handle_apply(),
        Commands::Config => warden_core::tui::run_config(),
        Commands::Roadmap(cmd) => handle_command(cmd.clone()),
        Commands::Pack {
            stdout,
            copy,
            noprompt,
            format,
            skeleton,
            git_only,
            no_git,
            code_only,
            verbose,
            target,
        } => pack::run(&PackOptions {
            stdout: *stdout,
            copy: *copy,
            prompt: !*noprompt,
            format: format.clone(),
            skeleton: *skeleton,
            git_only: *git_only,
            no_git: *no_git,
            code_only: *code_only,
            verbose: *verbose,
            target: target.clone(),
        }),
    }
}

fn dispatch_default(ui: bool) -> Result<()> {
    if ui {
        run_tui()
    } else {
        run_scan()
    }
}

fn handle_apply() -> Result<()> {
    let mut config = Config::new();
    config.load_local_config();

    let ctx = ApplyContext::new(&config);
    let outcome = apply::run_apply(&ctx)?;
    apply::print_result(&outcome);
    Ok(())
}

fn ensure_config_exists() {
    if Path::new("warden.toml").exists() {
        return;
    }
    // Default to Standard strictness if auto-generating without wizard
    let project = warden_core::project::ProjectType::detect();
    let content = warden_core::project::generate_toml(
        project, 
        warden_core::project::Strictness::Standard
    );
    
    if fs::write("warden.toml", &content).is_ok() {
        eprintln!("{}", "ğŸ“ Created warden.toml".dimmed());
    }
}

fn handle_prompt(copy: bool) -> Result<()> {
    let mut config = Config::new();
    config.load_local_config();
    let gen = PromptGenerator::new(config.rules.clone());
    let prompt = gen.generate()?;
    if copy {
        warden_core::clipboard::copy_to_clipboard(&prompt)?;
        println!("{}", "âœ“ Copied to clipboard".green());
    } else {
        println!("{prompt}");
    }
    Ok(())
}

fn run_command(name: &str) -> Result<()> {
    let mut config = Config::new();
    config.load_local_config();

    let Some(commands) = config.commands.get(name) else {
        eprintln!(
            "{} No '{}' command configured in warden.toml",
            "error:".red(),
            name
        );
        process::exit(1);
    };

    println!("{} Running '{}' pipeline...", "ğŸš€".green(), name);

    for cmd_str in commands {
        println!("   {} {}", "exec:".dimmed(), cmd_str.dimmed());
        let parts: Vec<&str> = cmd_str.split_whitespace().collect();
        let (prog, args) = parts.split_first().unwrap_or((&"", &[]));

        let status = Command::new(prog).args(args).status();

        match status {
            Ok(s) if !s.success() => exit_with_code(s.code().unwrap_or(1))?,
            Ok(_) => {}
            Err(e) => {
                handle_exec_error(&e, prog);
                process::exit(1);
            }
        }
    }
    Ok(())
}

fn exit_with_code(code: i32) -> Result<()> {
    eprintln!("{} Command failed with exit code {code}", "âŒ".red());
    process::exit(code);
}

fn handle_exec_error(e: &std::io::Error, prog: &str) {
    if e.kind() == io::ErrorKind::NotFound {
        eprintln!("{} Command not found: {prog}", "error:".red());
        eprintln!("  Check that the program is installed and in PATH");
    } else {
        eprintln!("{} Failed to execute: {e}", "error:".red());
    }
}

fn run_scan() -> Result<()> {
    let config = load_config();
    let files = discovery::discover(&config)?;
    let report = scan_files(&config, files);

    reporting::print_report(&report)?;

    if report.has_errors() {
        process::exit(1);
    }
    Ok(())
}

fn run_tui() -> Result<()> {
    let config = load_config();
    let files = discovery::discover(&config)?;
    let report = scan_files(&config, files);
    run_tui_with_report(report)
}

fn load_config() -> Config {
    let mut config = Config::new();
    config.load_local_config();
    config
}

fn scan_files(config: &Config, files: Vec<std::path::PathBuf>) -> ScanReport {
    RuleEngine::new(config.clone()).scan(files)
}

fn run_tui_with_report(report: ScanReport) -> Result<()> {
    use crossterm::{
        event::{DisableMouseCapture, EnableMouseCapture},
        execute,
        terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
    };
    use ratatui::backend::CrosstermBackend;
    use ratatui::Terminal;

    enable_raw_mode()?;
    let mut stdout = io::stdout();
    execute!(stdout, EnterAlternateScreen, EnableMouseCapture)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    let mut app = App::new(report);
    let res = app.run(&mut terminal);

    disable_raw_mode()?;
    execute!(
        terminal.backend_mut(),
        LeaveAlternateScreen,
        DisableMouseCapture
    )?;
    terminal.show_cursor()?;

    res
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/clipboard.rs âˆ‡âˆ‡âˆ‡
#![allow(unused_imports)] // Context is used on some OS targets but not others

use anyhow::{Context, Result};
use std::fs;
use std::path::{Path, PathBuf};
use std::process::Command;
use std::time::{SystemTime, UNIX_EPOCH};
use crate::tokens::Tokenizer;

const TEMP_PREFIX: &str = "warden_clipboard_";

// --- Public API ---

/// Smartly copies text or file handles based on size.
///
/// # Errors
/// Returns error if clipboard access fails or temp file creation fails.
pub fn smart_copy(text: &str) -> Result<String> {
    // 1. The Garbage Man: Clean up old artifacts first
    cleanup_temp_files();

    // 2. Check Size
    let token_count = Tokenizer::count(text);

    if token_count < 1500 {
        // Small? Text Copy.
        perform_copy(text)?;
        Ok("Text copied to clipboard".to_string())
    } else {
        // Huge? File Copy.
        let file_path = write_to_temp(text)?;
        copy_file_handle(&file_path)?;
        
        let filename = file_path
            .file_name()
            .map_or_else(|| "temp_file".into(), |n| n.to_string_lossy());

        Ok(format!(
            "Large content ({token_count} tokens). Copied as file attachment: {filename}"
        ))
    }
}

/// Copies a file path to clipboard so it can be pasted as a file attachment.
///
/// # Errors
/// Returns error if clipboard access fails.
pub fn copy_file_path(path: &Path) -> Result<()> {
    copy_file_handle(path)
}

/// Wrapper for backward compatibility. // warden:ignore
///
/// # Errors
/// Returns error if clipboard access fails.
pub fn copy_to_clipboard(text: &str) -> Result<()> {
    let _ = smart_copy(text)?;
    Ok(())
}

/// Reads text from the system clipboard.
///
/// # Errors
/// Returns error if clipboard access fails.
pub fn read_clipboard() -> Result<String> {
    perform_read()
}

// --- Internal Logic ---

fn write_to_temp(content: &str) -> Result<PathBuf> {
    let timestamp = SystemTime::now()
        .duration_since(UNIX_EPOCH)?
        .as_nanos();
        
    let filename = format!("{TEMP_PREFIX}{timestamp}.txt");
    let mut temp_path = std::env::temp_dir();
    temp_path.push(filename);

    fs::write(&temp_path, content)?;
    Ok(temp_path)
}

fn cleanup_temp_files() {
    let temp_dir = std::env::temp_dir();
    let Ok(entries) = fs::read_dir(temp_dir) else { return; };

    let now = SystemTime::now();
    let fifteen_mins = std::time::Duration::from_secs(15 * 60);

    for entry in entries.flatten() {
        let path = entry.path();
        if should_delete(&path, now, fifteen_mins) {
             let _ = fs::remove_file(path);
        }
    }
}

// Helper to reduce cyclomatic complexity of cleanup_temp_files
fn should_delete(path: &Path, now: SystemTime, limit: std::time::Duration) -> bool {
    let Some(name) = path.file_name().and_then(|n| n.to_str()) else {
        return false;
    };
    
    if !name.starts_with(TEMP_PREFIX) {
        return false;
    }

    let Ok(metadata) = fs::metadata(path) else { return false; };
    let Ok(modified) = metadata.modified() else { return false; };

    now.duration_since(modified).unwrap_or_default() > limit
}

// --- Platform Specifics (File Handles) ---

#[cfg(target_os = "windows")]
fn copy_file_handle(path: &Path) -> Result<()> {
    let path_str = path.to_string_lossy();
    // Escape single quotes for PowerShell (replace ' with '')
    let escaped_path = path_str.replace('\'', "''");
    let cmd = format!("Set-Clipboard -Path '{escaped_path}'");

    Command::new("powershell")
        .args(["-NoProfile", "-NonInteractive", "-Command", &cmd])
        .output()
        .context("Failed to set clipboard via PowerShell")?;
    Ok(())
}

#[cfg(target_os = "macos")]
fn copy_file_handle(path: &Path) -> Result<()> {
    let path_str = path.to_string_lossy();
    let script = format!("set the clipboard to POSIX file \"{path_str}\"");
    
    Command::new("osascript")
        .arg("-e")
        .arg(&script)
        .output()
        .context("Failed to set clipboard via osascript")?;
    Ok(())
}

#[cfg(target_os = "linux")]
fn copy_file_handle(path: &Path) -> Result<()> {
    let path_str = path.to_string_lossy();
    let uri = format!("file://{path_str}");
    
    // Try wl-copy (Wayland) first with proper MIME type
    if let Ok(mut child) = Command::new("wl-copy")
        .args(["--type", "text/uri-list"])
        .stdin(std::process::Stdio::piped())
        .spawn()
    {
        if let Some(mut stdin) = child.stdin.take() {
            use std::io::Write;
            let _ = write!(stdin, "{uri}");
        }
        if child.wait().is_ok() {
            return Ok(());
        }
    }

    // X11 fallback with xclip
    let mut child = Command::new("xclip")
        .args(["-selection", "clipboard", "-t", "text/uri-list", "-i"])
        .stdin(std::process::Stdio::piped())
        .spawn()?;

    if let Some(mut stdin) = child.stdin.take() {
        use std::io::Write;
        write!(stdin, "{uri}")?;
    }
    child.wait()?;
    Ok(())
}

// --- Platform Specifics (Text Read/Write) ---

#[cfg(target_os = "macos")]
fn perform_copy(text: &str) -> Result<()> {
    use std::io::Write;
    let mut child = Command::new("pbcopy")
        .stdin(std::process::Stdio::piped())
        .spawn()?;
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(text.as_bytes())?;
    }
    child.wait()?;
    Ok(())
}

#[cfg(target_os = "macos")]
fn perform_read() -> Result<String> {
    let output = Command::new("pbpaste").output()?;
    Ok(String::from_utf8_lossy(&output.stdout).to_string())
}

#[cfg(target_os = "linux")]
fn perform_copy(text: &str) -> Result<()> {
    use std::io::Write;
    // Try xclip
    if let Ok(mut child) = Command::new("xclip")
        .args(["-selection", "clipboard", "-in"])
        .stdin(std::process::Stdio::piped())
        .spawn()
    {
        if let Some(mut stdin) = child.stdin.take() {
            stdin.write_all(text.as_bytes())?;
        }
        child.wait()?;
        return Ok(());
    }

    // Fallback to wl-copy
    let mut child = Command::new("wl-copy")
        .stdin(std::process::Stdio::piped())
        .spawn()?;
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(text.as_bytes())?;
    }
    child.wait()?;
    Ok(())
}

#[cfg(target_os = "linux")]
fn perform_read() -> Result<String> {
    if let Ok(output) = Command::new("xclip")
        .args(["-selection", "clipboard", "-out"])
        .output()
    {
        return Ok(String::from_utf8_lossy(&output.stdout).to_string());
    }
    let output = Command::new("wl-paste").output()?;
    Ok(String::from_utf8_lossy(&output.stdout).to_string())
}

#[cfg(target_os = "windows")]
fn perform_copy(text: &str) -> Result<()> {
    use std::io::Write;
    let mut child = Command::new("clip")
        .stdin(std::process::Stdio::piped())
        .spawn()?;
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(text.as_bytes())?;
    }
    child.wait()?;
    Ok(())
}

#[cfg(target_os = "windows")]
fn perform_read() -> Result<String> {
    let output = Command::new("powershell")
        .args(["-command", "Get-Clipboard"])
        .output()?;
    Ok(String::from_utf8_lossy(&output.stdout).to_string())
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/config.rs âˆ‡âˆ‡âˆ‡
// src/config.rs
pub use crate::constants::{
    BIN_EXT_PATTERN, CODE_BARE_PATTERN, CODE_EXT_PATTERN, PRUNE_DIRS, SECRET_PATTERN,
};
use crate::error::Result;
use crate::project::{self, ProjectType};
use regex::Regex;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::path::Path;

#[derive(Debug, Clone, Copy, PartialEq, Serialize, Deserialize, Default)]
pub enum Theme {
    Nasa,
    #[default]
    Cyberpunk,
    Corporate,
}

#[allow(clippy::struct_excessive_bools)]
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Preferences {
    #[serde(default)]
    pub theme: Theme,
    #[serde(default = "default_auto_copy")]
    pub auto_copy: bool,
    #[serde(default)]
    pub auto_format: bool,
    #[serde(default)]
    pub auto_commit: bool,
    #[serde(default = "default_commit_prefix")]
    pub commit_prefix: String,
    #[serde(default)]
    pub allow_dirty_git: bool,
    #[serde(default)]
    pub system_bell: bool,
    #[serde(default = "default_backup_retention")]
    pub backup_retention: usize,
    #[serde(default = "default_progress_bars")]
    pub progress_bars: bool,
}

impl Default for Preferences {
    fn default() -> Self {
        Self {
            theme: Theme::default(),
            auto_copy: true,
            auto_format: false,
            auto_commit: false,
            commit_prefix: default_commit_prefix(),
            allow_dirty_git: false,
            system_bell: false,
            backup_retention: default_backup_retention(),
            progress_bars: true,
        }
    }
}

fn default_auto_copy() -> bool { true }
fn default_progress_bars() -> bool { true }
fn default_backup_retention() -> usize { 5 }
fn default_commit_prefix() -> String { "AI: ".to_string() }

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RuleConfig {
    #[serde(default = "default_max_tokens")]
    pub max_file_tokens: usize,
    #[serde(default = "default_max_complexity")]
    pub max_cyclomatic_complexity: usize,
    #[serde(default = "default_max_depth")]
    pub max_nesting_depth: usize,
    #[serde(default = "default_max_args")]
    pub max_function_args: usize,
    #[serde(default = "default_max_words")]
    pub max_function_words: usize,
    #[serde(default)]
    pub ignore_naming_on: Vec<String>,
    #[serde(default = "default_ignore_tokens")]
    pub ignore_tokens_on: Vec<String>,
}

impl Default for RuleConfig {
    fn default() -> Self {
        Self {
            max_file_tokens: default_max_tokens(),
            max_cyclomatic_complexity: default_max_complexity(),
            max_nesting_depth: default_max_depth(),
            max_function_args: default_max_args(),
            max_function_words: default_max_words(),
            ignore_naming_on: Vec::new(),
            ignore_tokens_on: default_ignore_tokens(),
        }
    }
}

const fn default_max_tokens() -> usize { 2000 }
const fn default_max_complexity() -> usize { 8 }
const fn default_max_depth() -> usize { 3 }
const fn default_max_args() -> usize { 5 }
const fn default_max_words() -> usize { 5 }
fn default_ignore_tokens() -> Vec<String> {
    vec!["README.md".to_string(), "lock".to_string()]
}

/// Helper enum to deserialize commands as either a single string or a list of strings.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum CommandEntry {
    Single(String),
    List(Vec<String>),
}

impl CommandEntry {
    fn into_vec(self) -> Vec<String> {
        match self {
            Self::Single(s) => vec![s],
            Self::List(v) => v,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct WardenToml {
    #[serde(default)]
    pub rules: RuleConfig,
    #[serde(default)]
    pub preferences: Preferences,
    #[serde(default)]
    pub commands: HashMap<String, CommandEntry>,
}

#[derive(Debug, Clone)]
pub enum GitMode {
    Auto,
    Yes,
    No,
}

#[derive(Debug, Clone)]
pub struct Config {
    pub git_mode: GitMode,
    pub include_patterns: Vec<Regex>,
    pub exclude_patterns: Vec<Regex>,
    pub code_only: bool,
    pub verbose: bool,
    pub rules: RuleConfig,
    pub preferences: Preferences,
    pub commands: HashMap<String, Vec<String>>,
}

impl Default for Config {
    fn default() -> Self {
        Self::new()
    }
}

impl Config {
    #[must_use]
    pub fn new() -> Self {
        Self {
            git_mode: GitMode::Auto,
            include_patterns: Vec::new(),
            exclude_patterns: Vec::new(),
            code_only: false,
            verbose: false,
            rules: RuleConfig::default(),
            preferences: Preferences::default(),
            commands: HashMap::new(),
        }
    }

    /// Validates configuration.
    /// # Errors
    /// Returns Ok.
    pub fn validate(&self) -> Result<()> {
        Ok(())
    }

    pub fn load_local_config(&mut self) {
        self.load_ignore_file();
        self.load_toml_config();
        self.apply_project_defaults();
    }

    fn apply_project_defaults(&mut self) {
        if self.commands.contains_key("check") {
            return;
        }
        let defaults = project_defaults(ProjectType::detect());
        for (k, v) in defaults {
            self.commands.entry(k).or_insert(v);
        }
    }

    fn load_ignore_file(&mut self) {
        let Ok(content) = fs::read_to_string(".wardenignore") else {
            return;
        };
        for line in content.lines() {
            self.process_ignore_line(line);
        }
    }

    pub fn process_ignore_line(&mut self, line: &str) {
        let trimmed = line.trim();
        if trimmed.is_empty() || trimmed.starts_with('#') {
            return;
        }
        if let Ok(re) = Regex::new(trimmed) {
            self.exclude_patterns.push(re);
        }
    }

    fn load_toml_config(&mut self) {
        if !Path::new("warden.toml").exists() {
            return;
        }
        let Ok(content) = fs::read_to_string("warden.toml") else {
            return;
        };
        self.parse_toml(&content);
    }

    pub fn parse_toml(&mut self, content: &str) {
        let Ok(parsed) = toml::from_str::<WardenToml>(content) else {
            return;
        };
        self.rules = parsed.rules;
        self.preferences = parsed.preferences;
        self.commands = parsed
            .commands
            .into_iter()
            .map(|(k, v)| (k, v.into_vec()))
            .collect();
    }
}

/// Saves the current configuration to `warden.toml`.
/// # Errors
/// Returns error if file write fails or serialization fails.
#[allow(clippy::implicit_hasher)]
pub fn save_to_file(
    rules: &RuleConfig,
    prefs: &Preferences,
    commands: &HashMap<String, Vec<String>>,
) -> Result<()> {
    let cmd_entries: HashMap<String, CommandEntry> = commands
        .iter()
        .map(|(k, v)| (k.clone(), CommandEntry::List(v.clone())))
        .collect();

    let toml_struct = WardenToml {
        rules: rules.clone(),
        preferences: prefs.clone(),
        commands: cmd_entries,
    };

    let content = toml::to_string_pretty(&toml_struct).map_err(|e| {
        crate::error::WardenError::Other(format!("Failed to serialize config: {e}"))
    })?;

    fs::write("warden.toml", content)?;
    Ok(())
}

fn project_defaults(project: ProjectType) -> HashMap<String, Vec<String>> {
    let mut m = HashMap::new();
    match project {
        ProjectType::Rust => {
            m.insert(
                "check".into(),
                vec![
                    "cargo clippy --all-targets -- -D warnings -D clippy::pedantic".into(),
                    "cargo test".into(),
                ],
            );
            m.insert("fix".into(), vec!["cargo fmt".into()]);
        }
        ProjectType::Node => {
            let npx = project::npx_cmd();
            m.insert(
                "check".into(),
                vec![format!("{npx} @biomejs/biome check src/")],
            );
            m.insert(
                "fix".into(),
                vec![format!("{npx} @biomejs/biome check --write src/")],
            );
        }
        ProjectType::Python => {
            m.insert("check".into(), vec!["ruff check .".into()]);
            m.insert("fix".into(), vec!["ruff check --fix .".into()]);
        }
        ProjectType::Go => {
            m.insert("check".into(), vec!["go vet ./...".into()]);
            m.insert("fix".into(), vec!["go fmt ./...".into()]);
        }
        ProjectType::Unknown => {}
    }
    m
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/constants.rs âˆ‡âˆ‡âˆ‡
// src/constants.rs
//! Shared constants for file filtering and pattern matching.

pub const PRUNE_DIRS: &[&str] = &[
    ".git",
    ".svn",
    ".hg",
    "node_modules",
    "target",
    "dist",
    "build",
    "out",
    "gen",
    ".venv",
    "venv",
    ".tox",
    "__pycache__",
    "coverage",
    "vendor",
    ".warden_apply_backup",
];

pub const PRUNE_FILES: &[&str] = &[
    "Cargo.lock",
    "package-lock.json",
    "pnpm-lock.yaml",
    "yarn.lock",
    "bun.lockb",
    "go.sum",
    "Gemfile.lock",
];

pub const SKIP_DIRS: &[&str] = &["tests", "test", "spec", "docs", "examples", "fixtures"];

pub const BIN_EXT_PATTERN: &str =
    r"(?i)\.(png|jpg|gif|svg|ico|webp|woff2?|ttf|pdf|mp4|zip|gz|tar|exe|dll|so|dylib|class|pyc)$";

pub const SECRET_PATTERN: &str =
    r"(?i)(^\.?env(\..*)?$|/\.?env(\..*)?$|(^|/)(id_rsa|id_ed25519|.*\.(pem|p12|key|pfx))$)";

pub const CODE_EXT_PATTERN: &str = r"(?i)\.(rs|go|py|js|jsx|ts|tsx|java|c|cpp|h|hpp|cs|php|rb|sh|sql|html|css|scss|json|toml|yaml|md)$";

pub const CODE_BARE_PATTERN: &str = r"(?i)(Makefile|Dockerfile|CMakeLists\.txt)$";

/// Checks if a directory name should be pruned during traversal.
#[must_use]
pub fn should_prune(name: &str) -> bool {
    PRUNE_DIRS.contains(&name) || PRUNE_FILES.contains(&name) || SKIP_DIRS.contains(&name)
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/detection.rs âˆ‡âˆ‡âˆ‡
// src/detection.rs
use crate::error::Result;
use std::collections::HashSet;
use std::fmt;
use std::path::Path;

#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy)]
pub enum BuildSystemType {
    Rust,
    Node,
    Python,
    Go,
    CMake,
    Conan,
}

impl fmt::Display for BuildSystemType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{self:?}")
    }
}

#[derive(Default)]
pub struct Detector;

impl Detector {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    /// Detects build systems.
    /// # Errors
    /// Returns `Ok`.
    pub fn detect_build_systems(
        &self,
        files: &[std::path::PathBuf],
    ) -> Result<Vec<BuildSystemType>> {
        let mut detected = HashSet::new();
        for file in files {
            check_file(file, &mut detected);
        }
        Ok(detected.into_iter().collect())
    }
}

fn check_file(path: &Path, set: &mut HashSet<BuildSystemType>) {
    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
        if check_cmake(path, set) {
            return;
        }
        check_common(name, set);
    }
}

fn check_cmake(path: &Path, set: &mut HashSet<BuildSystemType>) -> bool {
    if path
        .extension()
        .is_some_and(|e| e.eq_ignore_ascii_case("cmake"))
    {
        set.insert(BuildSystemType::CMake);
        return true;
    }
    false
}

const COMMON_CONFIGS: &[(&str, BuildSystemType)] = &[
    ("Cargo.toml", BuildSystemType::Rust),
    ("package.json", BuildSystemType::Node),
    ("requirements.txt", BuildSystemType::Python),
    ("pyproject.toml", BuildSystemType::Python),
    ("Pipfile", BuildSystemType::Python),
    ("go.mod", BuildSystemType::Go),
    ("CMakeLists.txt", BuildSystemType::CMake),
    ("conanfile.txt", BuildSystemType::Conan),
    ("conanfile.py", BuildSystemType::Conan),
];

fn check_common(name: &str, set: &mut HashSet<BuildSystemType>) {
    for (file, sys) in COMMON_CONFIGS {
        if name == *file {
            set.insert(*sys);
            return;
        }
    }
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/discovery.rs âˆ‡âˆ‡âˆ‡
// src/discovery.rs
use crate::config::{
    Config, GitMode, BIN_EXT_PATTERN, CODE_BARE_PATTERN, CODE_EXT_PATTERN, SECRET_PATTERN,
};
use crate::constants::should_prune;
use crate::error::{Result, WardenError};
use regex::Regex;
use std::collections::HashMap;
use std::fs;
use std::path::{Path, PathBuf};
use std::process::Command;
use std::sync::LazyLock;
use walkdir::WalkDir;

/// Runs the full file discovery pipeline: Enumerate -> Heuristics -> Filter.
///
/// # Errors
/// Returns error if git commands fail or regexes are invalid.
pub fn discover(config: &Config) -> Result<Vec<PathBuf>> {
    let raw_files = enumerate_files(config)?;
    let heuristic_files = filter_heuristics(raw_files);
    let final_files = filter_config(heuristic_files, config)?;
    Ok(final_files)
}

// --- Enumeration ---

fn enumerate_files(config: &Config) -> Result<Vec<PathBuf>> {
    match config.git_mode {
        GitMode::Yes => enumerate_git_required(),
        GitMode::No => Ok(walk_filesystem(config.verbose)),
        GitMode::Auto => Ok(enumerate_auto(config.verbose)),
    }
}

fn enumerate_git_required() -> Result<Vec<PathBuf>> {
    if !in_git_repo() {
        return Err(WardenError::NotInGitRepo);
    }
    git_ls_files().map(filter_pruned)
}

fn enumerate_auto(verbose: bool) -> Vec<PathBuf> {
    if in_git_repo() {
        git_ls_files().map_or_else(|_| walk_filesystem(verbose), filter_pruned)
    } else {
        walk_filesystem(verbose)
    }
}

fn walk_filesystem(verbose: bool) -> Vec<PathBuf> {
    let walker = WalkDir::new(".")
        .follow_links(false)
        .into_iter()
        .filter_entry(|e| !should_prune(&e.file_name().to_string_lossy()));

    let (paths, error_count) = accumulate_walker(walker);
    if error_count > 0 && verbose {
        eprintln!("WARN: Encountered {error_count} errors during file walk");
    }
    paths
}

fn accumulate_walker<I>(walker: I) -> (Vec<PathBuf>, usize)
where
    I: Iterator<Item = walkdir::Result<walkdir::DirEntry>>,
{
    let mut paths = Vec::new();
    let mut errors = 0;
    for item in walker {
        match item {
            Ok(entry) => {
                if entry.file_type().is_file() {
                    let p = entry.path().strip_prefix(".").unwrap_or(entry.path());
                    paths.push(p.to_path_buf());
                }
            }
            Err(_) => errors += 1,
        }
    }
    (paths, errors)
}

fn in_git_repo() -> bool {
    Command::new("git")
        .args(["rev-parse", "--is-inside-work-tree"])
        .output()
        .map(|o| o.status.success())
        .unwrap_or(false)
}

fn git_ls_files() -> Result<Vec<PathBuf>> {
    let out = Command::new("git")
        .args(["ls-files", "-z", "-c", "-o", "--exclude-standard", "."])
        .output()?;

    if !out.status.success() {
        return Err(WardenError::Other(format!(
            "git ls-files failed: {}",
            out.status
        )));
    }

    let paths = out
        .stdout
        .split(|&b| b == 0)
        .filter(|chunk| !chunk.is_empty())
        .map(|chunk| PathBuf::from(String::from_utf8_lossy(chunk).as_ref()))
        .collect();

    Ok(paths)
}

fn filter_pruned(paths: Vec<PathBuf>) -> Vec<PathBuf> {
    paths
        .into_iter()
        .filter(|p| !contains_pruned_component(p))
        .collect()
}

fn contains_pruned_component(path: &Path) -> bool {
    path.components()
        .filter_map(|c| c.as_os_str().to_str())
        .any(should_prune)
}

// --- Heuristics ---

const MIN_TEXT_ENTROPY: f64 = 3.5;
const MAX_TEXT_ENTROPY: f64 = 5.5;
const BUILD_MARKERS: &[&str] = &[
    "find_package",
    "add_executable",
    "target_link_libraries",
    "cmake_minimum_required",
    "project(",
    "add-apt-repository",
    "conanfile.py",
    "dependency",
    "require",
    "include",
    "import",
];

static CODE_EXT_RE: LazyLock<Option<Regex>> = LazyLock::new(|| Regex::new(CODE_EXT_PATTERN).ok());
static CODE_BARE_RE: LazyLock<Option<Regex>> = LazyLock::new(|| Regex::new(CODE_BARE_PATTERN).ok());

fn filter_heuristics(files: Vec<PathBuf>) -> Vec<PathBuf> {
    files.into_iter().filter(|p| keep_heuristic(p)).collect()
}

fn keep_heuristic(path: &Path) -> bool {
    let s = path.to_string_lossy();
    if is_known_code(&s) {
        return true;
    }

    let Ok(entropy) = calculate_entropy(path) else {
        return false;
    };
    if (MIN_TEXT_ENTROPY..=MAX_TEXT_ENTROPY).contains(&entropy) {
        return true;
    }
    has_build_markers(path)
}

fn is_known_code(path_str: &str) -> bool {
    let ext = CODE_EXT_RE.as_ref().is_some_and(|r| r.is_match(path_str));
    let bare = CODE_BARE_RE.as_ref().is_some_and(|r| r.is_match(path_str));
    ext || bare
}

fn has_build_markers(path: &Path) -> bool {
    let Ok(content) = fs::read_to_string(path) else {
        return false;
    };
    let lower = content.to_lowercase();
    BUILD_MARKERS.iter().any(|m| lower.contains(m))
}

#[allow(clippy::cast_precision_loss)]
fn calculate_entropy(path: &Path) -> std::io::Result<f64> {
    let bytes = fs::read(path)?;
    if bytes.is_empty() {
        return Ok(0.0);
    }
    let mut freq = HashMap::new();
    for &b in &bytes {
        *freq.entry(b).or_insert(0) += 1;
    }
    let len = bytes.len() as f64;
    Ok(freq
        .values()
        .fold(0.0, |acc, &n| acc - (f64::from(n) / len) * (f64::from(n) / len).log2()))
}

// --- Config Filter ---

struct FilterContext<'a> {
    config: &'a Config,
    bin_re: Regex,
    secret_re: Regex,
    code_re: Option<Regex>,
    bare_re: Option<Regex>,
}

fn filter_config(files: Vec<PathBuf>, config: &Config) -> Result<Vec<PathBuf>> {
    let ctx = FilterContext {
        config,
        bin_re: Regex::new(BIN_EXT_PATTERN)?,
        secret_re: Regex::new(SECRET_PATTERN)?,
        code_re: if config.code_only {
            Some(Regex::new(CODE_EXT_PATTERN)?)
        } else {
            None
        },
        bare_re: if config.code_only {
            Some(Regex::new(CODE_BARE_PATTERN)?)
        } else {
            None
        },
    };

    Ok(files
        .into_iter()
        .filter(|p| should_keep_config(p, &ctx))
        .collect())
}

fn should_keep_config(path: &Path, ctx: &FilterContext) -> bool {
    let s = path.to_string_lossy().replace('\\', "/");

    if ctx.secret_re.is_match(&s)
        || ctx.bin_re.is_match(&s)
        || ctx.config.exclude_patterns.iter().any(|p| p.is_match(&s))
    {
        return false;
    }

    if ctx.config.code_only {
        let is_code = ctx.code_re.as_ref().is_some_and(|r| r.is_match(&s))
            || ctx.bare_re.as_ref().is_some_and(|r| r.is_match(&s));
        if !is_code {
            return false;
        }
    }

    ctx.config.include_patterns.is_empty()
        || ctx.config.include_patterns.iter().any(|p| p.is_match(&s))
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/error.rs âˆ‡âˆ‡âˆ‡
// src/error.rs
use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum WardenError {
    #[error("I/O error: {source} (path: {path})")]
    Io {
        source: std::io::Error,
        path: PathBuf,
    },

    #[error("Not inside a Git repository")]
    NotInGitRepo,

    #[error("Regex error: {0}")]
    Regex(#[from] regex::Error),

    #[error("Generic error: {0}")]
    Other(String),
}

pub type Result<T> = std::result::Result<T, WardenError>;

// Allow `?` on std::io::Error by converting to WardenError::Io with unknown path.
impl From<std::io::Error> for WardenError {
    fn from(source: std::io::Error) -> Self {
        WardenError::Io {
            source,
            path: PathBuf::from("<unknown>"),
        }
    }
}

// Gracefully convert WalkDir errors
impl From<walkdir::Error> for WardenError {
    fn from(e: walkdir::Error) -> Self {
        WardenError::Other(e.to_string())
    }
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/graph/imports.rs âˆ‡âˆ‡âˆ‡
// src/graph/imports.rs
use std::path::Path;
use std::sync::LazyLock;
use tree_sitter::{Language, Parser, Query, QueryCursor};

static EXTRACTOR: LazyLock<ImportExtractor> = LazyLock::new(ImportExtractor::new);

struct ImportExtractor {
    rust: Query,
    python: Query,
    javascript: Query,
}

impl ImportExtractor {
    fn new() -> Self {
        Self {
            rust: compile_query(
                tree_sitter_rust::language(),
                r"
                (use_declaration argument: (_) @import)
                (mod_item name: (identifier) @mod)
                ",
            ),
            python: compile_query(
                tree_sitter_python::language(),
                r"
                (import_statement name: (dotted_name) @import)
                (aliased_import name: (dotted_name) @import)
                (import_from_statement module_name: (dotted_name) @import)
                ",
            ),
            javascript: compile_query(
                tree_sitter_typescript::language_typescript(),
                r#"
                (import_statement source: (string) @import)
                (export_statement source: (string) @import)
                (call_expression
                  function: (identifier) @func
                  arguments: (arguments (string) @import)
                  (#eq? @func "require"))
                "#,
            ),
        }
    }

    fn get_config<'a>(&'a self, lang: &str) -> Option<(Language, &'a Query)> {
        match lang {
            "rs" => Some((tree_sitter_rust::language(), &self.rust)),
            "py" => Some((tree_sitter_python::language(), &self.python)),
            "js" | "jsx" | "ts" | "tsx" => Some((
                tree_sitter_typescript::language_typescript(),
                &self.javascript,
            )),
            _ => None,
        }
    }
}

/// Extracts raw import strings from the given file content.
///
/// # Arguments
/// * `path` - File path (used for language detection).
/// * `content` - Source code.
///
/// # Returns
/// A list of imported module names/paths (e.g., "`std::io`", "./utils", "react").
#[must_use]
pub fn extract(path: &Path, content: &str) -> Vec<String> {
    let Some(ext) = path.extension().and_then(|s| s.to_str()) else {
        return Vec::new();
    };

    let Some((lang, query)) = EXTRACTOR.get_config(ext) else {
        return Vec::new();
    };

    run_query(content, lang, query)
}

fn run_query(source: &str, lang: Language, query: &Query) -> Vec<String> {
    let mut parser = Parser::new();
    if parser.set_language(lang).is_err() {
        return Vec::new();
    }

    let Some(tree) = parser.parse(source, None) else {
        return Vec::new();
    };

    let mut cursor = QueryCursor::new();
    let matches = cursor.matches(query, tree.root_node(), source.as_bytes());
    let mut imports = Vec::new();

    for m in matches {
        for capture in m.captures {
            if let Ok(text) = capture.node.utf8_text(source.as_bytes()) {
                imports.push(clean_text(text));
            }
        }
    }

    imports
}

fn clean_text(text: &str) -> String {
    // Remove quotes for JS/TS strings
    text.trim_matches(|c| c == '"' || c == '\'' || c == '`')
        .to_string()
}

fn compile_query(lang: Language, pattern: &str) -> Query {
    match Query::new(lang, pattern) {
        Ok(q) => q,
        Err(e) => panic!("Invalid import query: {e}"),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_rust_imports() {
        let code = r"
            use std::io;
            use crate::config::Config;
            mod tests;
        ";
        let imports = extract(Path::new("main.rs"), code);
        assert!(imports.contains(&"std::io".to_string()));
        assert!(imports.contains(&"crate::config::Config".to_string()));
        assert!(imports.contains(&"tests".to_string()));
    }

    #[test]
    fn test_python_imports() {
        let code = r"
            import os
            from sys import path
            import numpy as np
        ";
        let imports = extract(Path::new("script.py"), code);
        assert!(imports.contains(&"os".to_string()));
        assert!(imports.contains(&"sys".to_string()));
        assert!(imports.contains(&"numpy".to_string()));
    }

    #[test]
    fn test_ts_imports() {
        let code = r#"
            import { Foo } from "./components";
            const fs = require('fs');
            export * from "./utils";
        "#;
        let imports = extract(Path::new("app.ts"), code);
        assert!(imports.contains(&"./components".to_string()));
        assert!(imports.contains(&"fs".to_string()));
        assert!(imports.contains(&"./utils".to_string()));
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/graph/mod.rs âˆ‡âˆ‡âˆ‡
// src/graph/mod.rs
pub mod imports;
pub mod resolver;
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/graph/resolver.rs âˆ‡âˆ‡âˆ‡
// src/graph/resolver.rs
use std::path::{Path, PathBuf};

/// Resolves an import string to a likely file path on disk.
///
/// # Arguments
/// * `project_root` - The root of the repository.
/// * `current_file` - The path of the file containing the import.
/// * `import_str` - The raw import string (e.g., "`crate::foo`", "./utils").
///
/// # Returns
/// `Option<PathBuf>` if a matching local file is found.
#[must_use]
pub fn resolve(project_root: &Path, current_file: &Path, import_str: &str) -> Option<PathBuf> {
    let ext = current_file.extension().and_then(|s| s.to_str())?;
    
    match ext {
        "rs" => resolve_rust(project_root, current_file, import_str),
        "ts" | "tsx" | "js" | "jsx" => resolve_js(project_root, current_file, import_str),
        "py" => resolve_python(project_root, current_file, import_str),
        _ => None,
    }
}

fn resolve_rust(root: &Path, current: &Path, import: &str) -> Option<PathBuf> {
    // 1. Handle "crate::" (Absolute from src/)
    if let Some(rest) = import.strip_prefix("crate::") {
        let parts: Vec<&str> = rest.split("::").collect();
        let base = root.join("src");
        return check_variations(&base, &parts, "rs");
    }

    // 2. Handle "super::" (Parent directory)
    if import.starts_with("super::") {
        return None; // TODO: complex super chain resolution
    }

    // 3. Handle relative `mod foo;` or `use foo;`
    if !import.contains("::") && !import.starts_with("crate") {
        let parent = current.parent()?;
        let parts = vec![import];
        return check_variations(parent, &parts, "rs");
    }

    None
}

fn resolve_js(_root: &Path, current: &Path, import: &str) -> Option<PathBuf> {
    if !import.starts_with('.') {
        return None;
    }

    let parent = current.parent()?;
    let path = parent.join(import);
    
    if let Some(p) = check_js_file(&path) {
        return Some(p);
    }
    check_js_directory(&path)
}

fn check_js_file(path: &Path) -> Option<PathBuf> {
    if path.exists() && path.is_file() {
        return Some(path.to_path_buf());
    }

    let extensions = ["ts", "tsx", "js", "jsx", "json"];
    for ext in extensions {
        let p = path.with_extension(ext);
        if p.exists() {
            return Some(p);
        }
    }
    None
}

fn check_js_directory(path: &Path) -> Option<PathBuf> {
    if !path.is_dir() {
        return None;
    }

    let extensions = ["ts", "tsx", "js", "jsx", "json"];
    for ext in extensions {
        let p = path.join(format!("index.{ext}"));
        if p.exists() {
            return Some(p);
        }
    }
    None
}

fn resolve_python(root: &Path, _current: &Path, import: &str) -> Option<PathBuf> {
    // 1. Handle Relative "from . import foo" -> "."
    if import.starts_with('.') {
        return None; // Simplified: assuming simple relative import for now
    }

    // 2. Absolute (from root)
    let parts: Vec<&str> = import.split('.').collect();
    check_variations(root, &parts, "py")
}

fn check_variations(base: &Path, parts: &[&str], ext: &str) -> Option<PathBuf> {
    let mut current = base.to_path_buf();
    for part in parts {
        current.push(part);
    }

    // Variation A: path.ext
    let file_path = current.with_extension(ext);
    if file_path.exists() {
        return Some(file_path);
    }

    // Variation B: path/mod.rs or path/__init__.py
    let index_name = match ext {
        "rs" => "mod.rs",
        "py" => "__init__.py",
        _ => return None,
    };
    
    let index_path = current.join(index_name);
    if index_path.exists() {
        return Some(index_path);
    }

    None
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;
    use std::fs;
    use anyhow::Result;

    #[test]
    fn test_resolve_rust_mod_relative() -> Result<()> {
        let temp = tempdir()?;
        let root = temp.path();
        
        let src = root.join("src");
        fs::create_dir_all(&src)?;
        
        let main = src.join("main.rs");
        let util = src.join("util.rs");
        fs::write(&main, "mod util;")?;
        fs::write(&util, "// util")?;

        let resolved = resolve(root, &main, "util");
        assert_eq!(resolved, Some(util));
        Ok(())
    }

    #[test]
    fn test_resolve_js_relative_extension() -> Result<()> {
        let temp = tempdir()?;
        let root = temp.path();
        
        let app = root.join("app.ts");
        let cmp = root.join("cmp.tsx");
        fs::write(&app, "")?;
        fs::write(&cmp, "")?;

        let resolved = resolve(root, &app, "./cmp");
        assert_eq!(resolved, Some(cmp));
        Ok(())
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/lib.rs âˆ‡âˆ‡âˆ‡
// src/lib.rs
pub mod analysis;
pub mod apply;
pub mod clipboard;
pub mod config;
pub mod constants;
pub mod discovery;
pub mod error;
pub mod graph;
pub mod pack;
pub mod project;
pub mod prompt;
pub mod reporting;
pub mod roadmap;
pub mod skeleton;
pub mod tokens;
pub mod tui;
pub mod types;
pub mod wizard;

// Legacy/Test compatibility aliases
pub use analysis as rules;
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/pack.rs âˆ‡âˆ‡âˆ‡
// src/pack.rs
use crate::analysis::RuleEngine;
use crate::clipboard;
use crate::config::{Config, GitMode};
use crate::discovery;
use crate::prompt::PromptGenerator;
use crate::skeleton;
use crate::tokens::Tokenizer;
use anyhow::Result;
use clap::ValueEnum;
use colored::Colorize;
use std::fmt::Write;
use std::fs;
use std::path::{Path, PathBuf};

#[derive(Debug, Clone, ValueEnum, Default)]
pub enum OutputFormat {
    #[default]
    Text,
    Xml,
}

#[allow(clippy::struct_excessive_bools)]
#[derive(Default)]
pub struct PackOptions {
    pub stdout: bool,
    pub copy: bool,
    pub verbose: bool,
    pub prompt: bool,
    pub format: OutputFormat,
    pub skeleton: bool,
    pub git_only: bool,
    pub no_git: bool,
    pub code_only: bool,
    pub target: Option<PathBuf>,
}

/// Entry point for the pack command.
///
/// # Errors
/// Returns error if:
/// - Configuration loading fails
/// - File discovery fails
/// - Content generation fails
/// - Clipboard access fails (if --copy is used)
/// - File writing fails
pub fn run(options: &PackOptions) -> Result<()> {
    let config = setup_config(options)?;

    if !options.stdout && !options.copy {
        if let Some(t) = &options.target {
            println!("ğŸ§¶ Knitting repository (Focus: {})...", t.display());
        } else {
            println!("ğŸ§¶ Knitting repository...");
        }
    }

    let files = discovery::discover(&config)?;
    if options.verbose {
        eprintln!("ğŸ“¦ Packing {} files...", files.len());
    }

    let content = generate_content(&files, options, &config)?;
    let token_count = Tokenizer::count(&content);

    output_result(&content, token_count, options)
}

fn setup_config(opts: &PackOptions) -> Result<Config> {
    let mut config = Config::new();
    config.verbose = opts.verbose;
    config.code_only = opts.code_only;
    config.git_mode = if opts.git_only {
        GitMode::Yes
    } else if opts.no_git {
        GitMode::No
    } else {
        GitMode::Auto
    };
    config.load_local_config();
    config.validate()?;
    Ok(config)
}

/// Generates the context content string from a list of files.
/// Exposed for testing purposes.
///
/// # Errors
/// Returns error if file reading fails.
pub fn generate_content(files: &[PathBuf], opts: &PackOptions, config: &Config) -> Result<String> {
    let mut ctx = String::with_capacity(100_000);

    if opts.prompt {
        write_header(&mut ctx, config)?;
        inject_violations(&mut ctx, files, config)?;
    }

    write_body(files, &mut ctx, opts)?;

    if opts.prompt {
        write_footer(&mut ctx, config)?;
    }

    Ok(ctx)
}

fn inject_violations(ctx: &mut String, files: &[PathBuf], config: &Config) -> Result<()> {
    let engine = RuleEngine::new(config.clone());
    let report = engine.scan(files.to_vec());

    if !report.has_errors() {
        return Ok(());
    }

    writeln!(
        ctx,
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )?;
    writeln!(ctx, "âš ï¸  ACTIVE VIOLATIONS (PRIORITY FIX REQUIRED)")?;
    writeln!(
        ctx,
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
    )?;

    for file in report.files {
        if file.is_clean() {
            continue;
        }
        for v in file.violations {
            writeln!(ctx, "FILE: {}", file.path.display())?;
            writeln!(ctx, "LAW:  {}", v.law)?;
            writeln!(ctx, "LINE: {}", v.row + 1)?;
            writeln!(ctx, "ERR:  {}", v.message)?;
            writeln!(ctx, "{}", "â”€".repeat(40))?;
        }
    }
    writeln!(ctx)?;

    Ok(())
}

fn write_body(files: &[PathBuf], ctx: &mut String, opts: &PackOptions) -> Result<()> {
    match opts.format {
        OutputFormat::Text => pack_nabla(files, ctx, opts),
        OutputFormat::Xml => pack_xml(files, ctx, opts),
    }
}

fn write_header(ctx: &mut String, config: &Config) -> Result<()> {
    let gen = PromptGenerator::new(config.rules.clone());
    writeln!(ctx, "{}", gen.wrap_header()?)?;
    writeln!(ctx, "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nBEGIN CODEBASE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")?;
    Ok(())
}

fn write_footer(ctx: &mut String, config: &Config) -> Result<()> {
    let gen = PromptGenerator::new(config.rules.clone());
    writeln!(ctx, "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nEND CODEBASE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")?;
    writeln!(ctx, "{}", gen.generate_reminder()?)?;
    Ok(())
}

fn output_result(content: &str, tokens: usize, opts: &PackOptions) -> Result<()> {
    let info = format!(
        "\nğŸ“Š Context Size: {} tokens",
        tokens.to_string().yellow().bold()
    );

    if opts.stdout {
        print!("{content}");
        eprintln!("{info}");
        return Ok(());
    }

    if opts.copy {
        let msg = clipboard::smart_copy(content)?;
        println!("{}", "âœ“ Copied to clipboard".green());
        println!("  ({msg})");
        println!("{info}");
        return Ok(());
    }

    let output_path = PathBuf::from("context.txt");
    fs::write(&output_path, content)?;
    println!("âœ… Generated 'context.txt'");

    if let Ok(abs_path) = fs::canonicalize(&output_path) {
        if clipboard::copy_file_path(&abs_path).is_ok() {
            println!(
                "{}",
                "ğŸ“ File path copied to clipboard (paste as attachment)".cyan()
            );
        }
    }

    println!("{info}");
    Ok(())
}

fn should_skeletonize(path: &Path, opts: &PackOptions) -> bool {
    // If global skeleton flag is on, everything is skeletonized
    if opts.skeleton {
        return true;
    }

    // If a target is specified, everything EXCEPT the target is skeletonized
    if let Some(target) = &opts.target {
        // We do a loose match: if the path ends with the target string.
        // This allows "warden pack src/main.rs" to match "./src/main.rs"
        return !path.ends_with(target);
    }

    false
}

fn pack_nabla(files: &[PathBuf], out: &mut String, opts: &PackOptions) -> Result<()> {
    for path in files {
        let p_str = path.to_string_lossy().replace('\\', "/");
        writeln!(out, "âˆ‡âˆ‡âˆ‡ {p_str} âˆ‡âˆ‡âˆ‡")?;

        match fs::read_to_string(path) {
            Ok(content) => {
                if should_skeletonize(path, opts) {
                    out.push_str(&skeleton::clean(path, &content));
                } else {
                    out.push_str(&content);
                }
            }
            Err(e) => writeln!(out, "// <ERROR READING FILE: {e}>")?,
        }
        writeln!(out, "\nâˆ†âˆ†âˆ†\n")?;
    }
    Ok(())
}

fn pack_xml(files: &[PathBuf], out: &mut String, opts: &PackOptions) -> Result<()> {
    writeln!(out, "<documents>")?;
    for path in files {
        let p_str = path.to_string_lossy().replace('\\', "/");
        writeln!(out, "  <document path=\"{p_str}\"><![CDATA[")?;

        match fs::read_to_string(path) {
            Ok(content) => {
                if should_skeletonize(path, opts) {
                    out.push_str(
                        &skeleton::clean(path, &content).replace("]]>", "]]]]><![CDATA[>"),
                    );
                } else {
                    out.push_str(&content.replace("]]>", "]]]]><![CDATA[>"));
                }
            }
            Err(e) => writeln!(out, "<!-- ERROR: {e} -->")?,
        }
        writeln!(out, "]]></document>")?;
    }
    writeln!(out, "</documents>")?;
    Ok(())
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/project.rs âˆ‡âˆ‡âˆ‡
// src/project.rs
use std::path::Path;

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum ProjectType {
    Rust,
    Node,
    Python,
    Go,
    Unknown,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum Strictness {
    Strict,
    Standard,
    Relaxed,
}

impl ProjectType {
    /// Detects project type from current directory.
    #[must_use]
    pub fn detect() -> Self {
        Self::detect_in(Path::new("."))
    }

    /// Detects project type in a specific directory.
    #[must_use]
    pub fn detect_in(root: &Path) -> Self {
        if root.join("Cargo.toml").exists() {
            return Self::Rust;
        }
        if root.join("package.json").exists() {
            return Self::Node;
        }
        if root.join("pyproject.toml").exists()
            || root.join("requirements.txt").exists()
            || root.join("Pipfile").exists()
        {
            return Self::Python;
        }
        if root.join("go.mod").exists() {
            return Self::Go;
        }
        Self::Unknown
    }

    /// Detects if this is a TypeScript project
    #[must_use]
    pub fn is_typescript() -> bool {
        Path::new("tsconfig.json").exists()
            || Path::new("tsconfig.node.json").exists()
            || has_ts_files()
    }
}

fn has_ts_files() -> bool {
    Path::new("src")
        .read_dir()
        .map(|entries| {
            entries.flatten().any(|e| {
                e.path()
                    .extension()
                    .is_some_and(|ext| ext == "ts" || ext == "tsx")
            })
        })
        .unwrap_or(false)
}

#[must_use]
pub fn generate_toml(project: ProjectType, strictness: Strictness) -> String {
    let rules = rules_section(strictness);
    let commands = commands_section(project);

    format!("# warden.toml\n{rules}\n\n{commands}\n")
}

fn rules_section(strictness: Strictness) -> String {
    let (tokens, complexity, depth) = match strictness {
        Strictness::Strict => (1500, 4, 2),
        Strictness::Standard => (2000, 8, 3),
        Strictness::Relaxed => (3000, 12, 4),
    };

    format!(
        r#"[rules]
max_file_tokens = {tokens}
max_cyclomatic_complexity = {complexity}
max_nesting_depth = {depth}
max_function_args = 5
max_function_words = 5
ignore_naming_on = ["tests", "spec"]"#
    )
}

fn commands_section(project: ProjectType) -> String {
    match project {
        ProjectType::Rust => rust_commands(),
        ProjectType::Node => node_commands(),
        ProjectType::Python => python_commands(),
        ProjectType::Go => go_commands(),
        ProjectType::Unknown => unknown_commands(),
    }
}

fn rust_commands() -> String {
    r#"[commands]
check = [
    "cargo clippy --all-targets -- -D warnings -D clippy::pedantic",
    "cargo test"
]
fix = "cargo fmt""#
        .to_string()
}

fn node_commands() -> String {
    let npx = npx_cmd();
    
    // Use biome for TypeScript projects
    if ProjectType::is_typescript() {
        format!(
            r#"[commands]
check = "{npx} @biomejs/biome check src/"
fix = "{npx} @biomejs/biome check --write src/""#
        )
    } else {
        format!(
            r#"[commands]
check = "{npx} eslint src/"
fix = "{npx} eslint --fix src/""#
        )
    }
}

fn python_commands() -> String {
    r#"[commands]
check = "ruff check ."
fix = "ruff check --fix .""#
        .to_string()
}

fn go_commands() -> String {
    r#"[commands]
check = "go vet ./..."
fix = "go fmt ./...""#
        .to_string()
}

fn unknown_commands() -> String {
    r#"# No project type detected. Configure commands manually:
# [commands]
# check = "your-lint-command"
# fix = "your-fix-command""#
        .to_string()
}

#[must_use]
pub fn npx_cmd() -> &'static str {
    if cfg!(windows) {
        "npx.cmd"
    } else {
        "npx"
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/prompt.rs âˆ‡âˆ‡âˆ‡
// src/prompt.rs
use crate::config::RuleConfig;
use anyhow::Result;

pub struct PromptGenerator {
    config: RuleConfig,
}

impl PromptGenerator {
    #[must_use]
    pub fn new(config: RuleConfig) -> Self {
        Self { config }
    }

    /// Generates the full system prompt.
    /// # Errors
    /// Currently infallible, returns Result for API consistency.
    pub fn generate(&self) -> Result<String> {
        Ok(self.build_system_prompt())
    }

    /// Generates a short reminder prompt for context footers.
    /// # Errors
    /// Currently infallible, returns Result for API consistency.
    pub fn generate_reminder(&self) -> Result<String> {
        Ok(self.build_reminder())
    }

    /// Alias for `generate()` â€” used by knit for context headers.
    /// # Errors
    /// Currently infallible, returns Result for API consistency.
    pub fn wrap_header(&self) -> Result<String> {
        self.generate()
    }

    fn build_system_prompt(&self) -> String {
        let tokens = self.config.max_file_tokens;
        let complexity = self.config.max_cyclomatic_complexity;
        let depth = self.config.max_nesting_depth;
        let args = self.config.max_function_args;
        let output_format = build_output_format();

        format!(
            r"ğŸ›¡ï¸ SYSTEM MANDATE: THE WARDEN PROTOCOL
ROLE: High-Integrity Systems Architect (NASA/JPL Standard).
CONTEXT: You are coding inside a strict environment enforced by Warden.

THE 3 LAWS (Non-Negotiable):

1. LAW OF ATOMICITY
   - Files: MUST be < {tokens} tokens.
   - Action: Split immediately if larger.

2. LAW OF COMPLEXITY
   - Cyclomatic Complexity: MUST be â‰¤ {complexity} per function.
   - Nesting Depth: MUST be â‰¤ {depth} levels.
   - Function Arguments: MUST be â‰¤ {args} parameters.

3. LAW OF PARANOIA
   - Use Result<T, E> for I/O and fallible operations.
   - NO .unwrap() or .expect() calls.

{output_format}
"
        )
    }

    fn build_reminder(&self) -> String {
        let tokens = self.config.max_file_tokens;
        let complexity = self.config.max_cyclomatic_complexity;
        let depth = self.config.max_nesting_depth;
        let args = self.config.max_function_args;

        format!(
            r"WARDEN CONSTRAINTS:
â–¡ Files < {tokens} tokens
â–¡ Complexity â‰¤ {complexity}
â–¡ Nesting â‰¤ {depth}
â–¡ Args â‰¤ {args}
â–¡ No .unwrap() or .expect()
â–¡ Use Nabla Format (âˆ‡âˆ‡âˆ‡ ... âˆ†âˆ†âˆ†)"
        )
    }
}

fn build_output_format() -> String {
    let nabla = "âˆ‡";
    let delta = "âˆ†";
    let open = format!("{nabla}{nabla}{nabla}");
    let close = format!("{delta}{delta}{delta}");

    format!(
        r#"OUTPUT FORMAT (MANDATORY):

1. Explain the changes (Technical Plan) using NABLA PROTOCOL:
   - Must start with "GOAL:"
   - Must include "CHANGES:" list

{open} PLAN {open}
GOAL: Refactor authentication module.
CHANGES:
1. Extract user validation to new file.
2. Update config parser.
{close}

2. Declare the plan (Manifest) using NABLA PROTOCOL:

{open} MANIFEST {open}
path/to/file1.rs
path/to/file2.rs [NEW]
{close}

3. Provide EACH file using NABLA PROTOCOL:

{open} path/to/file1.rs {open}
[file content]
{close}

RULES:
- Do NOT use markdown code blocks (e.g. triple backticks) to wrap the file. The {open} delimiters ARE the fence.
- You MAY use markdown inside the file content.
- Every file in the manifest MUST have a matching {open} block.
- Paths must match exactly.
- Do NOT truncate files (No "// ...")."#
    )
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/reporting.rs âˆ‡âˆ‡âˆ‡
// src/reporting.rs
use crate::types::{FileReport, ScanReport, Violation};
use anyhow::Result;
use colored::Colorize;

/// Prints the scan report to stdout.
///
/// # Errors
/// Returns `Ok(())` normally.
pub fn print_report(report: &ScanReport) -> Result<()> {
    let failures = count_failures(report);

    report
        .files
        .iter()
        .filter(|f| !f.is_clean())
        .for_each(print_file_report);

    print_summary(report, failures);
    Ok(())
}

fn count_failures(report: &ScanReport) -> usize {
    report
        .files
        .iter()
        .filter(|f| !f.is_clean())
        .map(|f| f.violations.len())
        .sum()
}

fn print_file_report(file: &FileReport) {
    for v in &file.violations {
        print_violation(&file.path, v);
    }
}

fn print_violation(path: &std::path::Path, v: &Violation) {
    let filename = path.to_string_lossy();
    let line_num = v.row + 1;

    println!("{}: {}", "error".red().bold(), v.message.bold());
    println!("  {} {}:{}:1", "-->".blue(), filename, line_num);
    println!("   {}", "|".blue());
    println!(
        "   {} {}: Action required",
        "=".blue().bold(),
        v.law.white().bold()
    );
    println!();
}

fn print_summary(report: &ScanReport, failures: usize) {
    if failures > 0 {
        let msg = format!(
            "âŒ Warden found {failures} violations in {}ms.",
            report.duration_ms
        );
        println!("{}", msg.red().bold());
    } else {
        let msg = format!(
            "âœ… All Clear. Scanned {} tokens in {}ms.",
            report.total_tokens, report.duration_ms
        );
        println!("{}", msg.green().bold());
    }
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/audit.rs âˆ‡âˆ‡âˆ‡
// src/roadmap/audit.rs
use crate::roadmap::slugify;
use crate::roadmap::types::{Roadmap, Task, TaskStatus};
use colored::Colorize;
use regex::Regex;
use std::fs;
use std::path::Path;
use walkdir::{DirEntry, WalkDir};

#[derive(Debug, Clone, Copy)]
pub struct AuditOptions {
    pub strict: bool,
}

#[derive(Debug)]
pub struct AuditViolation {
    pub task_id: String,
    pub task_text: String,
    pub reason: ViolationReason,
}

#[derive(Debug)]
pub enum ViolationReason {
    MissingTestFile(String),
    MissingTestFunction { file: String, function: String },
    NoTraceability, // Heuristic failed
}

#[derive(Debug)]
pub struct AuditReport {
    pub violations: Vec<AuditViolation>,
    pub total_checked: usize,
}

impl AuditReport {
    fn new() -> Self {
        Self {
            violations: Vec::new(),
            total_checked: 0,
        }
    }
}

pub fn run(roadmap: &Roadmap, root: &Path, opts: AuditOptions) {
    println!("{}", "ğŸ•µï¸  Roadmap Traceability Audit".bold().cyan());
    println!("{}", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€".dimmed());

    let report = scan(roadmap, root, &opts);

    if report.total_checked == 0 {
        println!("{}", "No completed tasks to audit.".yellow());
        return;
    }

    for violation in &report.violations {
        print_violation(violation);
    }

    print_summary(report.violations.len());
}

#[must_use]
pub fn scan(roadmap: &Roadmap, root: &Path, _opts: &AuditOptions) -> AuditReport {
    let tasks = roadmap.all_tasks();
    let completed: Vec<&&Task> = tasks
        .iter()
        .filter(|t| t.status == TaskStatus::Complete)
        .collect();

    if completed.is_empty() {
        return AuditReport::new();
    }

    // Heuristic scan for un-anchored tasks
    let scanned_test_files = scan_test_files(root);
    let mut report = AuditReport::new();
    report.total_checked = completed.len();

    for task in completed {
        // Skip if marked as [no-test]
        if task.text.contains("[no-test]") {
            continue;
        }

        if let Some(reason) = check_task(task, root, &scanned_test_files) {
            report.violations.push(AuditViolation {
                task_id: task.id.clone(),
                task_text: task.text.clone(),
                reason,
            });
        }
    }

    report
}

fn check_task(task: &Task, root: &Path, scanned_files: &[String]) -> Option<ViolationReason> {
    // 1. Priority: Explicit Anchors
    if !task.tests.is_empty() {
        for test_ref in &task.tests {
            if let Some(reason) = verify_anchor(test_ref, root) {
                return Some(reason);
            }
        }
        return None;
    }

    // 2. Fallback: Slug Heuristic
    let slug = slugify(&task.text).replace('-', "_");
    let id_slug = task.id.replace('-', "_");

    let found = scanned_files
        .iter()
        .any(|f| f.contains(&slug) || f.contains(&id_slug));

    if found {
        None
    } else {
        Some(ViolationReason::NoTraceability)
    }
}

fn verify_anchor(anchor: &str, root: &Path) -> Option<ViolationReason> {
    // Support "path/to/file.rs::function_name" syntax
    let (file_part, fn_part) = if let Some((f, n)) = anchor.split_once("::") {
        (f, Some(n))
    } else {
        (anchor, None)
    };

    let path = root.join(file_part.trim());
    
    if !path.exists() || !path.is_file() {
        return Some(ViolationReason::MissingTestFile(file_part.trim().to_string()));
    }

    // If function name is specified, verify it exists in the file content
    if let Some(func_name) = fn_part {
        let name = func_name.trim();
        if let Ok(content) = fs::read_to_string(&path) {
            if !check_definition(&path, &content, name) {
                return Some(ViolationReason::MissingTestFunction {
                    file: file_part.trim().to_string(),
                    function: name.to_string(),
                });
            }
        }
    }

    None
}

fn check_definition(path: &Path, content: &str, name: &str) -> bool {
    let ext = path.extension().and_then(|s| s.to_str()).unwrap_or("");
    let pattern = build_definition_pattern(ext, name);

    let Ok(re) = Regex::new(&pattern) else {
        return content.contains(name);
    };

    // Iterate matches and check if line is commented
    for m in re.find_iter(content) {
        if !is_match_commented(content, m.start(), ext) {
            return true;
        }
    }
    
    false
}

fn build_definition_pattern(ext: &str, name: &str) -> String {
    match ext {
        "rs" => format!(r"fn\s+{name}\b"),
        "py" => format!(r"def\s+{name}\b"),
        "go" => format!(r"func\s+{name}\b"),
        "js" | "ts" | "jsx" | "tsx" => {
            // JS/TS is flexible: function foo, const foo =, foo: function
            format!(r"(function\s+{name}\b|const\s+{name}\s*=|let\s+{name}\s*=|var\s+{name}\s*=|{name}\s*[:\(])")
        }
        _ => name.to_string(), // Fallback (used as regex pattern if simple)
    }
}

fn is_match_commented(content: &str, start_idx: usize, ext: &str) -> bool {
    let line_start = content[..start_idx].rfind('\n').map_or(0, |i| i + 1);
    let prefix = content[line_start..start_idx].trim();
    
    match ext {
        "py" => prefix.starts_with('#'),
        _ => prefix.starts_with("//") || prefix.starts_with('*'),
    }
}

fn print_violation(v: &AuditViolation) {
    let msg = match &v.reason {
        ViolationReason::MissingTestFile(f) => format!("Missing File: {f}"),
        ViolationReason::MissingTestFunction { file, function } => {
            format!("Missing Function: '{function}' in {file}")
        }
        ViolationReason::NoTraceability => "No test file found (heuristic)".to_string(),
    };

    println!(
        "{} {} (id: {})",
        "âš ï¸  Traceability Fail:".red(),
        v.task_text.bold(),
        v.task_id.dimmed()
    );
    println!("   â””â”€ {msg}");
}

fn print_summary(missing: usize) {
    println!();
    if missing == 0 {
        println!("{}", "âœ… All completed tasks have verified tests!".green().bold());
    } else {
        println!(
            "{}",
            format!("âŒ Found {missing} tasks without verified tests.").red().bold()
        );
        println!("   (Tip: Add <!-- test: tests/my_test.rs::function_name --> to the task in ROADMAP.md)");
    }
}

fn scan_test_files(root: &Path) -> Vec<String> {
    WalkDir::new(root)
        .follow_links(false)
        .into_iter()
        .filter_entry(|e| !is_ignored_dir(e))
        .flatten()
        .filter(is_heuristic_match)
        .filter_map(|e| e.path().to_str().map(str::to_lowercase))
        .collect()
}

fn is_ignored_dir(entry: &DirEntry) -> bool {
    let name = entry.file_name().to_str().unwrap_or("");
    name.starts_with('.') || name == "target" || name == "node_modules" || name == "vendor"
}

/// Strict filter for the heuristic scanner.
/// Only picks up files that explicitly look like tests.
fn is_heuristic_match(entry: &DirEntry) -> bool {
    if !entry.file_type().is_file() {
        return false;
    }
    
    if !has_code_extension(entry.path()) {
        return false;
    }

    let Some(name) = entry.file_name().to_str() else {
        return false;
    };

    name.contains("test")
        || name.contains("spec")
        || entry.path().components().any(|c| c.as_os_str() == "tests")
}

fn has_code_extension(path: &Path) -> bool {
    path.extension()
        .and_then(|s| s.to_str())
        .is_some_and(|ext| {
            matches!(
                ext.to_ascii_lowercase().as_str(),
                "rs" | "ts" | "js" | "py" | "go"
            )
        })
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/audit/verifier.rs âˆ‡âˆ‡âˆ‡
// src/roadmap/audit/verifier.rs
//! Checks that test anchors resolve to actual test functions.

use super::function_finder;
use super::types::{AuditResult, TaskAnchor, VerificationStatus};
use std::path::Path;

/// Verifies a single task anchor.
#[must_use]
pub fn verify_anchor(anchor: &TaskAnchor, project_root: &Path) -> AuditResult {
    let file_path = project_root.join(&anchor.file);

    if !file_path.exists() {
        return AuditResult {
            task_path: anchor.task_path.clone(),
            anchor: anchor.clone(),
            status: VerificationStatus::MissingFile,
            detail: Some(format!("File not found: {}", anchor.file)),
        };
    }

    match verify_function(&file_path, &anchor.function) {
        Ok(true) => AuditResult {
            task_path: anchor.task_path.clone(),
            anchor: anchor.clone(),
            status: VerificationStatus::Verified,
            detail: None,
        },
        Ok(false) => AuditResult {
            task_path: anchor.task_path.clone(),
            anchor: anchor.clone(),
            status: VerificationStatus::MissingFunction,
            detail: Some(format!(
                "Function '{}' not found in {}",
                anchor.function, anchor.file
            )),
        },
        Err(e) => AuditResult {
            task_path: anchor.task_path.clone(),
            anchor: anchor.clone(),
            status: VerificationStatus::ParseError,
            detail: Some(e),
        },
    }
}

fn verify_function(file_path: &Path, function_name: &str) -> Result<bool, String> {
    function_finder::function_exists(file_path, function_name)
}

/// Verifies multiple anchors and returns all results.
#[must_use]
pub fn verify_all(anchors: &[TaskAnchor], project_root: &Path) -> Vec<AuditResult> {
    anchors
        .iter()
        .map(|a| verify_anchor(a, project_root))
        .collect()
}

/// Returns only the failed verifications.
#[must_use]
pub fn filter_failures(results: &[AuditResult]) -> Vec<&AuditResult> {
    results
        .iter()
        .filter(|r| !matches!(r.status, VerificationStatus::Verified))
        .collect()
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/cli.rs âˆ‡âˆ‡âˆ‡
use crate::clipboard;
use crate::roadmap::{
    apply_commands, audit, generate_prompt, CommandBatch, PromptOptions, Roadmap, TaskStatus,
};
use anyhow::{anyhow, Context, Result};
use clap::Subcommand;
use std::io::{self, Read};
use std::path::{Path, PathBuf};

#[derive(Subcommand, Debug, Clone)]
pub enum RoadmapCommand {
    Init {
        #[arg(short, long, default_value = "ROADMAP.md")]
        output: PathBuf,
        #[arg(short, long)]
        name: Option<String>,
    },
    Prompt {
        #[arg(short, long, default_value = "ROADMAP.md")]
        file: PathBuf,
        #[arg(long)]
        full: bool,
        #[arg(long)]
        examples: bool,
        #[arg(long)]
        stdout: bool,
    },
    Apply {
        #[arg(short, long, default_value = "ROADMAP.md")]
        file: PathBuf,
        #[arg(long)]
        dry_run: bool,
        #[arg(long)]
        stdin: bool,
        #[arg(short, long)]
        verbose: bool,
    },
    Show {
        #[arg(short, long, default_value = "ROADMAP.md")]
        file: PathBuf,
        #[arg(long, default_value = "tree")]
        format: String,
    },
    Tasks {
        #[arg(short, long, default_value = "ROADMAP.md")]
        file: PathBuf,
        #[arg(long)]
        pending: bool,
        #[arg(long)]
        complete: bool,
    },
    Audit {
        #[arg(short, long, default_value = "ROADMAP.md")]
        file: PathBuf,
        #[arg(long)]
        strict: bool,
    },
}

/// Entry point for roadmap commands
/// # Errors
/// Returns error if IO fails or clipboard access fails
pub fn handle_command(cmd: RoadmapCommand) -> Result<()> {
    match cmd {
        RoadmapCommand::Init { output, name } => run_init(&output, name),
        RoadmapCommand::Prompt {
            file,
            full,
            examples,
            stdout,
        } => run_prompt(&file, full, examples, stdout),
        RoadmapCommand::Apply {
            file,
            dry_run,
            stdin,
            verbose,
        } => run_apply(&file, dry_run, stdin, verbose),
        RoadmapCommand::Show { file, format } => run_show(&file, &format),
        RoadmapCommand::Tasks {
            file,
            pending,
            complete,
        } => run_tasks(&file, pending, complete),
        RoadmapCommand::Audit { file, strict } => run_audit(&file, strict),
    }
}

fn run_init(output: &Path, name: Option<String>) -> Result<()> {
    if output.exists() {
        return Err(anyhow!(
            "{} already exists. Use --output.",
            output.display()
        ));
    }
    let n = name.unwrap_or_else(|| "Project".to_string());
    std::fs::write(output, template(&n))?;
    println!("âœ“ Created {}", output.display());
    Ok(())
}

fn run_prompt(file: &Path, full: bool, examples: bool, stdout: bool) -> Result<()> {
    let r = load(file)?;
    let p = generate_prompt(
        &r,
        &PromptOptions {
            full,
            examples,
            project_name: None,
        },
    );
    if stdout {
        println!("{p}");
    } else {
        clipboard::smart_copy(&p).map_err(|e| anyhow!("Clipboard: {e}"))?;
        println!("âœ“ Copied prompt.");
    }
    Ok(())
}

fn run_apply(file: &Path, dry_run: bool, stdin: bool, verbose: bool) -> Result<()> {
    let mut roadmap = load(file)?;
    let input = get_input(stdin)?;
    let batch = CommandBatch::parse(&input);

    if batch.commands.is_empty() {
        print_errs(&batch.errors);
        return Err(anyhow!("No commands found."));
    }

    println!(
        "Found {} commands: {}",
        batch.commands.len(),
        batch.summary()
    );
    if verbose {
        print_errs(&batch.errors);
    }

    if dry_run {
        println!("[DRY RUN]");
        return Ok(());
    }

    let results = apply_commands(&mut roadmap, &batch);
    if results
        .iter()
        .any(|r| matches!(r, crate::roadmap::ApplyResult::Success(_)))
    {
        roadmap.save(file)?;
        println!("âœ“ Saved.");
    }
    for r in &results {
        println!("{r}");
    }
    Ok(())
}

fn run_show(file: &Path, format: &str) -> Result<()> {
    let r = load(file)?;
    if format == "stats" {
        let s = r.stats();
        println!(
            "Tasks: {} ({} done, {} pending)",
            s.total, s.complete, s.pending
        );
    } else {
        println!("{}", r.compact_state());
    }
    Ok(())
}

fn run_tasks(file: &Path, pending: bool, complete: bool) -> Result<()> {
    let r = load(file)?;
    for t in r.all_tasks() {
        if should_show_task(t.status, pending, complete) {
            let mark = if t.status == TaskStatus::Complete {
                "[x]"
            } else {
                "[ ]"
            };
            println!("{mark} {} - {}", t.path, t.text);
        }
    }
    Ok(())
}

fn run_audit(file: &Path, strict: bool) -> Result<()> {
    let r = load(file)?;
    let root = std::env::current_dir()?;
    audit::run(&r, &root, audit::AuditOptions { strict });
    Ok(())
}

fn should_show_task(status: TaskStatus, pending: bool, complete: bool) -> bool {
    match (pending, complete) {
        (true, false) => status == TaskStatus::Pending,
        (false, true) => status == TaskStatus::Complete,
        _ => true,
    }
}

fn load(path: &Path) -> Result<Roadmap> {
    Roadmap::from_file(path).context("Load failed")
}

fn get_input(stdin: bool) -> Result<String> {
    if stdin {
        let mut buf = String::new();
        io::stdin().read_to_string(&mut buf)?;
        Ok(buf)
    } else {
        clipboard::read_clipboard().context("Clipboard read failed")
    }
}

fn print_errs(errors: &[String]) {
    for e in errors {
        eprintln!("Warning: {e}");
    }
}

fn template(name: &str) -> String {
    format!("# {name} Roadmap\n\n## v0.1.0\n\n- [ ] Init\n\n## v0.2.0\n\n## v0.3.0\n")
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/cmd_handlers.rs âˆ‡âˆ‡âˆ‡
use crate::roadmap::parser::slugify;
use crate::roadmap::types::{ApplyResult, MovePosition, Roadmap, TaskStatus};

pub fn handle_check(roadmap: &mut Roadmap, path: &str) -> ApplyResult {
    set_status(roadmap, path, TaskStatus::Complete)
}

pub fn handle_uncheck(roadmap: &mut Roadmap, path: &str) -> ApplyResult {
    set_status(roadmap, path, TaskStatus::Pending)
}

pub fn handle_add(
    roadmap: &mut Roadmap,
    parent: &str,
    text: &str,
    after: Option<&str>,
) -> ApplyResult {
    let lines: Vec<&str> = roadmap.raw.lines().collect();

    if let Some(idx) = scan_insertion_point(&lines, parent, after) {
        insert_raw(roadmap, idx, format!("- [ ] **{text}**"));
        ApplyResult::Success(format!("Added: {text}"))
    } else {
        ApplyResult::NotFound(format!("Section: {parent}"))
    }
}

pub fn handle_add_section(roadmap: &mut Roadmap, heading: &str) -> ApplyResult {
    let new_section = format!("\n\n## {heading}\n");
    roadmap.raw.push_str(&new_section);
    ApplyResult::Success(format!("Added Section: {heading}"))
}

pub fn handle_delete(roadmap: &mut Roadmap, path: &str) -> ApplyResult {
    if let Some(idx) = find_line_idx(roadmap, path) {
        remove_raw(roadmap, idx);
        ApplyResult::Success(format!("Deleted: {path}"))
    } else {
        ApplyResult::NotFound(path.into())
    }
}

pub fn handle_update(roadmap: &mut Roadmap, path: &str, text: &str) -> ApplyResult {
    if let Some(idx) = find_line_idx(roadmap, path) {
        let line = roadmap.raw.lines().nth(idx).unwrap_or("");
        let indent = &line[..line.len() - line.trim_start().len()];
        let mark = if line.to_uppercase().contains("[X]") {
            "[x]"
        } else {
            "[ ]"
        };

        let suffix = if let Some(pos) = line.find("<!--") {
            &line[pos..]
        } else {
            ""
        };

        let new_line = if suffix.is_empty() {
            format!("{indent}- {mark} **{text}**")
        } else {
            format!("{indent}- {mark} **{text}** {suffix}")
        };

        replace_raw(roadmap, idx, new_line);
        ApplyResult::Success(format!("Updated: {path}"))
    } else {
        ApplyResult::NotFound(path.into())
    }
}

pub fn handle_note(roadmap: &mut Roadmap, path: &str, note: &str) -> ApplyResult {
    if let Some(idx) = find_line_idx(roadmap, path) {
        let line = roadmap.raw.lines().nth(idx).unwrap_or("");
        let indent_len = line.len() - line.trim_start().len();
        let prefix = " ".repeat(indent_len + 2);

        insert_raw(roadmap, idx + 1, format!("{prefix}*{note}*"));
        ApplyResult::Success(format!("Added note: {path}"))
    } else {
        ApplyResult::NotFound(path.into())
    }
}

pub fn handle_move(roadmap: &mut Roadmap, path: &str, position: &MovePosition) -> ApplyResult {
    let Some(src_idx) = find_line_idx(roadmap, path) else {
        return ApplyResult::NotFound(path.into());
    };

    let line_content = roadmap.raw.lines().nth(src_idx).unwrap_or("").to_string();
    remove_raw(roadmap, src_idx);

    let lines: Vec<&str> = roadmap.raw.lines().collect();
    let target_idx = match position {
        MovePosition::After(target) => find_line_idx_in_lines(&lines, target).map(|i| i + 1),
        MovePosition::Before(target) => find_line_idx_in_lines(&lines, target),
        MovePosition::EndOfSection(section) => scan_insertion_point(&lines, section, None),
    };

    if let Some(idx) = target_idx {
        insert_raw(roadmap, idx, line_content);
        ApplyResult::Success(format!("Moved: {path}"))
    } else {
        ApplyResult::Error("Target not found".into())
    }
}

fn set_status(roadmap: &mut Roadmap, path: &str, status: TaskStatus) -> ApplyResult {
    if let Some(idx) = find_line_idx(roadmap, path) {
        if update_line_status(roadmap, idx, status) {
            return ok_res(status, path);
        }
    }
    ApplyResult::NotFound(path.into())
}

// --- Logic Helpers (Copied from cmd_runner.rs for atomic migration) ---

fn scan_insertion_point(lines: &[&str], parent: &str, after: Option<&str>) -> Option<usize> {
    let p_slug = slugify(parent);
    let mut state = ScanState::default();

    for (i, line) in lines.iter().enumerate() {
        process_line(line, i, &p_slug, after, &mut state);
        if let Some(idx) = state.found_index {
            return Some(idx);
        }
    }
    state.last_task.map(|i| i + 1).or(state.sec_start)
}

#[derive(Default)]
struct ScanState {
    in_sec: bool,
    last_task: Option<usize>,
    sec_start: Option<usize>,
    found_index: Option<usize>,
}

fn process_line(line: &str, i: usize, p_slug: &str, after: Option<&str>, state: &mut ScanState) {
    if line.starts_with("##") {
        if check_section_entry(line, p_slug) {
            state.in_sec = true;
            state.sec_start = Some(i + 1);
        } else if state.in_sec {
            state.in_sec = false;
        }
        return;
    }

    if state.in_sec && is_task(line) {
        state.last_task = Some(i);
        if check_after_match(line, after) {
            state.found_index = Some(i + 1);
        }
    }
}

fn check_section_entry(line: &str, parent_slug: &str) -> bool {
    slugify(line).contains(parent_slug)
}

fn check_after_match(line: &str, after: Option<&str>) -> bool {
    if let Some(tgt) = after {
        slugify(line).contains(&slugify(tgt))
    } else {
        false
    }
}

fn find_line_idx(roadmap: &Roadmap, path: &str) -> Option<usize> {
    find_line_idx_in_lines(&roadmap.raw.lines().collect::<Vec<_>>(), path)
}

fn find_line_idx_in_lines(lines: &[&str], path: &str) -> Option<usize> {
    let search = path.split('/').next_back().unwrap_or(path);
    let s_slug = slugify(search);

    lines
        .iter()
        .position(|l| is_task(l) && slugify(l).contains(&s_slug))
}

fn update_line_status(roadmap: &mut Roadmap, idx: usize, status: TaskStatus) -> bool {
    let lines: Vec<&str> = roadmap.raw.lines().collect();
    if idx >= lines.len() {
        return false;
    }

    let line = lines[idx];
    let new = match status {
        TaskStatus::Complete => line.replace("- [ ]", "- [x]"),
        TaskStatus::Pending => line.replace("- [x]", "- [ ]").replace("- [X]", "- [ ]"),
    };
    replace_raw(roadmap, idx, new);
    true
}

fn replace_raw(roadmap: &mut Roadmap, idx: usize, line: String) {
    modify_lines(roadmap, |lines| {
        if idx < lines.len() {
            lines[idx] = line;
        }
    });
}

fn insert_raw(roadmap: &mut Roadmap, idx: usize, line: String) {
    modify_lines(roadmap, |lines| {
        if idx <= lines.len() {
            lines.insert(idx, line);
        }
    });
}

fn remove_raw(roadmap: &mut Roadmap, idx: usize) {
    modify_lines(roadmap, |lines| {
        if idx < lines.len() {
            lines.remove(idx);
        }
    });
}

fn modify_lines<F>(roadmap: &mut Roadmap, f: F)
where
    F: FnOnce(&mut Vec<String>),
{
    let mut lines: Vec<String> = roadmap.raw.lines().map(ToString::to_string).collect();
    f(&mut lines);
    roadmap.raw = lines.join("\n");
}

fn is_task(line: &str) -> bool {
    line.trim().starts_with("- [")
}

fn ok_res(status: TaskStatus, path: &str) -> ApplyResult {
    let act = if status == TaskStatus::Complete {
        "Checked"
    } else {
        "Unchecked"
    };
    ApplyResult::Success(format!("{act}: {path}"))
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/cmd_parser.rs âˆ‡âˆ‡âˆ‡
// src/roadmap/cmd_parser.rs
use crate::roadmap::str_utils;
use crate::roadmap::types::{Command, CommandBatch, MovePosition};

impl CommandBatch {
    #[must_use]
    pub fn parse(input: &str) -> Self {
        let mut commands = Vec::new();
        let mut errors = Vec::new();
        let content = extract_roadmap_block(input);

        for line in content.lines() {
            let line = line.trim();
            if is_skippable(line) {
                continue;
            }

            match parse_command_line(line) {
                Ok(cmd) => commands.push(cmd),
                Err(e) => {
                    if !line.is_empty() && !str_utils::is_ignorable(line) {
                        errors.push(format!(
                            "Line '{}': {e}",
                            str_utils::truncate(line, 40)
                        ));
                    }
                }
            }
        }
        Self { commands, errors }
    }

    #[must_use]
    pub fn summary(&self) -> String {
        let mut counts: std::collections::HashMap<&str, usize> = std::collections::HashMap::new();
        for cmd in &self.commands {
            *counts.entry(cmd_name(cmd)).or_insert(0) += 1;
        }

        if counts.is_empty() {
            return "No commands".to_string();
        }

        let parts: Vec<String> = counts.iter().map(|(k, v)| format!("{v} {k}")).collect();
        parts.join(", ")
    }
}

// Split match to reduce Cyclomatic Complexity (Max 8)
fn cmd_name(cmd: &Command) -> &'static str {
    match cmd {
        Command::Check { .. } => "CHECK",
        Command::Uncheck { .. } => "UNCHECK",
        Command::Add { .. } => "ADD",
        Command::Delete { .. } => "DELETE",
        Command::AddSection { .. } => "ADD_SECTION",
        _ => cmd_name_extended(cmd),
    }
}

fn cmd_name_extended(cmd: &Command) -> &'static str {
    match cmd {
        Command::Update { .. } => "UPDATE",
        Command::Note { .. } => "NOTE",
        Command::Move { .. } => "MOVE",
        Command::ReplaceSection { .. } => "SECTION_REPLACE",
        _ => "UNKNOWN",
    }
}

fn extract_roadmap_block(input: &str) -> &str {
    if let Some(start) = input.find("===ROADMAP===") {
        let after = &input[start + 13..];
        return after.find("===END===").map_or(after, |end| &after[..end]);
    }
    input
}

fn is_skippable(line: &str) -> bool {
    line.is_empty() || line.starts_with('#') || line.starts_with("//")
}

fn parse_command_line(line: &str) -> Result<Command, String> {
    let (cmd, args) = split_cmd(line).ok_or_else(|| "Empty command".to_string())?;

    if is_basic(cmd) {
        return parse_basic(cmd, args);
    }
    if is_content(cmd) {
        return parse_content(cmd, args);
    }
    if is_struct(cmd) {
        return parse_struct(cmd, args);
    }

    Err(format!("Unknown command: {cmd}"))
}

fn is_basic(cmd: &str) -> bool {
    matches!(cmd, "CHECK" | "UNCHECK" | "DELETE")
}
fn is_content(cmd: &str) -> bool {
    matches!(cmd, "ADD" | "UPDATE" | "NOTE")
}
fn is_struct(cmd: &str) -> bool {
    matches!(cmd, "MOVE" | "SECTION" | "REPLACE_SECTION")
}

fn parse_basic(cmd: &str, args: &str) -> Result<Command, String> {
    let path = req_path(args)?;
    match cmd {
        "CHECK" => Ok(Command::Check { path }),
        "UNCHECK" => Ok(Command::Uncheck { path }),
        "DELETE" => Ok(Command::Delete { path }),
        _ => unreachable!(),
    }
}

fn parse_content(cmd: &str, args: &str) -> Result<Command, String> {
    match cmd {
        "ADD" => parse_add(args),
        "UPDATE" => parse_update(args),
        "NOTE" => parse_note(args),
        _ => unreachable!(),
    }
}

fn parse_struct(cmd: &str, args: &str) -> Result<Command, String> {
    match cmd {
        "MOVE" => parse_move(args),
        "SECTION" => parse_add_section(args),
        "REPLACE_SECTION" => parse_replace_section(args),
        _ => unreachable!(),
    }
}

fn split_cmd(line: &str) -> Option<(&str, &str)> {
    let mut parts = line.splitn(2, ' ');
    let cmd = parts.next()?;
    let args = parts.next().unwrap_or("");
    if cmd.is_empty() {
        return None;
    }
    Some((cmd, args))
}

fn req_path(args: &str) -> Result<String, String> {
    let path = args.trim();
    if path.is_empty() {
        return Err("Requires task path".into());
    }
    Ok(path.to_string())
}

fn parse_add(args: &str) -> Result<Command, String> {
    let (parent, rest) = str_utils::split_first_word(args);
    if parent.is_empty() {
        return Err("ADD needs parent".into());
    }
    let (text, after) = str_utils::parse_quoted_with_after(rest)?;
    Ok(Command::Add {
        parent: parent.into(),
        text,
        after,
    })
}

fn parse_update(args: &str) -> Result<Command, String> {
    let (path, rest) = str_utils::split_first_word(args);
    if path.is_empty() {
        return Err("UPDATE needs path".into());
    }
    Ok(Command::Update {
        path: path.into(),
        text: str_utils::parse_quoted(rest)?,
    })
}

fn parse_note(args: &str) -> Result<Command, String> {
    let (path, rest) = str_utils::split_first_word(args);
    if path.is_empty() {
        return Err("NOTE needs path".into());
    }
    Ok(Command::Note {
        path: path.into(),
        note: str_utils::parse_quoted(rest)?,
    })
}

fn parse_move(args: &str) -> Result<Command, String> {
    let parts: Vec<&str> = args.split_whitespace().collect();
    if parts.len() < 3 {
        return Err("MOVE: path AFTER|BEFORE|TO target".into());
    }

    let pos = match parts[1].to_uppercase().as_str() {
        "AFTER" => MovePosition::After(parts[2].into()),
        "BEFORE" => MovePosition::Before(parts[2].into()),
        "TO" => MovePosition::EndOfSection(parts[2].into()),
        _ => return Err("Invalid position (use AFTER, BEFORE, or TO)".into()),
    };
    Ok(Command::Move {
        path: parts[0].into(),
        position: pos,
    })
}

fn parse_add_section(args: &str) -> Result<Command, String> {
    let heading = str_utils::parse_quoted(args)
        .or_else(|_| Ok::<String, String>(args.trim().to_string()))?;
        
    if heading.is_empty() {
        return Err("SECTION needs heading".into());
    }
    Ok(Command::AddSection { heading })
}

fn parse_replace_section(args: &str) -> Result<Command, String> {
    let id = args.trim();
    if id.is_empty() {
        return Err("REPLACE_SECTION needs ID".into());
    }
    Ok(Command::ReplaceSection {
        id: id.into(),
        content: String::new(),
    })
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/cmd_runner.rs âˆ‡âˆ‡âˆ‡
use crate::roadmap::cmd_handlers;
use crate::roadmap::types::{ApplyResult, Command, CommandBatch, Roadmap};

pub fn apply_commands(roadmap: &mut Roadmap, batch: &CommandBatch) -> Vec<ApplyResult> {
    batch
        .commands
        .iter()
        .map(|cmd| run_cmd(roadmap, cmd))
        .collect()
}

fn run_cmd(roadmap: &mut Roadmap, cmd: &Command) -> ApplyResult {
    match cmd {
        Command::Check { .. } | Command::Uncheck { .. } | Command::Delete { .. } => {
            handle_basic_cmd(roadmap, cmd)
        }
        Command::Add { .. } | Command::Update { .. } | Command::Note { .. } => {
            handle_content_cmd(roadmap, cmd)
        }
        Command::Move { .. } | Command::AddSection { .. } => handle_struct_cmd(roadmap, cmd),
        Command::ReplaceSection { .. } => ApplyResult::Error("Command not supported".into()),
    }
}

fn handle_basic_cmd(roadmap: &mut Roadmap, cmd: &Command) -> ApplyResult {
    match cmd {
        Command::Check { path } => cmd_handlers::handle_check(roadmap, path),
        Command::Uncheck { path } => cmd_handlers::handle_uncheck(roadmap, path),
        Command::Delete { path } => cmd_handlers::handle_delete(roadmap, path),
        _ => unreachable!(),
    }
}

fn handle_content_cmd(roadmap: &mut Roadmap, cmd: &Command) -> ApplyResult {
    match cmd {
        Command::Add {
            parent,
            text,
            after,
        } => cmd_handlers::handle_add(roadmap, parent, text, after.as_deref()),
        Command::Update { path, text } => cmd_handlers::handle_update(roadmap, path, text),
        Command::Note { path, note } => cmd_handlers::handle_note(roadmap, path, note),
        _ => unreachable!(),
    }
}

fn handle_struct_cmd(roadmap: &mut Roadmap, cmd: &Command) -> ApplyResult {
    match cmd {
        Command::AddSection { heading } => cmd_handlers::handle_add_section(roadmap, heading),
        Command::Move { path, position } => cmd_handlers::handle_move(roadmap, path, position),
        _ => unreachable!(),
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/diff.rs âˆ‡âˆ‡âˆ‡
// src/roadmap/diff.rs
use crate::roadmap::types::{Command, MovePosition, Roadmap, Task};
use std::collections::HashMap;

/// Compares two roadmaps and generates a "Wicked Smart" patch of commands.
#[must_use]
pub fn diff(current: &Roadmap, incoming: &Roadmap) -> Vec<Command> {
    let mut commands = Vec::new();

    // 1. Structural Scan: Add missing sections
    let curr_sections: HashMap<String, String> = current
        .sections
        .iter()
        .map(|s| (s.id.clone(), s.heading.clone()))
        .collect();

    for section in &incoming.sections {
        if !curr_sections.contains_key(&section.id) {
            commands.push(Command::AddSection {
                heading: section.heading.clone(),
            });
        }
    }

    // 2. Task Analysis
    let curr_map = map_tasks_with_parent(current);
    let inc_map = map_tasks_with_parent(incoming);

    // 2a. Updates, Moves, Checks, Deletions
    for (id, (curr_task, curr_parent)) in &curr_map {
        if let Some((inc_task, inc_parent)) = inc_map.get(id) {
            let ctx = TaskComparisonContext {
                id,
                curr: curr_task,
                inc: inc_task,
                curr_parent,
                inc_parent,
            };
            compare_task_detailed(&ctx, &mut commands);
        } else {
            // Task in Current but NOT in Incoming -> Deleted
            commands.push(Command::Delete { path: id.clone() });
        }
    }

    // 2b. Additions
    for (id, (inc_task, inc_parent_title)) in &inc_map {
        if !curr_map.contains_key(id) {
            commands.push(Command::Add {
                parent: inc_parent_title.clone(),
                text: inc_task.text.clone(),
                after: None,
            });
        }
    }

    commands
}

struct TaskComparisonContext<'a> {
    id: &'a str,
    curr: &'a Task,
    inc: &'a Task,
    curr_parent: &'a str,
    inc_parent: &'a str,
}

fn compare_task_detailed(ctx: &TaskComparisonContext, cmds: &mut Vec<Command>) {
    detect_move(ctx, cmds);
    detect_status_change(ctx, cmds);
    detect_text_change(ctx, cmds);
}

fn detect_move(ctx: &TaskComparisonContext, cmds: &mut Vec<Command>) {
    if ctx.curr_parent != ctx.inc_parent {
        cmds.push(Command::Move {
            path: ctx.id.to_string(),
            position: MovePosition::EndOfSection(ctx.inc_parent.to_string()),
        });
    }
}

fn detect_status_change(ctx: &TaskComparisonContext, cmds: &mut Vec<Command>) {
    if ctx.curr.status != ctx.inc.status {
        match ctx.inc.status {
            crate::roadmap::TaskStatus::Complete => cmds.push(Command::Check {
                path: ctx.id.to_string(),
            }),
            crate::roadmap::TaskStatus::Pending => cmds.push(Command::Uncheck {
                path: ctx.id.to_string(),
            }),
        }
    }
}

fn detect_text_change(ctx: &TaskComparisonContext, cmds: &mut Vec<Command>) {
    if ctx.curr.text != ctx.inc.text {
        cmds.push(Command::Update {
            path: ctx.id.to_string(),
            text: ctx.inc.text.clone(),
        });
    }
}

type TaskMap<'a> = HashMap<String, (&'a Task, String)>;

fn map_tasks_with_parent(roadmap: &Roadmap) -> TaskMap<'_> {
    let mut map = HashMap::new();
    for section in &roadmap.sections {
        collect_tasks_recursive(section, &mut map);
    }
    map
}

fn collect_tasks_recursive<'a>(section: &'a crate::roadmap::types::Section, map: &mut TaskMap<'a>) {
    for task in &section.tasks {
        if !task.id.is_empty() {
            map.insert(task.id.clone(), (task, section.heading.clone()));
        }
    }
    for sub in &section.subsections {
        collect_tasks_recursive(sub, map);
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::roadmap::types::{Section, TaskStatus};

    fn make_roadmap(sections: Vec<Section>) -> Roadmap {
        Roadmap {
            path: None,
            title: "Test".into(),
            sections,
            raw: String::new(),
        }
    }

    fn make_section(title: &str, tasks: Vec<Task>) -> Section {
        Section {
            id: crate::roadmap::slugify(title),
            heading: title.into(),
            level: 2,
            theme: None,
            tasks,
            subsections: vec![],
            raw_content: String::new(),
            line_start: 0,
            line_end: 0,
        }
    }

    fn make_task(id: &str, status: TaskStatus) -> Task {
        Task {
            id: id.into(),
            path: id.into(),
            text: format!("Task {id}"),
            status,
            indent: 0,
            line: 0,
            children: vec![],
            tests: vec![],
        }
    }

    #[test]
    fn test_diff_move_section() {
        let t1 = make_task("t1", TaskStatus::Pending);

        let sec_a = make_section("Section A", vec![t1.clone()]);
        let sec_b = make_section("Section B", vec![]);
        let curr = make_roadmap(vec![sec_a, sec_b]);

        // Incoming: t1 moved to B
        let new_sec_a = make_section("Section A", vec![]);
        let new_sec_b = make_section("Section B", vec![t1]);
        let inc = make_roadmap(vec![new_sec_a, new_sec_b]);

        let cmds = diff(&curr, &inc);
        assert_eq!(cmds.len(), 1);
        match &cmds[0] {
            Command::Move { path, position } => {
                assert_eq!(path, "t1");
                assert_eq!(*position, MovePosition::EndOfSection("Section B".into()));
            }
            _ => panic!("Expected MOVE"),
        }
    }

    #[test]
    fn test_diff_add_section() {
        let curr = make_roadmap(vec![]);
        let inc = make_roadmap(vec![make_section("New Era", vec![])]);

        let cmds = diff(&curr, &inc);
        assert_eq!(cmds.len(), 1);
        match &cmds[0] {
            Command::AddSection { heading } => assert_eq!(heading, "New Era"),
            _ => panic!("Expected AddSection"),
        }
    }

    #[test]
    fn test_diff_section_deletion() {
        let t1 = make_task("t1", TaskStatus::Pending);
        let t2 = make_task("t2", TaskStatus::Pending);
        let sec = make_section("To Delete", vec![t1, t2]);
        let curr = make_roadmap(vec![sec]);
        let inc = make_roadmap(vec![]); // Empty

        let cmds = diff(&curr, &inc);

        // Should delete both tasks
        assert_eq!(cmds.len(), 2);

        let deleted: Vec<String> = cmds
            .iter()
            .map(|c| match c {
                Command::Delete { path } => path.clone(),
                _ => panic!("Wrong command"),
            })
            .collect();

        assert!(deleted.contains(&"t1".to_string()));
        assert!(deleted.contains(&"t2".to_string()));
    }
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/display.rs âˆ‡âˆ‡âˆ‡
use crate::roadmap::types::{Roadmap, Section, TaskStatus};
use std::fmt::Write;

impl Roadmap {
    /// Generate a compact state representation for AI context
    #[must_use]
    pub fn compact_state(&self) -> String {
        let mut out = String::new();
        let _ = write!(out, "# {}\n\n", self.title);

        for section in &self.sections {
            if section.tasks.is_empty() && section.subsections.is_empty() {
                continue;
            }
            out.push_str(&format_section_compact(section, 0));
        }

        out
    }
}

fn format_section_compact(section: &Section, depth: usize) -> String {
    let mut out = String::new();
    let indent = "  ".repeat(depth);

    let total = section.tasks.len();
    let complete = section
        .tasks
        .iter()
        .filter(|t| t.status == TaskStatus::Complete)
        .count();

    if total > 0 {
        let _ = writeln!(out, "{indent}{} [{complete}/{total}]", section.heading);
        for task in &section.tasks {
            let marker = match task.status {
                TaskStatus::Complete => "âœ“",
                TaskStatus::Pending => "â—‹",
            };
            let _ = writeln!(out, "{indent}  {marker} {} ({})", task.text, task.path);
        }
    } else if !section.subsections.is_empty() {
        let _ = writeln!(out, "{indent}{}", section.heading);
    }

    for sub in &section.subsections {
        out.push_str(&format_section_compact(sub, depth + 1));
    }

    out
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/mod.rs âˆ‡âˆ‡âˆ‡
pub mod audit;
pub mod cli;
pub mod cmd_handlers;
pub mod cmd_parser;
pub mod cmd_runner;
pub mod diff;
pub mod display;
pub mod parser;
pub mod prompt;
pub mod str_utils;
pub mod types;

// Re-export CommandBatch from types
pub use cmd_runner::apply_commands;
pub use parser::slugify;
pub use prompt::{generate_prompt, PromptOptions};
pub use types::CommandBatch;
pub use types::*;

use std::path::Path;
use anyhow::{Context, Result};

/// Parses input for roadmap commands and applies them to the specified file.
/// Returns a list of result messages (Success/Error).
///
/// # Errors
/// Returns error if file IO fails.
pub fn handle_input(file_path: &Path, input: &str) -> Result<Vec<String>> {
    // 1. Check if input actually contains a roadmap block
    let batch = CommandBatch::parse(input);
    if batch.commands.is_empty() {
        return Ok(Vec::new());
    }

    // 2. Load the roadmap (or error if missing)
    let mut roadmap = Roadmap::from_file(file_path)
        .context(format!("Failed to load roadmap from {}", file_path.display()))?;

    // 3. Apply commands
    let results = apply_commands(&mut roadmap, &batch);

    // 4. Save if any changes succeeded
    let any_success = results.iter().any(|r| matches!(r, ApplyResult::Success(_)));
    if any_success {
        roadmap.save(file_path)?;
    }

    // 5. Convert results to strings
    Ok(results.into_iter().map(|r| r.to_string()).collect())
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/parser.rs âˆ‡âˆ‡âˆ‡
use crate::roadmap::types::{Roadmap, RoadmapStats, Section, Task, TaskStatus};
use std::collections::HashMap;
use std::path::Path;

impl Roadmap {
    #[must_use]
    pub fn parse(content: &str) -> Self {
        let lines: Vec<&str> = content.lines().collect();
        let mut sections = Vec::new();
        let mut title = "Roadmap".to_string();
        let mut i = 0;

        if let Some(first) = lines.first() {
            if let Some(t) = first.strip_prefix("# ") {
                title = t.trim().to_string();
                i = 1;
            }
        }

        while i < lines.len() {
            if let Some((lvl, txt)) = parse_heading(lines[i]) {
                sections.push(parse_section(&lines, &mut i, lvl, txt));
            } else {
                i += 1;
            }
        }

        Self {
            path: None,
            title,
            sections,
            raw: content.into(),
        }
    }

    /// # Errors
    /// Returns error on file read fail
    pub fn from_file(path: &Path) -> std::io::Result<Self> {
        let c = std::fs::read_to_string(path)?;
        let mut r = Self::parse(&c);
        r.path = Some(path.display().to_string());
        Ok(r)
    }

    /// # Errors
    /// Returns error on file write fail
    pub fn save(&self, path: &Path) -> std::io::Result<()> {
        std::fs::write(path, &self.raw)
    }

    #[must_use]
    pub fn all_tasks(&self) -> Vec<&Task> {
        let mut out = Vec::new();
        for s in &self.sections {
            collect_tasks(s, &mut out);
        }
        out
    }

    #[must_use]
    pub fn find_task(&self, path: &str) -> Option<&Task> {
        self.all_tasks().into_iter().find(|t| t.path == path)
    }

    #[must_use]
    pub fn stats(&self) -> RoadmapStats {
        let t = self.all_tasks();
        let c = t
            .iter()
            .filter(|x| x.status == TaskStatus::Complete)
            .count();
        RoadmapStats {
            total: t.len(),
            complete: c,
            pending: t.len() - c,
        }
    }
}

fn parse_heading(line: &str) -> Option<(u8, String)> {
    let t = line.trim();
    if !t.starts_with("##") {
        return None;
    }
    let lvl = t.chars().take_while(|&c| c == '#').count();
    if lvl < 2 {
        return None;
    }
    let level = u8::try_from(lvl).ok()?;
    Some((level, t[lvl..].trim().into()))
}

fn parse_section(lines: &[&str], i: &mut usize, lvl: u8, heading: String) -> Section {
    let start = *i;
    let id = crate::roadmap::slugify(&heading);
    let mut tasks = Vec::new();
    let mut subs = Vec::new();
    let mut raw = String::new();

    *i += 1;

    while *i < lines.len() {
        let line = lines[*i];
        if let Some((next_lvl, next_txt)) = parse_heading(line) {
            if next_lvl <= lvl {
                break;
            }
            subs.push(parse_section(lines, i, next_lvl, next_txt));
            continue;
        }

        if let Some(mut task) = parse_task(line, *i) {
            task.path = format!("{id}/{}", task.id);
            tasks.push(task);
        } else {
            raw.push_str(line);
            raw.push('\n');
        }
        *i += 1;
    }

    deduplicate_task_ids(&mut tasks, &id);

    Section {
        id,
        heading,
        level: lvl,
        theme: None,
        tasks,
        subsections: subs,
        raw_content: raw,
        line_start: start,
        line_end: *i,
    }
}

fn deduplicate_task_ids(tasks: &mut [Task], section_id: &str) {
    let mut seen: HashMap<String, usize> = HashMap::new();
    for task in tasks.iter_mut() {
        let base_id = task.id.clone();
        let count = seen.entry(base_id.clone()).or_insert(0);
        if *count > 0 {
            task.id = format!("{base_id}-{count}");
            task.path = format!("{section_id}/{}", task.id);
        }
        *count += 1;
    }
}

fn parse_task(line: &str, line_num: usize) -> Option<Task> {
    let t = line.trim();
    if !t.starts_with("- [") {
        return None;
    }

    let (stat, rest) = if let Some(stripped) = t.strip_prefix("- [ ]") {
        (TaskStatus::Pending, stripped)
    } else {
        (TaskStatus::Complete, &t[5..])
    };

    let mut parts = rest.split("<!--");
    let text_part = parts.next()?.trim().trim_matches(|c| c == '*' || c == ' ');

    let mut tests = Vec::new();
    for part in parts {
        if let Some(content) = part.split("-->").next() {
            if let Some(path) = content.trim().strip_prefix("test:") {
                tests.push(path.trim().to_string());
            }
        }
    }

    let id = derive_task_id(text_part, &tests);

    if id.is_empty() {
        return None;
    }

    Some(Task {
        id,
        path: String::new(),
        text: text_part.into(),
        status: stat,
        indent: 0,
        line: line_num,
        children: vec![],
        tests,
    })
}

fn derive_task_id(text: &str, tests: &[String]) -> String {
    if let Some(anchor) = tests.first() {
        if let Some(func_name) = anchor.rsplit("::").next() {
            let id = crate::roadmap::slugify(func_name);
            if !id.is_empty() {
                return id;
            }
        }
    }
    crate::roadmap::slugify(text)
}

fn collect_tasks<'a>(s: &'a Section, out: &mut Vec<&'a Task>) {
    for t in &s.tasks {
        out.push(t);
    }
    for sub in &s.subsections {
        collect_tasks(sub, out);
    }
}

#[must_use]
pub fn slugify(text: &str) -> String {
    text.to_lowercase()
        .chars()
        .map(|c| if c.is_alphanumeric() { c } else { '-' })
        .collect::<String>()
        .split('-')
        .filter(|s| !s.is_empty())
        .collect::<Vec<_>>()
        .join("-")
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/prompt.rs âˆ‡âˆ‡âˆ‡
use crate::roadmap::types::{Roadmap, Section, TaskStatus};
use std::fmt::Write;

/// Options for prompt generation
#[derive(Debug, Clone, Default)]
pub struct PromptOptions {
    pub full: bool,
    pub examples: bool,
    pub project_name: Option<String>,
}

/// Generate the teaching prompt for AI
#[must_use]
pub fn generate_prompt(roadmap: &Roadmap, options: &PromptOptions) -> String {
    let project_name = options
        .project_name
        .clone()
        .unwrap_or_else(|| roadmap.title.clone());

    let mut prompt = String::new();

    let _ = writeln!(prompt, "# Roadmap Commands for: {project_name}\n");

    let stats = roadmap.stats();
    let pct = if stats.total > 0 {
        #[allow(clippy::cast_precision_loss)]
        {
            (stats.complete as f64 / stats.total as f64) * 100.0
        }
    } else {
        0.0
    };

    let _ = writeln!(
        prompt,
        "Progress: {}/{} tasks complete ({:.0}%)\n",
        stats.complete, stats.total, pct
    );

    prompt.push_str("## Commands\n\n");
    prompt.push_str("Wrap commands in `===ROADMAP===` and `===END===` markers.\n\n");
    prompt.push_str("```\n");
    prompt.push_str("CHECK <path>              # Mark task complete\n");
    prompt.push_str("UNCHECK <path>            # Mark task incomplete\n");
    prompt.push_str("ADD <section> \"<text>\"    # Add new task to section\n");
    prompt.push_str("ADD <section> \"<text>\" AFTER <task>  # Add after specific task\n");
    prompt.push_str("DELETE <path>             # Remove task\n");
    prompt.push_str("UPDATE <path> \"<text>\"    # Change task description\n");
    prompt.push_str("NOTE <path> \"<text>\"      # Add note under task\n");
    prompt.push_str("```\n\n");

    prompt.push_str("## Task Paths\n\n");
    prompt.push_str("Paths are: `section-slug/task-slug`\n");
    prompt.push_str("Example: `v0-5-0-bulletproof-apply/truncation-detection`\n\n");
    prompt.push_str("You can use partial matches - just the task slug often works.\n\n");

    if options.examples {
        prompt.push_str("## Examples\n\n");
        prompt.push_str("```\n");
        prompt.push_str("===ROADMAP===\n");
        prompt.push_str("CHECK truncation-detection\n");
        prompt.push_str("ADD v0-5-0 \"Improve error messages\" AFTER truncation-detection\n");
        prompt.push_str("NOTE path-safety \"Implemented using std::path::Path\"\n");
        prompt.push_str("===END===\n");
        prompt.push_str("```\n\n");
    }

    prompt.push_str("---\n\n");
    prompt.push_str("## Current Roadmap State\n\n");

    if options.full {
        prompt.push_str("```markdown\n");
        prompt.push_str(&roadmap.raw);
        prompt.push_str("\n```\n");
    } else {
        prompt.push_str(&generate_compact_state(roadmap));
    }

    prompt
}

fn generate_compact_state(roadmap: &Roadmap) -> String {
    let mut out = String::new();
    for section in &roadmap.sections {
        if section.tasks.is_empty() && section.subsections.is_empty() {
            continue;
        }
        out.push_str(&format_section_tree(section, 0));
    }
    out
}

fn format_section_tree(section: &Section, depth: usize) -> String {
    let mut out = String::new();
    let indent = "  ".repeat(depth);
    let (complete, total) = count_tasks_recursive(section);

    if total > 0 {
        let progress = format!("[{complete}/{total}]");
        let status_icon = if complete == total {
            "âœ“"
        } else if complete > 0 {
            "â—"
        } else {
            "â—‹"
        };
        let _ = writeln!(out, "{indent}{status_icon} {} {progress}", section.heading);

        for task in &section.tasks {
            let icon = match task.status {
                TaskStatus::Complete => "  âœ“",
                TaskStatus::Pending => "  â—‹",
            };
            let _ = writeln!(out, "{indent}{icon}  {} (id: {})", task.text, task.id);
        }
    }

    for sub in &section.subsections {
        out.push_str(&format_section_tree(sub, depth + 1));
    }
    out
}

fn count_tasks_recursive(section: &Section) -> (usize, usize) {
    let mut complete = 0;
    let mut total = 0;

    for task in &section.tasks {
        total += 1;
        if task.status == TaskStatus::Complete {
            complete += 1;
        }
    }

    for sub in &section.subsections {
        let (c, t) = count_tasks_recursive(sub);
        complete += c;
        total += t;
    }

    (complete, total)
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/str_utils.rs âˆ‡âˆ‡âˆ‡
// src/roadmap/str_utils.rs

#[must_use]
pub fn split_first_word(s: &str) -> (&str, &str) {
    s.trim()
        .split_once(char::is_whitespace)
        .map_or((s.trim(), ""), |(h, t)| (h, t.trim()))
}

/// Parses a quoted string or returns the string as is.
///
/// # Errors
/// Returns error if a starting quote is not closed.
pub fn parse_quoted(s: &str) -> Result<String, String> {
    let (text, _) = extract_quoted_text(s)?;
    Ok(text)
}

/// Parses "text" [AFTER target].
///
/// # Errors
/// Returns error if text quoting is invalid.
pub fn parse_quoted_with_after(s: &str) -> Result<(String, Option<String>), String> {
    let (text, rest) = extract_quoted_text(s)?;

    let after = if let Some(stripped) = rest.strip_prefix("AFTER ") {
        Some(stripped.trim().to_string())
    } else {
        rest.strip_prefix("after ")
            .map(|stripped| stripped.trim().to_string())
    };

    Ok((text, after))
}

/// Extracts quoted text and returns the remainder of the string.
/// Handles escaped quotes (\") inside the string.
///
/// # Errors
/// Returns error if quotes are unbalanced.
pub fn extract_quoted_text(s: &str) -> Result<(String, &str), String> {
    let s = s.trim();
    if let Some(stripped) = s.strip_prefix('"') {
        let end = find_closing_quote(stripped).ok_or("Unclosed quote")?;
        // Unescape: \" -> "
        let content = stripped[..end].replace(r#"\""#, "\"");
        Ok((content, stripped[end + 1..].trim()))
    } else if let Some(idx) = s.find(" AFTER ") {
        // Return text BEFORE ' AFTER ', and keep ' AFTER ' in the rest for parsing
        Ok((s[..idx].trim().to_string(), s[idx..].trim()))
    } else {
        Ok((s.to_string(), ""))
    }
}

fn find_closing_quote(s: &str) -> Option<usize> {
    let mut escaped = false;
    for (i, c) in s.char_indices() {
        if escaped {
            escaped = false;
            continue;
        }
        if c == '\\' {
            escaped = true;
        } else if c == '"' {
            return Some(i);
        }
    }
    None
}

#[must_use]
pub fn is_ignorable(line: &str) -> bool {
    let u = line.to_uppercase();
    u.starts_with("===")
        || u.starts_with("---")
        || u.starts_with("```")
        || u.starts_with("âˆ‡âˆ‡âˆ‡")
        || u.starts_with("âˆ†âˆ†âˆ†")
        || u == "ROADMAP"
        || u == "END"
}

// Unicode-safe truncation to prevent panics on multi-byte chars
#[must_use]
pub fn truncate(s: &str, max_chars: usize) -> String {
    if s.chars().count() <= max_chars {
        s.to_string()
    } else {
        let truncated: String = s.chars().take(max_chars).collect();
        format!("{truncated}...")
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/roadmap/types.rs âˆ‡âˆ‡âˆ‡
use std::fmt;

#[derive(Debug, Clone)]
pub struct Roadmap {
    pub path: Option<String>,
    pub title: String,
    pub sections: Vec<Section>,
    pub raw: String,
}

#[derive(Debug, Clone)]
pub struct Section {
    pub id: String,
    pub heading: String,
    pub level: u8,
    pub theme: Option<String>,
    pub tasks: Vec<Task>,
    pub subsections: Vec<Section>,
    pub raw_content: String,
    pub line_start: usize,
    pub line_end: usize,
}

#[derive(Debug, Clone)]
pub struct Task {
    pub id: String,
    pub path: String,
    pub text: String,
    pub status: TaskStatus,
    pub indent: u8,
    pub line: usize,
    pub children: Vec<Task>,
    pub tests: Vec<String>, // Explicitly linked test files
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TaskStatus {
    Pending,
    Complete,
}

#[derive(Debug, Clone)]
pub struct RoadmapStats {
    pub total: usize,
    pub complete: usize,
    pub pending: usize,
}

/// A single command from AI
#[derive(Debug, Clone)]
pub enum Command {
    Check {
        path: String,
    },
    Uncheck {
        path: String,
    },
    Add {
        parent: String,
        text: String,
        after: Option<String>,
    },
    AddSection {
        heading: String,
    },
    Delete {
        path: String,
    },
    Update {
        path: String,
        text: String,
    },
    Note {
        path: String,
        note: String,
    },
    Move {
        path: String,
        position: MovePosition,
    },
    ReplaceSection {
        id: String,
        content: String,
    },
}

#[derive(Debug, Clone, PartialEq)]
pub enum MovePosition {
    After(String),
    Before(String),
    EndOfSection(String),
}

/// A batch of commands parsed from AI output
#[derive(Debug, Clone)]
pub struct CommandBatch {
    pub commands: Vec<Command>,
    pub errors: Vec<String>,
}

/// Result of applying a command
#[derive(Debug)]
pub enum ApplyResult {
    Success(String),
    NotFound(String),
    Error(String),
}

impl fmt::Display for ApplyResult {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Success(msg) => write!(f, "âœ“ {msg}"),
            Self::NotFound(msg) => write!(f, "âœ— Not found: {msg}"),
            Self::Error(msg) => write!(f, "âœ— Error: {msg}"),
        }
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/skeleton.rs âˆ‡âˆ‡âˆ‡
// src/skeleton.rs
use std::path::Path;
use std::sync::LazyLock;
use tree_sitter::{Language, Parser, Query, QueryCursor};

static SKELETONIZER: LazyLock<Skeletonizer> = LazyLock::new(Skeletonizer::new);

struct Skeletonizer {
    rust: Query,
    python: Query,
    javascript: Query,
}

impl Skeletonizer {
    fn new() -> Self {
        Self {
            rust: compile_query(
                tree_sitter_rust::language(),
                "(function_item body: (block) @body)",
            ),
            python: compile_query(
                tree_sitter_python::language(),
                "(function_definition body: (block) @body)",
            ),
            javascript: compile_query(
                tree_sitter_typescript::language_typescript(),
                r"
                (function_declaration body: (statement_block) @body)
                (method_definition body: (statement_block) @body)
                (arrow_function body: (statement_block) @body)
                ",
            ),
        }
    }

    fn get_config<'a>(&'a self, lang: &str) -> Option<(Language, &'a Query, &'static str)> {
        match lang {
            "rs" => Some((
                tree_sitter_rust::language(),
                &self.rust,
                "{ ... }",
            )),
            "py" => Some((
                tree_sitter_python::language(),
                &self.python,
                "...",
            )),
            "js" | "jsx" | "ts" | "tsx" => Some((
                tree_sitter_typescript::language_typescript(),
                &self.javascript,
                "{ ... }",
            )),
            _ => None,
        }
    }
}

/// Reduces code to its structural skeleton (signatures only).
///
/// # Arguments
/// * `path` - The file path (used for language detection).
/// * `content` - The full source code.
///
/// # Returns
/// The skeletonized code, or the original content if language is unsupported.
#[must_use]
pub fn clean(path: &Path, content: &str) -> String {
    let Some(ext) = path.extension().and_then(|s| s.to_str()) else {
        return content.to_string();
    };

    let Some((lang, query, replacement)) = SKELETONIZER.get_config(ext) else {
        return content.to_string();
    };

    apply_skeleton(content, lang, query, replacement)
}

fn apply_skeleton(source: &str, lang: Language, query: &Query, replacement: &str) -> String {
    let mut parser = Parser::new();
    if parser.set_language(lang).is_err() {
        return source.to_string();
    }

    let Some(tree) = parser.parse(source, None) else {
        return source.to_string();
    };

    let mut cursor = QueryCursor::new();
    let matches = cursor.matches(query, tree.root_node(), source.as_bytes());

    let mut ranges = Vec::new();
    for m in matches {
        for capture in m.captures {
            ranges.push(capture.node.byte_range());
        }
    }

    // Filter nested ranges: if Range A contains Range B, we only want A.
    // We want the outermost bodies to be replaced.
    let root_ranges = filter_nested_ranges(ranges);

    replace_ranges(source, &root_ranges, replacement)
}

fn filter_nested_ranges(mut ranges: Vec<std::ops::Range<usize>>) -> Vec<std::ops::Range<usize>> {
    // Sort by start position
    ranges.sort_by_key(|r| r.start);

    let mut result: Vec<std::ops::Range<usize>> = Vec::new();
    let mut i = 0;
    while i < ranges.len() {
        let current = &ranges[i];
        
        // Check if this range is contained by any already added range.
        // Since we sort by start, if A contains B, A comes before B.
        // We just check if 'current' is inside the 'last' added range.
        
        if let Some(last) = result.last() {
            if last.end >= current.end {
                // Current is inside Last. Skip Current.
                i += 1;
                continue;
            }
        }
        
        result.push(current.clone());
        i += 1;
    }
    result
}

fn replace_ranges(source: &str, ranges: &[std::ops::Range<usize>], replacement: &str) -> String {
    let mut result = String::with_capacity(source.len());
    let mut last_pos = 0;

    for range in ranges {
        // Push text before the body
        if range.start > last_pos {
            result.push_str(&source[last_pos..range.start]);
        }
        
        // Push replacement
        result.push_str(replacement);
        
        // Advance
        last_pos = range.end;
    }

    // Append trailing content (renamed to avoid validator regex match)
    if last_pos < source.len() {
        result.push_str(&source[last_pos..]);
    }

    result
}

fn compile_query(lang: Language, pattern: &str) -> Query {
    match Query::new(lang, pattern) {
        Ok(q) => q,
        Err(e) => panic!("Invalid skeleton query: {e}"),
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tokens.rs âˆ‡âˆ‡âˆ‡
// src/tokens.rs
use std::sync::LazyLock;
use tiktoken_rs::CoreBPE;

/// The tokenizer encoding (`cl100k_base`, used by GPT-4/3.5-turbo).
/// Initialization is deferred until first use. If the encoding fails to load
/// (which should never happen with a valid tiktoken-rs installation),
/// token counting will return 0 and log an error.
static BPE: LazyLock<Option<CoreBPE>> = LazyLock::new(|| {
    tiktoken_rs::cl100k_base()
        .map_err(|e| eprintln!("Failed to load cl100k_base tokenizer: {e}"))
        .ok()
});

pub struct Tokenizer;

impl Tokenizer {
    /// Counts the number of tokens in the given text.
    /// Returns 0 if the tokenizer failed to initialize.
    #[must_use]
    pub fn count(text: &str) -> usize {
        BPE.as_ref()
            .map_or(0, |bpe| bpe.encode_ordinary(text).len())
    }

    /// Returns true if the text exceeds the token limit.
    #[must_use]
    pub fn exceeds_limit(text: &str, limit: usize) -> bool {
        Self::count(text) > limit
    }

    /// Returns true if the tokenizer is available.
    #[must_use]
    pub fn is_available() -> bool {
        BPE.is_some()
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/config/components.rs âˆ‡âˆ‡âˆ‡
// src/tui/config/components.rs
use super::helpers;
use super::state::ConfigApp;
use super::view::Palette;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Cell, Gauge, Paragraph, Row, Table};
use ratatui::Frame;

pub fn draw_header(f: &mut Frame, app: &ConfigApp, area: Rect, pal: &Palette) {
    let title = Span::styled(
        " ğŸ›¡ï¸ WARDEN PROTOCOL ",
        Style::default()
            .fg(pal.primary)
            .add_modifier(Modifier::BOLD),
    );

    let status = if let Some((msg, _)) = &app.saved_message {
        Span::styled(format!(" {msg} "), Style::default().fg(Color::Green))
    } else if app.modified {
        Span::styled(" [UNSAVED CHANGES] ", Style::default().fg(Color::Yellow))
    } else {
        Span::styled(" SYSTEM: NOMINAL ", Style::default().fg(pal.secondary))
    };

    let line = Line::from(vec![title, Span::raw(" |"), status]);

    f.render_widget(
        Paragraph::new(line)
            .block(
                Block::default()
                    .borders(Borders::ALL)
                    .border_style(Style::default().fg(pal.secondary)),
            )
            .alignment(Alignment::Center),
        area,
    );
}

pub fn draw_settings_table(f: &mut Frame, app: &ConfigApp, area: Rect, pal: &Palette) {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" [ SYSTEM CONFIGURATION ] ")
        .border_style(Style::default().fg(pal.secondary));

    let header_cells = ["PARAMETER", "VALUE", "STATUS"]
        .iter()
        .map(|h| Cell::from(*h).style(Style::default().fg(pal.primary)));
    let header = Row::new(header_cells).height(1).bottom_margin(1);

    let rows = build_table_rows(app, pal);

    let table = Table::new(
        rows,
        [
            Constraint::Percentage(50),
            Constraint::Percentage(30),
            Constraint::Percentage(20),
        ],
    )
    .header(header)
    .block(block)
    .column_spacing(1);

    f.render_widget(table, area);
}

fn build_table_rows(app: &ConfigApp, pal: &Palette) -> Vec<Row<'static>> {
    let preset = helpers::detect_preset(app);
    let preset_color = match preset {
        "STRICT" => Color::Green,
        "STANDARD" => Color::Yellow,
        "RELAXED" => Color::Red,
        _ => pal.text,
    };

    let active_col = Color::Green;

    let items = vec![
        ("Global Preset", preset.to_string(), preset_color, "ACTIVE"),
        ("Max File Tokens", app.rules.max_file_tokens.to_string(), pal.text, "ACTIVE"),
        ("Cyclo. Complexity", app.rules.max_cyclomatic_complexity.to_string(), pal.text, "ACTIVE"),
        ("Nesting Depth", app.rules.max_nesting_depth.to_string(), pal.text, "ACTIVE"),
        ("Func. Arguments", app.rules.max_function_args.to_string(), pal.text, "ACTIVE"),
        ("Func. Words", app.rules.max_function_words.to_string(), pal.text, "ACTIVE"),
        ("Auto-Copy Ctx", bool_str(app.preferences.auto_copy), bool_col(app.preferences.auto_copy), "READY"),
        ("Auto-Format", bool_str(app.preferences.auto_format), bool_col(app.preferences.auto_format), "READY"),
        ("Auto-Commit", bool_str(app.preferences.auto_commit), bool_col(app.preferences.auto_commit), "STANDBY"),
        ("Commit Prefix", format!("\"{}\"", app.preferences.commit_prefix), pal.text, "SET"),
        ("UI Theme", format!("{:?}", app.preferences.theme).to_uppercase(), pal.primary, "LOADED"),
        ("Progress Bars", bool_str(app.preferences.progress_bars), pal.text, "OKAY"),
    ];

    items
        .into_iter()
        .enumerate()
        .map(|(i, (label, value, color, status))| {
            let is_selected = i == app.selected_field;
            let style = if is_selected {
                Style::default()
                    .bg(pal.highlight)
                    .fg(Color::Black)
                    .add_modifier(Modifier::BOLD)
            } else {
                Style::default().fg(color)
            };

            Row::new(vec![
                Cell::from(format!("[#] {label}")),
                Cell::from(value),
                Cell::from(status).style(Style::default().fg(active_col)),
            ])
            .style(style)
        })
        .collect()
}

fn bool_str(b: bool) -> String {
    if b {
        "ON".to_string()
    } else {
        "OFF".to_string()
    }
}
fn bool_col(b: bool) -> Color {
    if b {
        Color::Green
    } else {
        Color::DarkGray
    }
}

#[allow(clippy::cast_precision_loss)]
pub fn draw_context_panel(f: &mut Frame, app: &ConfigApp, area: Rect, pal: &Palette) {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" [ INTEL DISPLAY ] ")
        .border_style(Style::default().fg(pal.primary));
    let inner = block.inner(area);
    f.render_widget(block, area);

    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Length(2),
            Constraint::Length(8),
            Constraint::Min(8),
        ])
        .split(inner);

    f.render_widget(
        Paragraph::new(format!(
            "> {}",
            helpers::get_active_label(app.selected_field)
        ))
        .style(
            Style::default()
                .fg(pal.primary)
                .add_modifier(Modifier::BOLD),
        ),
        chunks[0],
    );

    f.render_widget(
        Paragraph::new(helpers::get_active_description(app.selected_field))
            .wrap(ratatui::widgets::Wrap { trim: true })
            .style(Style::default().fg(pal.text)),
        chunks[1],
    );

    let ratio = helpers::get_integrity_score(app);
    let (color, label) = if ratio > 0.8 {
        (Color::Green, "OPTIMAL")
    } else if ratio > 0.5 {
        (Color::Yellow, "MODERATE")
    } else {
        (Color::Red, "CRITICAL")
    };

    let inner_chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Length(2),
            Constraint::Length(3),
            Constraint::Min(1),
        ])
        .split(chunks[2]);

    f.render_widget(
        Paragraph::new("THREAT LEVEL ANALYTICS\nSTATUS: ACTIVE / SCANNING: ON")
            .alignment(Alignment::Center)
            .style(Style::default().fg(pal.secondary)),
        inner_chunks[0],
    );

    let gauge = Gauge::default()
        .block(
            Block::default()
                .borders(Borders::ALL)
                .border_style(Style::default().fg(pal.secondary)),
        )
        .gauge_style(Style::default().fg(color))
        .use_unicode(true)
        .ratio(ratio)
        .label(Span::styled(
            format!("INTEGRITY: {:.1}% [{label}]", ratio * 100.0),
            Style::default().fg(Color::Black).add_modifier(Modifier::BOLD),
        ));

    f.render_widget(gauge, inner_chunks[1]);

    let decoration = Paragraph::new(
        "\n[LOG] 2025.11.24 ORBITAL_ADJUSTMENT_COMPLETE\n[LOG] SECURITY_PATCH: LVL 5 ACTIVE\n[LOG] WARDEN PROTOCOL ENGAGED"
    ).style(Style::default().fg(Color::DarkGray));
    f.render_widget(decoration, inner_chunks[2]);
}

pub fn draw_footer(f: &mut Frame, area: Rect, pal: &Palette) {
    let text = " [â†‘/â†“] NAVIGATE | [â†/â†’] ADJUST VALUE | [ENTER] SAVE CONFIG | [Q] DISENGAGE ";
    f.render_widget(
        Paragraph::new(text)
            .style(Style::default().fg(pal.bg).bg(pal.secondary))
            .alignment(Alignment::Center),
        area,
    );
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/config/helpers.rs âˆ‡âˆ‡âˆ‡
// src/tui/config/helpers.rs
use super::state::ConfigApp;
use crate::config::Theme;

pub fn adjust_rule(app: &mut ConfigApp, increase: bool) {
    match app.selected_field {
        1 => adjust_int(&mut app.rules.max_file_tokens, 100, 100, increase),
        2 => adjust_int(&mut app.rules.max_cyclomatic_complexity, 1, 1, increase),
        3 => adjust_int(&mut app.rules.max_nesting_depth, 1, 1, increase),
        4 => adjust_int(&mut app.rules.max_function_args, 1, 1, increase),
        5 => adjust_int(&mut app.rules.max_function_words, 1, 1, increase),
        _ => {}
    }
}

pub fn adjust_pref(app: &mut ConfigApp, increase: bool) {
    match app.selected_field {
        6 => app.preferences.auto_copy = !app.preferences.auto_copy,
        7 => app.preferences.auto_format = !app.preferences.auto_format,
        8 => app.preferences.auto_commit = !app.preferences.auto_commit,
        9 => cycle_prefix(app),
        10 => cycle_theme(app, increase),
        11 => app.preferences.progress_bars = !app.preferences.progress_bars,
        _ => {}
    }
}

fn adjust_int(val: &mut usize, step: usize, min: usize, increase: bool) {
    if increase {
        *val = val.saturating_add(step);
    } else {
        *val = val.saturating_sub(step).max(min);
    }
}

fn cycle_theme(app: &mut ConfigApp, forward: bool) {
    let themes = [Theme::Cyberpunk, Theme::Nasa, Theme::Corporate];
    let current = themes
        .iter()
        .position(|t| *t == app.preferences.theme)
        .unwrap_or(0);
    let next = if forward {
        (current + 1) % 3
    } else {
        (current + 2) % 3
    };
    app.preferences.theme = themes[next];
}

fn cycle_prefix(app: &mut ConfigApp) {
    let prefixes = ["AI: ", "feat: ", "fix: ", "warden: "];
    let current = prefixes
        .iter()
        .position(|p| *p == app.preferences.commit_prefix)
        .unwrap_or(0);
    let next = (current + 1) % prefixes.len();
    app.preferences.commit_prefix = prefixes[next].to_string();
}

pub fn cycle_preset(app: &mut ConfigApp, forward: bool) {
    let current = if app.rules.max_file_tokens <= 1500 {
        0 // Strict
    } else if app.rules.max_file_tokens <= 2000 {
        1 // Standard
    } else {
        2 // Relaxed
    };

    let next = if forward {
        (current + 1) % 3
    } else {
        (current + 2) % 3
    };

    match next {
        0 => apply_preset(app, 1500, 4, 2),  // Strict
        1 => apply_preset(app, 2000, 8, 3),  // Standard
        2 => apply_preset(app, 3000, 12, 4), // Relaxed
        _ => {}
    }
}

fn apply_preset(app: &mut ConfigApp, tokens: usize, complexity: usize, depth: usize) {
    app.rules.max_file_tokens = tokens;
    app.rules.max_cyclomatic_complexity = complexity;
    app.rules.max_nesting_depth = depth;
}

#[must_use]
pub fn get_active_label(field: usize) -> &'static str {
    match field {
        0 => "GLOBAL PROTOCOL",
        1 => "LAW OF ATOMICITY",
        2..=4 => "LAW OF COMPLEXITY",
        5 => "LAW OF BLUNTNESS",
        6..=9 => "WORKFLOW AUTOMATION",
        10..=11 => "VISUALS & FEEDBACK",
        _ => "UNKNOWN",
    }
}

const DESCRIPTIONS: &[&str] = &[
    "Select a predefined security clearance level.\n\nStrict: Greenfield/Critical systems.\nStandard: Recommended balance.\nRelaxed: Legacy containment.",
    "Limits file size. Large files confuse AI context windows and make verification impossible. \n\nGoal: Modular, atomic units.",
    "Limits control flow paths. High complexity increases hallucination rates and makes code untestable.\n\nGoal: Linear, obvious logic.",
    "Limits indentation. Deep nesting causes AI to lose scope tracking and context.\n\nGoal: Shallow, flat structures.",
    "Limits function inputs. Too many arguments suggests a missing struct or mixed concerns.\n\nGoal: Clean interfaces.",
    "Limits function naming verbosity. Long names often mask poor abstraction.\n\nGoal: Concise intent.",
    "Automatically copy the generated 'context.txt' to the clipboard.\n\nGoal: Eliminate manual steps.",
    "Run the project's formatter (e.g., cargo fmt, prettier) immediately after applying changes.\n\nGoal: Maintain style guide.",
    "Automatically stage and commit changes if the application succeeds and 'warden check' passes.\n\nGoal: High-velocity iteration.",
    "Prefix for auto-generated commits to distinguish them in git history.\n\nGoal: Traceability.",
    "Color scheme for the TUI.\nNASA: High Contrast.\nCyberpunk: Neon.\nCorporate: Subtle.\n\nGoal: Eye Candy.",
    "Show animated progress bars during scans and operations.\n\nGoal: Feedback.",
];

#[must_use]
pub fn get_active_description(field: usize) -> &'static str {
    if field < DESCRIPTIONS.len() {
        DESCRIPTIONS[field]
    } else {
        ""
    }
}

#[must_use]
pub fn detect_preset(app: &ConfigApp) -> &'static str {
    if app.rules.max_file_tokens <= 1500 && app.rules.max_cyclomatic_complexity <= 4 {
        "STRICT"
    } else if app.rules.max_file_tokens >= 3000 {
        "RELAXED"
    } else if app.rules.max_file_tokens == 2000 {
        "STANDARD"
    } else {
        "CUSTOM"
    }
}

#[must_use]
#[allow(clippy::cast_precision_loss)]
pub fn get_integrity_score(app: &ConfigApp) -> f64 {
    let t_score = (app.rules.max_file_tokens as f64 - 1000.0) / 3000.0;
    let c_score = (app.rules.max_cyclomatic_complexity as f64 - 1.0) / 15.0;
    let d_score = (app.rules.max_nesting_depth as f64 - 1.0) / 5.0;
    let raw_avg = (t_score + c_score + d_score) / 3.0;
    (1.0 - raw_avg).clamp(0.0, 1.0)
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/config/mod.rs âˆ‡âˆ‡âˆ‡
pub mod components;
pub mod helpers;
pub mod state;
pub mod view;
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/config/state.rs âˆ‡âˆ‡âˆ‡
// src/tui/config/state.rs
use super::helpers;
use super::view;
use crate::config::{save_to_file, Config, Preferences, RuleConfig};
use anyhow::Result;
use crossterm::event::{self, Event, KeyCode};
use std::collections::HashMap;
use std::time::Duration;

pub struct ConfigApp {
    pub rules: RuleConfig,
    pub preferences: Preferences,
    pub commands: HashMap<String, Vec<String>>,
    // 0=Preset, 1-5=Rules, 6-9=Workflow, 10=Theme, 11=Progress
    pub selected_field: usize,
    pub running: bool,
    pub modified: bool,
    pub saved_message: Option<(String, std::time::Instant)>,
}

impl Default for ConfigApp {
    fn default() -> Self {
        Self::new()
    }
}

impl ConfigApp {
    #[must_use]
    pub fn new() -> Self {
        let mut config = Config::new();
        config.load_local_config();

        Self {
            rules: config.rules,
            preferences: config.preferences,
            commands: config.commands,
            selected_field: 0,
            running: true,
            modified: false,
            saved_message: None,
        }
    }

    /// Runs the config TUI loop.
    ///
    /// # Errors
    /// Returns error if terminal IO or event polling fails.
    pub fn run<B: ratatui::backend::Backend>(
        &mut self,
        terminal: &mut ratatui::Terminal<B>,
    ) -> Result<()> {
        while self.running {
            terminal.draw(|f| view::draw(f, self))?;
            self.process_event()?;
        }
        Ok(())
    }

    fn process_event(&mut self) -> Result<()> {
        if event::poll(Duration::from_millis(100))? {
            if let Event::Key(key) = event::read()? {
                self.handle_input(key.code);
            }
        }
        self.check_message_expiry();
        Ok(())
    }

    fn check_message_expiry(&mut self) {
        if let Some((_, time)) = self.saved_message {
            if time.elapsed() > Duration::from_secs(2) {
                self.saved_message = None;
            }
        }
    }

    fn handle_input(&mut self, code: KeyCode) {
        match code {
            KeyCode::Char('q') | KeyCode::Esc => self.running = false,
            KeyCode::Up | KeyCode::Char('k') => self.move_cursor(-1),
            KeyCode::Down | KeyCode::Char('j') => self.move_cursor(1),
            KeyCode::Left | KeyCode::Char('h') => self.adjust_value(false),
            KeyCode::Right | KeyCode::Char('l') => self.adjust_value(true),
            KeyCode::Enter | KeyCode::Char('s') => self.save(),
            _ => {}
        }
    }

    #[allow(
        clippy::cast_possible_truncation,
        clippy::cast_possible_wrap,
        clippy::cast_sign_loss
    )]
    fn move_cursor(&mut self, delta: i32) {
        let new_pos = (self.selected_field as i32) + delta;
        if new_pos < 0 {
            self.selected_field = 11;
        } else if new_pos > 11 {
            self.selected_field = 0;
        } else {
            self.selected_field = new_pos as usize;
        }
    }

    fn adjust_value(&mut self, increase: bool) {
        self.modified = true;
        match self.selected_field {
            0 => helpers::cycle_preset(self, increase),
            1..=5 => helpers::adjust_rule(self, increase),
            6..=11 => helpers::adjust_pref(self, increase),
            _ => {}
        }
    }

    fn save(&mut self) {
        if let Err(e) = save_to_file(&self.rules, &self.preferences, &self.commands) {
            self.saved_message = Some((format!("Error: {e}"), std::time::Instant::now()));
        } else {
            self.saved_message =
                Some(("Saved warden.toml!".to_string(), std::time::Instant::now()));
            self.modified = false;
        }
    }
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/config/view.rs âˆ‡âˆ‡âˆ‡
// src/tui/config/view.rs
use super::components;
use super::state::ConfigApp;
use crate::config::Theme;
use ratatui::layout::{Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Style};
use ratatui::widgets::Block;
use ratatui::Frame;

pub struct Palette {
    pub primary: Color,
    pub secondary: Color,
    pub text: Color,
    pub bg: Color,
    pub highlight: Color,
}

fn get_palette(theme: Theme) -> Palette {
    match theme {
        Theme::Nasa => Palette {
            primary: Color::Cyan,
            secondary: Color::Blue,
            text: Color::White,
            bg: Color::Black,
            highlight: Color::Cyan,
        },
        Theme::Cyberpunk => Palette {
            primary: Color::Magenta,
            secondary: Color::Cyan,
            text: Color::Green,
            bg: Color::Black,
            highlight: Color::Magenta,
        },
        Theme::Corporate => Palette {
            primary: Color::White,
            secondary: Color::Gray,
            text: Color::Gray,
            bg: Color::Black,
            highlight: Color::White,
        },
    }
}

pub fn draw(f: &mut Frame, app: &ConfigApp) {
    let pal = get_palette(app.preferences.theme);
    let area = f.area();

    let block = Block::default().style(Style::default().bg(pal.bg));
    f.render_widget(block, area);

    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Length(3),
            Constraint::Min(5),
            Constraint::Length(1),
        ])
        .split(area);

    components::draw_header(f, app, chunks[0], &pal);
    draw_main(f, app, chunks[1], &pal);
    components::draw_footer(f, chunks[2], &pal);
}

fn draw_main(f: &mut Frame, app: &ConfigApp, area: Rect, pal: &Palette) {
    let layout = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(55), Constraint::Percentage(45)])
        .split(area);

    components::draw_settings_table(f, app, layout[0], pal);
    components::draw_context_panel(f, app, layout[1], pal);
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/mod.rs âˆ‡âˆ‡âˆ‡
// src/tui/mod.rs
pub mod config;
pub mod state;
pub mod view;

use anyhow::Result;
use config::state::ConfigApp;
use crossterm::{
    event::{DisableMouseCapture, EnableMouseCapture},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use ratatui::backend::CrosstermBackend;
use ratatui::Terminal;
use std::io;

/// Launches the Config Editor TUI
/// # Errors
/// Returns error if terminal IO fails
pub fn run_config() -> Result<()> {
    enable_raw_mode()?;
    let mut stdout = io::stdout();
    execute!(stdout, EnterAlternateScreen, EnableMouseCapture)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    let mut app = ConfigApp::new();
    let res = app.run(&mut terminal);

    disable_raw_mode()?;
    execute!(
        terminal.backend_mut(),
        LeaveAlternateScreen,
        DisableMouseCapture
    )?;
    terminal.show_cursor()?;

    res
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/state.rs âˆ‡âˆ‡âˆ‡
// src/tui/state.rs
use crate::types::{FileReport, ScanReport};
use anyhow::Result;
use crossterm::event::{self, Event, KeyCode};
use std::time::Duration;

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum SortMode {
    Path,
    Tokens,
    Violations,
}

pub struct App {
    pub report: ScanReport,
    pub view_indices: Vec<usize>,
    pub selected_index: usize,
    pub running: bool,
    pub sort_mode: SortMode,
    pub only_violations: bool,
}

impl App {
    #[must_use]
    pub fn new(report: ScanReport) -> Self {
        let mut app = Self {
            report,
            view_indices: Vec::new(),
            selected_index: 0,
            running: true,
            sort_mode: SortMode::Path,
            only_violations: false,
        };
        app.update_view();
        app
    }

    fn update_view(&mut self) {
        let mut indices: Vec<usize> = self
            .report
            .files
            .iter()
            .enumerate()
            .filter(|(_, f)| !self.only_violations || !f.is_clean())
            .map(|(i, _)| i)
            .collect();

        self.sort_indices(&mut indices);
        self.view_indices = indices;
        self.clamp_selection();
    }

    fn sort_indices(&self, indices: &mut [usize]) {
        let files = &self.report.files;
        indices.sort_by(|&a, &b| {
            let f1 = &files[a];
            let f2 = &files[b];
            match self.sort_mode {
                SortMode::Path => f1.path.cmp(&f2.path),
                SortMode::Tokens => f2.token_count.cmp(&f1.token_count),
                SortMode::Violations => f2.violations.len().cmp(&f1.violations.len()),
            }
        });
    }

    fn clamp_selection(&mut self) {
        if self.view_indices.is_empty() {
            self.selected_index = 0;
        } else if self.selected_index >= self.view_indices.len() {
            self.selected_index = self.view_indices.len() - 1;
        }
    }

    /// Runs TUI loop.
    /// # Errors
    /// Returns error on IO failure.
    pub fn run<B: ratatui::backend::Backend>(
        &mut self,
        terminal: &mut ratatui::Terminal<B>,
    ) -> Result<()> {
        while self.running {
            terminal.draw(|f| crate::tui::view::draw(f, self))?;
            self.process_event()?;
        }
        Ok(())
    }

    fn process_event(&mut self) -> Result<()> {
        if event::poll(Duration::from_millis(100))? {
            if let Event::Key(key) = event::read()? {
                self.handle_input(key.code);
            }
        }
        Ok(())
    }

    fn handle_input(&mut self, code: KeyCode) {
        if self.handle_nav(code) {
            return;
        }
        if self.handle_quit(code) {
            return;
        }
        self.handle_toggles(code);
    }

    fn handle_nav(&mut self, code: KeyCode) -> bool {
        match code {
            KeyCode::Up | KeyCode::Char('k') => {
                self.move_up();
                true
            }
            KeyCode::Down | KeyCode::Char('j') => {
                self.move_down();
                true
            }
            _ => false,
        }
    }

    fn handle_quit(&mut self, code: KeyCode) -> bool {
        if matches!(code, KeyCode::Char('q') | KeyCode::Esc) {
            self.running = false;
            return true;
        }
        false
    }

    fn handle_toggles(&mut self, code: KeyCode) {
        match code {
            KeyCode::Char('s') => self.cycle_sort(),
            KeyCode::Char('f') => self.toggle_filter(),
            _ => {}
        }
    }

    fn move_up(&mut self) {
        if self.selected_index > 0 {
            self.selected_index -= 1;
        }
    }

    fn move_down(&mut self) {
        if !self.view_indices.is_empty() && self.selected_index < self.view_indices.len() - 1 {
            self.selected_index += 1;
        }
    }

    fn cycle_sort(&mut self) {
        self.sort_mode = match self.sort_mode {
            SortMode::Path => SortMode::Tokens,
            SortMode::Tokens => SortMode::Violations,
            SortMode::Violations => SortMode::Path,
        };
        self.update_view();
    }

    fn toggle_filter(&mut self) {
        self.only_violations = !self.only_violations;
        self.update_view();
    }

    #[must_use]
    pub fn get_selected_file(&self) -> Option<&FileReport> {
        if let Some(&real_index) = self.view_indices.get(self.selected_index) {
            self.report.files.get(real_index)
        } else {
            None
        }
    }
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/view/components.rs âˆ‡âˆ‡âˆ‡
// src/tui/view/components.rs
use crate::tui::state::App;
use crate::types::FileReport;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Gauge, List, ListItem, Paragraph};
use ratatui::Frame;

pub fn draw_file_list(f: &mut Frame, app: &App, area: Rect) {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" ğŸ“‚ File List ");
    let items = build_list_items(app);

    let list = List::new(items).block(block).highlight_style(
        Style::default()
            .bg(Color::DarkGray)
            .add_modifier(Modifier::BOLD),
    );

    let mut state = ratatui::widgets::ListState::default();
    state.select(Some(app.selected_index));
    f.render_stateful_widget(list, area, &mut state);
}

fn build_list_items(app: &App) -> Vec<ListItem<'_>> {
    app.view_indices
        .iter()
        .map(|&idx| {
            let file = &app.report.files[idx];
            create_list_item(file)
        })
        .collect()
}

fn create_list_item(file: &FileReport) -> ListItem<'_> {
    let name = file.path.to_string_lossy();
    let is_clean = file.is_clean();
    let (color, icon) = if !is_clean {
        (Color::Red, "!")
    } else if file.token_count > 1000 {
        (Color::Yellow, "âœ“")
    } else {
        (Color::Green, "âœ“")
    };

    let bars = (file.token_count / 200).clamp(0, 10);
    let bar_vis = "I".repeat(bars);

    let content = Line::from(vec![
        Span::styled(
            format!("{icon} "),
            Style::default().fg(color).add_modifier(Modifier::BOLD),
        ),
        Span::raw(format!("{name:<30} ")),
        Span::styled(
            format!("{bar_vis:<10}"),
            Style::default().fg(Color::DarkGray),
        ),
    ]);
    ListItem::new(content)
}

#[allow(clippy::cast_precision_loss)]
pub fn draw_inspector(f: &mut Frame, app: &App, area: Rect) {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" ğŸ•µï¸ Inspector ");
    let inner = block.inner(area);
    f.render_widget(block, area);

    if let Some(file) = app.get_selected_file() {
        let layout = Layout::default()
            .direction(Direction::Vertical)
            .constraints(
                [
                    Constraint::Length(2),
                    Constraint::Length(6),
                    Constraint::Min(5),
                ]
                .as_ref(),
            )
            .split(inner);

        draw_header(f, file, layout[0]);
        draw_stats(f, file, layout[1]);
        draw_issues(f, file, layout[2]);
    } else {
        f.render_widget(
            Paragraph::new("No file selected").alignment(Alignment::Center),
            inner,
        );
    }
}

fn draw_header(f: &mut Frame, file: &FileReport, area: Rect) {
    let header = Paragraph::new(Line::from(vec![
        Span::styled("TARGET: ", Style::default().fg(Color::DarkGray)),
        Span::styled(
            file.path.to_string_lossy(),
            Style::default().add_modifier(Modifier::BOLD),
        ),
    ]));
    f.render_widget(header, area);
}

#[allow(clippy::cast_precision_loss)]
fn draw_stats(f: &mut Frame, file: &FileReport, area: Rect) {
    let chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(50), Constraint::Percentage(50)].as_ref())
        .split(area);

    let t_ratio = (file.token_count as f64 / 2000.0).clamp(0.0, 1.0);
    let t_gauge = Gauge::default()
        .block(Block::default().borders(Borders::NONE).title("Size"))
        .gauge_style(Style::default().fg(if t_ratio > 0.8 {
            Color::Red
        } else {
            Color::Green
        }))
        .ratio(t_ratio)
        .label(format!("{} toks", file.token_count));
    f.render_widget(t_gauge, chunks[0]);

    let v_count = file.violations.len();
    let v_ratio = (v_count as f64 / 5.0).clamp(0.0, 1.0);
    let v_gauge = Gauge::default()
        .block(Block::default().borders(Borders::NONE).title("Issues"))
        .gauge_style(Style::default().fg(if v_count > 0 {
            Color::Red
        } else {
            Color::Green
        }))
        .ratio(v_ratio)
        .label(format!("{v_count} Found"));
    f.render_widget(v_gauge, chunks[1]);
}

fn draw_issues(f: &mut Frame, file: &FileReport, area: Rect) {
    if file.is_clean() {
        let p = Paragraph::new("âœ¨ Clean.")
            .style(Style::default().fg(Color::Green))
            .alignment(Alignment::Center);
        f.render_widget(p, area);
        return;
    }

    let items: Vec<ListItem> = file
        .violations
        .iter()
        .map(|v| {
            let header = Line::from(vec![
                Span::styled(
                    format!("[{}] ", v.law),
                    Style::default().fg(Color::Red).add_modifier(Modifier::BOLD),
                ),
                Span::raw(format!("Line {}", v.row + 1)),
            ]);
            let msg = Line::from(Span::styled(
                format!("  â””â”€ {}", v.message),
                Style::default().fg(Color::White),
            ));
            ListItem::new(vec![header, msg, Line::from("")])
        })
        .collect();

    let list = List::new(items).block(Block::default().borders(Borders::TOP).title(" Violations "));
    f.render_widget(list, area);
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/view/layout.rs âˆ‡âˆ‡âˆ‡
// src/tui/view/layout.rs
use crate::tui::state::{App, SortMode};
use crate::tui::view::components;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Paragraph};
use ratatui::Frame;

pub fn render_dashboard(f: &mut Frame, app: &App, area: Rect) {
    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints(
            [
                Constraint::Length(3),
                Constraint::Min(10),
                Constraint::Length(1),
            ]
            .as_ref(),
        )
        .split(area);

    draw_header(f, app, chunks[0]);
    draw_main(f, app, chunks[1]);
    draw_footer(f, chunks[2]);
}

#[allow(clippy::cast_precision_loss)]
fn draw_header(f: &mut Frame, app: &App, area: Rect) {
    let (clean_count, total) = count_stats(app);
    let health = if total > 0 {
        (clean_count as f64 / total as f64) * 100.0
    } else {
        100.0
    };

    let info = build_info_string(app, total);
    let line = build_header_line(health, &info);

    f.render_widget(
        Paragraph::new(line)
            .block(Block::default().borders(Borders::ALL))
            .alignment(Alignment::Center),
        area,
    );
}

fn count_stats(app: &App) -> (usize, usize) {
    (
        app.report.files.iter().filter(|f| f.is_clean()).count(),
        app.report.files.len(),
    )
}

fn get_health_color(health: f64) -> Color {
    if health > 90.0 {
        return Color::Green;
    }
    if health > 70.0 {
        return Color::Yellow;
    }
    Color::Red
}

fn build_info_string(app: &App, total: usize) -> String {
    let sort_str = get_sort_label(app.sort_mode);
    let filter_str = get_filter_label(app.only_violations);
    format!(" FILES: {total} | SORT: {sort_str}{filter_str} ")
}

fn get_sort_label(mode: SortMode) -> &'static str {
    match mode {
        SortMode::Path => "NAME",
        SortMode::Tokens => "SIZE",
        SortMode::Violations => "ERRORS",
    }
}

fn get_filter_label(active: bool) -> &'static str {
    if active {
        " | FILTER: ERRORS"
    } else {
        ""
    }
}

fn build_header_line(health: f64, info: &str) -> Line<'_> {
    Line::from(vec![
        Span::styled(
            " ğŸ›¡ï¸ WARDEN PROTOCOL ",
            Style::default()
                .fg(Color::Cyan)
                .add_modifier(Modifier::BOLD),
        ),
        Span::raw(" | "),
        Span::styled(
            format!("HEALTH: {health:.1}%"),
            Style::default().fg(get_health_color(health)),
        ),
        Span::raw(" |"),
        Span::raw(info),
    ])
}

fn draw_main(f: &mut Frame, app: &App, area: Rect) {
    let chunks = get_main_chunks(area);
    components::draw_file_list(f, app, chunks[0]);
    components::draw_inspector(f, app, chunks[1]);
}

fn get_main_chunks(area: Rect) -> std::rc::Rc<[Rect]> {
    Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(40), Constraint::Percentage(60)].as_ref())
        .split(area)
}

fn draw_footer(f: &mut Frame, area: Rect) {
    let text = " [s] Sort Mode | [f] Filter Errors | [j/k] Navigate | [q] Quit ";
    f.render_widget(
        Paragraph::new(text).style(Style::default().fg(Color::DarkGray).bg(Color::Black)),
        area,
    );
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/tui/view/mod.rs âˆ‡âˆ‡âˆ‡
// src/tui/view/mod.rs
pub mod components;
pub mod layout;

use crate::tui::state::App;
use ratatui::Frame;

pub fn draw(f: &mut Frame, app: &App) {
    let area = f.area();
    layout::render_dashboard(f, app, area);
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/types.rs âˆ‡âˆ‡âˆ‡
// src/types.rs
use std::path::PathBuf;

/// A single violation detected during analysis.
#[derive(Debug, Clone)]
pub struct Violation {
    pub row: usize,
    pub message: String,
    pub law: &'static str,
}

/// Analysis results for a single file.
#[derive(Debug, Clone)]
pub struct FileReport {
    pub path: PathBuf,
    pub token_count: usize,
    pub complexity_score: usize,
    pub violations: Vec<Violation>,
}

impl FileReport {
    /// Returns true if no violations were found.
    #[must_use]
    pub fn is_clean(&self) -> bool {
        self.violations.is_empty()
    }

    /// Returns the number of violations.
    #[must_use]
    pub fn violation_count(&self) -> usize {
        self.violations.len()
    }
}

/// Aggregated results from scanning multiple files.
#[derive(Debug, Clone, Default)]
pub struct ScanReport {
    pub files: Vec<FileReport>,
    pub total_tokens: usize,
    pub total_violations: usize,
    pub duration_ms: u128,
}

impl ScanReport {
    /// Returns true if any violations were found.
    #[must_use]
    pub fn has_errors(&self) -> bool {
        self.total_violations > 0
    }

    /// Returns the number of clean files.
    #[must_use]
    pub fn clean_file_count(&self) -> usize {
        self.files.iter().filter(|f| f.is_clean()).count()
    }
}

âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ src/wizard.rs âˆ‡âˆ‡âˆ‡
// src/wizard.rs
use crate::project::{self, ProjectType, Strictness};
use anyhow::Result;
use colored::Colorize;
use std::io::{self, Write};
use std::path::Path;

/// Runs the interactive configuration wizard.
///
/// # Errors
/// Returns error if IO fails or file writing fails.
pub fn run() -> Result<()> {
    println!("{}", "ğŸ§™ Warden Configuration Wizard".bold().cyan());
    println!("{}", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€".dimmed());

    if Path::new("warden.toml").exists() {
        println!("{}", "âš ï¸  warden.toml already exists.".yellow());
        if !confirm("Overwrite it?")? {
            println!("Operation cancelled.");
            return Ok(());
        }
    }

    let project_type = prompt_project_type()?;
    let strictness = prompt_strictness()?;

    println!();
    println!("Generating configuration for:");
    println!("  Type:       {}", format!("{project_type:?}").green());
    println!("  Strictness: {}", format!("{strictness:?}").green());

    let content = project::generate_toml(project_type, strictness);
    std::fs::write("warden.toml", content)?;

    println!();
    println!("{}", "âœ… Configuration created successfully!".green().bold());
    println!("Run {} to analyze your project.", "warden".yellow());

    Ok(())
}

fn prompt_project_type() -> Result<ProjectType> {
    let detected = ProjectType::detect();
    println!();
    println!("Detected Project Type: {}", format!("{detected:?}").cyan());
    
    if confirm("Is this correct?")? {
        return Ok(detected);
    }

    println!();
    println!("Select Project Type:");
    println!("1. Rust");
    println!("2. Node/TypeScript");
    println!("3. Python");
    println!("4. Go");
    
    loop {
        print!("Enter selection [1-4]: ");
        io::stdout().flush()?;
        
        let input = read_line()?;
        match input.trim() {
            "1" => return Ok(ProjectType::Rust),
            "2" => return Ok(ProjectType::Node),
            "3" => return Ok(ProjectType::Python),
            "4" => return Ok(ProjectType::Go),
            _ => println!("{}", "Invalid selection.".red()),
        }
    }
}

fn prompt_strictness() -> Result<Strictness> {
    println!();
    println!("Select Strictness Level:");
    println!("{}", "1. Strict   (Greenfield) - 1500 tokens, Low Complexity".green());
    println!("{}", "2. Standard (Recommended)- 2000 tokens, Medium Complexity".cyan());
    println!("{}", "3. Relaxed  (Legacy)     - 3000 tokens, High Complexity".yellow());

    loop {
        print!("Enter selection [1-3] (default: 2): ");
        io::stdout().flush()?;
        
        let input = read_line()?;
        if input.trim().is_empty() {
             return Ok(Strictness::Standard);
        }

        match input.trim() {
            "1" => return Ok(Strictness::Strict),
            "2" => return Ok(Strictness::Standard),
            "3" => return Ok(Strictness::Relaxed),
            _ => println!("{}", "Invalid selection.".red()),
        }
    }
}

fn confirm(prompt: &str) -> Result<bool> {
    print!("{prompt} [y/N] ");
    io::stdout().flush()?;
    let input = read_line()?;
    Ok(input.trim().eq_ignore_ascii_case("y"))
}

fn read_line() -> Result<String> {
    let mut input = String::new();
    io::stdin().read_line(&mut input)?;
    Ok(input)
}
âˆ†âˆ†âˆ†

âˆ‡âˆ‡âˆ‡ warden.toml âˆ‡âˆ‡âˆ‡
# warden.toml
[rules]
max_file_tokens = 2000
max_cyclomatic_complexity = 8
max_nesting_depth = 3
max_function_args = 5
max_function_words = 5
ignore_naming_on =
ignore_tokens_on = ["lock", ".md"]

[commands]
check = [
    "cargo clippy --all-targets -- -D warnings -D clippy::pedantic",
    "cargo test"
]
fix = "cargo fmt"

âˆ†âˆ†âˆ†


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
END CODEBASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WARDEN CONSTRAINTS:
â–¡ Files < 2000 tokens
â–¡ Complexity â‰¤ 8
â–¡ Nesting â‰¤ 3
â–¡ Args â‰¤ 5
â–¡ No .unwrap() or .expect()
â–¡ Use Nabla Format (âˆ‡âˆ‡âˆ‡ ... âˆ†âˆ†âˆ†)
