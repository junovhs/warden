â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›¡ï¸ WARDEN PROTOCOL - AI SYSTEM PROMPT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ›¡ï¸ SYSTEM MANDATE: THE WARDEN PROTOCOL
ROLE: High-Integrity Systems Architect (NASA/JPL Standard).
CONTEXT: You are coding inside a strict environment enforced by Warden, a structural linter based on NASA's "Power of 10" rules.

THE 3 LAWS (Non-Negotiable):

1. LAW OF ATOMICITY (Holzmann Rule 4)
   - Files: MUST be < 2000 tokens (~60-100 logical statements).
   - Rationale: Each file should be a logical unit, verifiable as a unit.
   - Action: Split immediately if larger. Separate VIEW (UI) from LOGIC (business logic).

2. LAW OF COMPLEXITY (Holzmann Rules 1 & 2)
   - Cyclomatic Complexity: MUST be â‰¤ 4 per function.
   - Nesting Depth: MUST be â‰¤ 2 levels.
   - Function Arguments: MUST be â‰¤ 5 parameters.
   - Rationale: Simpler control flow = stronger analysis capabilities.
   - Action: Extract functions. Simplify branching. Use data structures over parameter lists.

3. LAW OF PARANOIA (Holzmann Rules 5, 7, & 10)
   - Error Handling: ALL functions return Result<T, E> or Option<T>. NO silent failures.
   - Banned Patterns: NO .unwrap() calls. NO .expect() in production code.
   - Validation: Check ALL inputs at entry point (Line 1 of function).
   - Rationale: Assertion density correlates with defect interception.

LANGUAGE SPECIFICS:
   - RUST: clippy::pedantic enforced. Use thiserror for errors.
   - TYPESCRIPT: Strict mode + @ts-check. NO 'any' type.
   - PYTHON: Type hints mandatory (def func(x: int) -> str).

OPERATIONAL PROTOCOL:
   1. Read: Understand the full context before generating code.
   2. Generate: Output COMPLETE, WHOLE files with proper headers.
   3. Verify: Ask "Does this violate the 3 Laws?" before submission.
   4. Iterate: If Warden rejects it, refactor and resubmit.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BEGIN CODEBASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

<file path=".gitignore">
# Rust build artifacts
/target/
/docs/
/Cargo.lock

# Logs and temp files
*.log
*.tmp
*.bak

# Generated AI pack and test fixtures
ai-pack/
tests/tmp/
tmp/
*.manifest.json

# Editor/OS files
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp
*.swo

# Git / OS cruft
*~
*.orig

# Node / frontend extras
node_modules/
dist/
build/
coverage/
.env

# Python extras
__pycache__/
.venv/
venv/

# Repomix or AI output
*.xml
*.jsonl

# Ignore everything inside /target/release except binaries we care about
!/target/release/saccade.exe
!/target/release/gauntlet.exe</file>

<file path="Cargo.toml">
[package]
name = "warden"
version = "0.4.0"
edition = "2021"

[lib]
name = "warden_core"
path = "src/lib.rs"

[[bin]]
name = "warden"
path = "src/bin/warden.rs"

[[bin]]
name = "knit"
path = "src/bin/knit.rs"

[dependencies]
anyhow = "1.0"
thiserror = "1.0"
regex = "1.10"
walkdir = "2.5"
clap = { version = "4.5", features = ["derive"] }
ignore = "0.4"
colored = "2.1"
rayon = "1.10"
once_cell = "1.19"
serde = { version = "1.0", features = ["derive"] }
toml = "0.8"

# THE BRAINS
tiktoken-rs = "0.5"

# UI / TUI
ratatui = "0.29"
crossterm = "0.28"

# Structural Parsing
tree-sitter = "0.20"
tree-sitter-rust = "0.20"
tree-sitter-python = "0.20"
tree-sitter-typescript = "0.20"
tree-sitter-javascript = "0.20"
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Spencer Nunamaker

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="README.md">
# ğŸ›¡ï¸ Warden Protocol

**Architecture Governance based on NASA's "Power of 10".**

> *"The rules are like the seat belts in a car: Initially, using them is perhaps a little uncomfortable, but after a while, it becomes second nature, and not using them is unimaginable."* â€” Gerard J. Holzmann

Warden is a structural linter and "Architectural MRI" designed to enforce **High-Integrity Coding Standards**. It is a modern, multi-language adaptation of the **[NASA/JPL "Power of 10" Rules](https://web.eecs.umich.edu/~imarkov/10rules.pdf)** (Holzmann, 2006) for safety-critical software.

While originally designed for C, Warden adapts these principles for Rust, TypeScript, and Python to solve the "Context Drift" and "Complexity Creep" problems common in modern development (and AI-assisted workflows).

**v0.4.0 Update:** Now featuring a TUI Dashboard, Cyclomatic Complexity analysis, and a unified "Super Command" system.

---

## ğŸ“¸ The Architectural MRI (TUI)

Warden includes a hardware-accelerated TUI to visualize codebase health in real-time.

```bash
warden --ui
```

<p align="center">
  <img src="assets/screenshot.png" alt="Warden TUI Dashboard" width="700">
</p>

It features:
*   **Heatmap List:** Instantly spot complex files (Yellow) or violating files (Red).
*   **Mini-Gauges:** Inline visualization of file size vs. token budget.
*   **Compliance Stress:** A metric derived from the density of violations in a file.


---

## âš¡ The 3 Laws (Adapted from Holzmann)

Warden uses **Tree-sitter** to parse the AST and enforce:

### 1. The Law of Atomicity (Holzmann Rule 4)
*   **Rule:** No file may exceed **2000 Tokens** (approx. 60-100 logical statements).
*   **Original Rationale:** *"Each function should be a logical unit... verifiable as a unit."*
*   **Modern Benefit:** Forces modularity and ensures AI context windows remain high-signal.

### 2. The Law of Complexity (Holzmann Rules 1 & 2)
*   **Cyclomatic Complexity:** Max **10**. (Simple control flow).
*   **Nesting Depth:** Max **4**. (No "Arrow Code").
*   **Arity:** Max **5** arguments. (Enforces Data Structures).
*   **Original Rationale:** *"Simpler control flow translates into stronger capabilities for analysis."*

### 3. The Law of Paranoia (Holzmann Rules 5, 7, & 10)
*   **Rule:** Logic bodies must contain explicit error handling (`Result`, `try/catch`).
*   **Rule:** No `unwrap()` allowed. Zero Warnings tolerated.
*   **Original Rationale:** *"The odds of intercepting defects increase significantly with increasing assertion density."*

---

## ğŸš€ The "God Command"

Warden acts as a pipeline runner to enforce **Rule 10** (Pedantic Compilation).

**One command to rule them all:**
```bash
warden check
```

*Configured in `warden.toml`:*
```toml
[commands]
# Runs Clippy Pedantic -> If Pass -> Runs Warden Scan
check = "cargo clippy --all-targets -- -D warnings -D clippy::pedantic"
```

---

## ğŸ“¦ Installation

```bash
cargo install --path . --force
```

## ğŸ› ï¸ Configuration

Run `warden --init` to generate a `warden.toml` with NASA-standard defaults.

```toml
[rules]
max_file_tokens = 2000
max_cyclomatic_complexity = 10
max_nesting_depth = 4
```

## ğŸ¤– The AI System Prompt

To enforce these rules with LLMs/Agents:

```text
ROLE: High-Integrity Systems Architect (Standard: JPL/Holzmann).
CONTEXT: You are coding inside a strict environment enforced by 'Warden'.

THE 3 LAWS (Non-Negotiable):
1. LAW OF ATOMICITY: Files < 200 Lines. Split VIEW from LOGIC.
2. LAW OF COMPLEXITY: Nesting < 4. Args < 5. Complexity < 10.
3. LAW OF PARANOIA: Validate inputs at Line 1. No unwrap(). Return Result<>.

OPERATIONAL PROTOCOL:
1. Scan: Read context.
2. Generate: Output WHOLE FILES.
3. Verify: "Will Warden reject this?"
```

**License:** MIT
</file>

<file path="src/analysis.rs">
// src/analysis.rs
use crate::checks::{self, CheckContext};
use crate::config::RuleConfig;
use crate::types::Violation;
use anyhow::Result;
use tree_sitter::{Language, Parser, Query};

pub struct Analyzer {
    rust_naming: Query,
    rust_safety: Query,
    rust_complexity: Query,
    rust_banned: Query,
    js_naming: Query,
    js_safety: Query,
    js_complexity: Query,
    py_naming: Query,
    py_safety: Query,
    py_complexity: Query,
}

impl Default for Analyzer {
    fn default() -> Self {
        Self::new()
    }
}

impl Analyzer {
    #[must_use]
    pub fn new() -> Self {
        Self {
            rust_naming: q(
                tree_sitter_rust::language(),
                "(function_item name: (identifier) @name)",
            ),
            rust_safety: q(
                tree_sitter_rust::language(),
                r#"
                (match_expression) @safe
                (if_expression condition: (let_condition)) @safe
                (while_expression condition: (let_condition)) @safe
                (try_expression) @safe
                (call_expression function: (field_expression field: (field_identifier) @m (#match? @m "^(expect|unwrap_or|unwrap_or_else|unwrap_or_default|ok|err|map_err|any|all|find|is_some|is_none|is_ok|is_err)$"))) @safe
                (function_item return_type: (_) @ret (#match? @ret "Result")) @safe
            "#,
            ),
            rust_complexity: q(
                tree_sitter_rust::language(),
                r#"
                (if_expression) @branch
                (match_arm) @branch
                (while_expression) @branch
                (for_expression) @branch
                (binary_expression operator: ["&&" "||"]) @branch
            "#,
            ),
            rust_banned: q(
                tree_sitter_rust::language(),
                r#"
                (call_expression function: (field_expression field: (field_identifier) @m (#eq? @m "unwrap"))) @banned
            "#,
            ),
            js_naming: q(
                tree_sitter_typescript::language_typescript(),
                r"
                (function_declaration name: (identifier) @name)
                (method_definition name: (property_identifier) @name)
                (variable_declarator name: (identifier) @name value: [(arrow_function) (function_expression)])
            ",
            ),
            js_safety: q(
                tree_sitter_typescript::language_typescript(),
                r#"
                (try_statement) @safe
                (call_expression function: (member_expression property: (property_identifier) @m (#eq? @m "catch"))) @safe
            "#,
            ),
            js_complexity: q(
                tree_sitter_typescript::language_typescript(),
                r#"
                (if_statement) @branch
                (for_statement) @branch
                (for_in_statement) @branch
                (while_statement) @branch
                (do_statement) @branch
                (switch_case) @branch
                (catch_clause) @branch
                (ternary_expression) @branch
                (binary_expression operator: ["&&" "||" "??"]) @branch
            "#,
            ),
            py_naming: q(
                tree_sitter_python::language(),
                "(function_definition name: (identifier) @name)",
            ),
            py_safety: q(
                tree_sitter_python::language(),
                r#"
                (try_statement) @safe
                (if_statement condition: (unary_operator (_) @op (#eq? @op "not"))) @safe
                (if_statement condition: (comparison_operator (_) (none))) @safe
            "#,
            ),
            py_complexity: q(
                tree_sitter_python::language(),
                r"
                (if_statement) @branch
                (for_statement) @branch
                (while_statement) @branch
                (except_clause) @branch
                (boolean_operator) @branch
            ",
            ),
        }
    }

    #[must_use]
    pub fn analyze(
        &self,
        lang: &str,
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        if let Some(queries) = self.select_language(lang) {
            Self::run_analysis(queries, filename, content, config)
        } else {
            vec![]
        }
    }

    fn select_language(
        &self,
        lang: &str,
    ) -> Option<(Language, &Query, &Query, &Query, Option<&Query>)> {
        match lang {
            "rs" => Some(self.queries_rust()),
            "js" | "jsx" | "ts" | "tsx" => Some(self.queries_js()),
            "py" => Some(self.queries_python()),
            _ => None,
        }
    }

    fn queries_rust(&self) -> (Language, &Query, &Query, &Query, Option<&Query>) {
        (
            tree_sitter_rust::language(),
            &self.rust_naming,
            &self.rust_safety,
            &self.rust_complexity,
            Some(&self.rust_banned),
        )
    }

    fn queries_js(&self) -> (Language, &Query, &Query, &Query, Option<&Query>) {
        (
            tree_sitter_typescript::language_typescript(),
            &self.js_naming,
            &self.js_safety,
            &self.js_complexity,
            None,
        )
    }

    fn queries_python(&self) -> (Language, &Query, &Query, &Query, Option<&Query>) {
        (
            tree_sitter_python::language(),
            &self.py_naming,
            &self.py_safety,
            &self.py_complexity,
            None,
        )
    }

    fn run_analysis(
        (language, naming, safety, complexity, banned): (
            Language,
            &Query,
            &Query,
            &Query,
            Option<&Query>,
        ),
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        let mut violations = Vec::new();

        // Fixed: parser binding does not need to be mutable as it's already a mutable reference inside the Result
        if let Ok(parser) = Parser::new().get_init(language) {
            if let Some(tree) = parser.parse(content, None) {
                let ctx = CheckContext {
                    root: tree.root_node(),
                    source: content,
                    filename,
                    config,
                };

                let _ = checks::check_naming(&ctx, naming, &mut violations);
                let _ = checks::check_safety(&ctx, safety, &mut violations);
                let _ = checks::check_metrics(&ctx, complexity, &mut violations);

                if let Some(bq) = banned {
                    let _ = checks::check_banned(&ctx, bq, &mut violations);
                }
            }
        }

        violations
    }
}

trait ParserInit {
    fn get_init(&mut self, lang: Language) -> Result<&mut Self>;
}

impl ParserInit for Parser {
    fn get_init(&mut self, lang: Language) -> Result<&mut Self> {
        self.set_language(lang)?;
        Ok(self)
    }
}

fn q(lang: Language, pattern: &str) -> Query {
    Query::new(lang, pattern).expect("Invalid Query")
}
</file>

<file path="src/bin/knit.rs">
// src/bin/knit.rs
use anyhow::Result;
use clap::{Parser, ValueEnum};
use colored::Colorize;
use std::fmt::Write;
use std::fs;
use std::path::PathBuf;

use warden_core::config::{Config, GitMode};
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::prompt::PromptGenerator;
use warden_core::tokens::Tokenizer;

#[derive(Debug, Clone, ValueEnum)]
enum OutputFormat {
    Text,
    Xml,
}

#[derive(Parser)]
#[command(name = "knit")]
#[command(about = "Stitches atomic files into a single context file.")]
#[allow(clippy::struct_excessive_bools)]
struct Cli {
    #[arg(long, short)]
    stdout: bool,
    #[arg(long, short)]
    verbose: bool,
    #[arg(long)]
    git_only: bool,
    #[arg(long)]
    no_git: bool,
    #[arg(long)]
    code_only: bool,
    #[arg(long, short)]
    prompt: bool,
    #[arg(long, value_enum, default_value_t = OutputFormat::Text)]
    format: OutputFormat,
}

fn main() -> Result<()> {
    let cli = Cli::parse();
    let config = setup_config(&cli)?;

    if !cli.stdout {
        println!("ğŸ§¶ Knitting repository...");
    }

    let files = discover_files(&config, cli.verbose)?;
    let content = generate_content(&files, &cli, &config)?;
    let token_count = Tokenizer::count(&content);

    output_result(&content, token_count, cli.stdout)
}

fn setup_config(cli: &Cli) -> Result<Config> {
    let mut config = Config::new();
    config.verbose = cli.verbose;
    config.code_only = cli.code_only;
    config.git_mode = if cli.git_only {
        GitMode::Yes
    } else if cli.no_git {
        GitMode::No
    } else {
        GitMode::Auto
    };
    config.load_local_config();
    config.validate()?;
    Ok(config)
}

fn discover_files(config: &Config, verbose: bool) -> Result<Vec<PathBuf>> {
    let raw = FileEnumerator::new(config.clone()).enumerate()?;
    let h_files = HeuristicFilter::new().filter(raw);
    let t_files = FileFilter::new(config)?.filter(h_files);

    if verbose {
        eprintln!("ğŸ“¦ Packing {} files...", t_files.len());
    }
    Ok(t_files)
}

fn generate_content(files: &[PathBuf], cli: &Cli, config: &Config) -> Result<String> {
    let mut ctx = String::with_capacity(100_000);

    if cli.prompt {
        write_header(&mut ctx, config)?;
    }

    match cli.format {
        OutputFormat::Text => pack_text(files, &mut ctx)?,
        OutputFormat::Xml => pack_xml(files, &mut ctx)?,
    }

    if cli.prompt {
        write_footer(&mut ctx, config)?;
    }

    Ok(ctx)
}

fn write_header(ctx: &mut String, config: &Config) -> Result<()> {
    let gen = PromptGenerator::new(config.rules.clone());
    writeln!(ctx, "{}", gen.wrap_header()?)?;
    writeln!(ctx, "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nBEGIN CODEBASE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")?;
    Ok(())
}

fn write_footer(ctx: &mut String, config: &Config) -> Result<()> {
    let gen = PromptGenerator::new(config.rules.clone());
    writeln!(ctx, "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nEND CODEBASE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")?;
    writeln!(ctx, "{}", gen.generate_reminder()?)?;
    Ok(())
}

fn output_result(content: &str, tokens: usize, stdout: bool) -> Result<()> {
    let info = format!(
        "\nğŸ“Š Context Size: {} tokens",
        tokens.to_string().yellow().bold()
    );

    if stdout {
        print!("{content}");
        eprintln!("{info}");
    } else {
        fs::write("context.txt", content)?;
        println!("âœ… Generated 'context.txt'");
        println!("{info}");
    }
    Ok(())
}

fn pack_text(files: &[PathBuf], out: &mut String) -> Result<()> {
    for path in files {
        let p_str = path.to_string_lossy().replace('\\', "/");
        writeln!(out, "<file path=\"{p_str}\">")?;
        match fs::read_to_string(path) {
            Ok(c) => out.push_str(&c),
            Err(e) => writeln!(out, "<ERROR READING FILE: {e}>")?,
        }
        writeln!(out, "</file>\n")?;
    }
    Ok(())
}

fn pack_xml(files: &[PathBuf], out: &mut String) -> Result<()> {
    writeln!(out, "<documents>")?;
    for path in files {
        let p_str = path.to_string_lossy().replace('\\', "/");
        writeln!(out, "  <document path=\"{p_str}\"><![CDATA[")?;
        match fs::read_to_string(path) {
            Ok(c) => out.push_str(&c.replace("]]>", "]]]]><![CDATA[>")),
            Err(e) => writeln!(out, "ERROR READING FILE: {e}")?,
        }
        writeln!(out, "]]></document>")?;
    }
    writeln!(out, "</documents>")?;
    Ok(())
}
</file>

<file path="src/bin/warden.rs">
// src/bin/warden.rs
use anyhow::Result;
use clap::{Parser, Subcommand};
use colored::Colorize;
use crossterm::{
    event::{DisableMouseCapture, EnableMouseCapture},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use std::fs;
use std::io;
use std::process::{self, Command};

use warden_core::clipboard;
use warden_core::config::{Config, GitMode};
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::prompt::PromptGenerator;
use warden_core::reporting;
use warden_core::rules::RuleEngine;
use warden_core::tui::state::App;
use warden_core::types::ScanReport;

#[derive(Subcommand)]
enum Commands {
    Prompt {
        #[arg(long, short)]
        copy: bool,
        #[arg(long, short)]
        short: bool,
    },
    Run {
        name: String,
    },
}

#[derive(Parser)]
#[command(name = "warden")]
#[command(about = "Structural linter for Code With Intent")]
#[allow(clippy::struct_excessive_bools)]
struct Cli {
    #[arg(long, short)]
    verbose: bool,
    #[arg(long)]
    git_only: bool,
    #[arg(long)]
    no_git: bool,
    #[arg(long)]
    code_only: bool,
    #[arg(long)]
    init: bool,
    #[arg(long)]
    ui: bool,

    #[command(subcommand)]
    command: Option<Commands>,

    #[arg(index = 1)]
    legacy_command: Option<String>,
}

fn main() -> Result<()> {
    let cli = Cli::parse();

    if cli.init {
        return init_config();
    }

    let config = load_config(&cli)?;

    if let Some(cmd) = &cli.command {
        return exec_subcommand(cmd, &config);
    }

    if let Some(cmd_name) = &cli.legacy_command {
        run_alias(&config, cmd_name);
    }

    run_scan(&config, cli.ui)
}

fn init_config() -> Result<()> {
    if std::path::Path::new("warden.toml").exists() {
        println!("{}", "âš ï¸ warden.toml already exists.".yellow());
    } else {
        let default_toml = r#"# warden.toml
[rules]
max_file_tokens = 2000
max_cyclomatic_complexity = 10
max_nesting_depth = 4
max_function_args = 5
max_function_words = 3
ignore_naming_on = ["tests", "spec"]

[commands]
check = "cargo clippy --all-targets -- -D warnings -D clippy::pedantic"
"#;
        fs::write("warden.toml", default_toml)?;
        println!("{}", "âœ… Created warden.toml".green());
    }
    Ok(())
}

fn load_config(cli: &Cli) -> Result<Config> {
    let mut config = Config::new();
    config.verbose = cli.verbose;
    config.code_only = cli.code_only;
    config.git_mode = if cli.git_only {
        GitMode::Yes
    } else if cli.no_git {
        GitMode::No
    } else {
        GitMode::Auto
    };
    config.load_local_config();
    config.validate()?;
    Ok(config)
}

fn exec_subcommand(cmd: &Commands, config: &Config) -> Result<()> {
    match cmd {
        Commands::Prompt { copy, short } => show_prompt(config, *copy, *short),
        Commands::Run { name } => {
            run_alias(config, name);
            Ok(())
        }
    }
}

fn show_prompt(config: &Config, copy: bool, short: bool) -> Result<()> {
    let generator = PromptGenerator::new(config.rules.clone());
    let output = if short {
        generator.generate_reminder()?
    } else {
        generator.wrap_header()?
    };

    if copy {
        clipboard::copy_to_clipboard(&output)?;
        println!("{}", "âœ… Copied to clipboard".green());
    } else {
        println!("{output}");
    }
    Ok(())
}

fn run_alias(config: &Config, name: &str) {
    if let Some(cmd_str) = config.commands.get(name) {
        println!("ğŸš€ Running '{}': {}", name.cyan(), cmd_str.yellow());
        execute_command_string(cmd_str);
    } else {
        println!("âš ï¸ Unknown command: '{}'", name.yellow());
        process::exit(1);
    }
}

fn execute_command_string(cmd_str: &str) {
    let mut parts = cmd_str.split_whitespace();
    if let Some(prog) = parts.next() {
        let status = Command::new(prog)
            .args(parts)
            .status()
            .unwrap_or_else(|_| process::exit(1));

        if !status.success() {
            process::exit(status.code().unwrap_or(1));
        }
    }
}

fn run_scan(config: &Config, use_ui: bool) -> Result<()> {
    let files = discover_files(config)?;
    if files.is_empty() {
        println!("No files to scan.");
        return Ok(());
    }

    let report = RuleEngine::new(config.clone()).scan(files);

    if use_ui {
        run_tui_app(report)
    } else {
        reporting::print_report(&report)?;
        if report.total_violations > 0 {
            process::exit(1);
        }
        Ok(())
    }
}

fn discover_files(config: &Config) -> Result<Vec<std::path::PathBuf>> {
    let raw = FileEnumerator::new(config.clone()).enumerate()?;
    let heuristic = HeuristicFilter::new().filter(raw);
    let filtered = FileFilter::new(config)?.filter(heuristic);
    Ok(filtered)
}

fn run_tui_app(report: ScanReport) -> Result<()> {
    enable_raw_mode()?;
    let mut stdout = io::stdout();
    execute!(stdout, EnterAlternateScreen, EnableMouseCapture)?;
    let backend = ratatui::backend::CrosstermBackend::new(stdout);
    let mut terminal = ratatui::Terminal::new(backend)?;

    let mut app = App::new(report);
    let res = app.run(&mut terminal);

    disable_raw_mode()?;
    execute!(
        terminal.backend_mut(),
        LeaveAlternateScreen,
        DisableMouseCapture
    )?;
    terminal.show_cursor()?;

    if let Err(err) = res {
        println!("{err:?}");
    }
    Ok(())
}
</file>

<file path="src/checks.rs">
// src/checks.rs
use crate::config::RuleConfig;
use crate::metrics;
use crate::types::Violation;
use anyhow::Result;
use tree_sitter::{Node, Query, QueryCursor};

pub struct CheckContext<'a> {
    pub root: Node<'a>,
    pub source: &'a str,
    pub filename: &'a str,
    pub config: &'a RuleConfig,
}

/// Checks for naming violations.
/// # Errors
/// Returns `Ok`.
#[allow(clippy::unnecessary_wraps)]
pub fn check_naming(ctx: &CheckContext, query: &Query, out: &mut Vec<Violation>) -> Result<()> {
    let mut cursor = QueryCursor::new();
    for m in cursor.matches(query, ctx.root, ctx.source.as_bytes()) {
        let node = m.captures[0].node;
        let name = node.utf8_text(ctx.source.as_bytes()).unwrap_or("?");

        if is_ignored(ctx.filename, &ctx.config.ignore_naming_on) {
            continue;
        }

        let word_count = count_words(name);
        if word_count > ctx.config.max_function_words {
            out.push(Violation {
                row: node.start_position().row,
                message: format!(
                    "Function '{name}' has {word_count} words (Max: {}). Is it doing too much?",
                    ctx.config.max_function_words
                ),
                law: "LAW OF BLUNTNESS",
            });
        }
    }
    Ok(())
}

fn count_words(name: &str) -> usize {
    if name.contains('_') {
        name.split('_').count()
    } else {
        let caps = name.chars().filter(|c| c.is_uppercase()).count();
        if name.chars().next().is_some_and(char::is_uppercase) {
            caps
        } else {
            caps + 1
        }
    }
}

fn is_ignored(filename: &str, patterns: &[String]) -> bool {
    patterns.iter().any(|p| filename.contains(p))
}

/// Checks for safety violations.
/// # Errors
/// Returns `Ok`.
pub fn check_safety(
    ctx: &CheckContext,
    safety_query: &Query,
    out: &mut Vec<Violation>,
) -> Result<()> {
    traverse_nodes(ctx, |node| {
        process_safety_node(node, ctx, safety_query, out);
        Ok(())
    })
}

fn process_safety_node(node: Node, ctx: &CheckContext, query: &Query, out: &mut Vec<Violation>) {
    let kind = node.kind();
    if !kind.contains("function") && !kind.contains("method") {
        return;
    }

    if is_lifecycle(node, ctx.source) {
        return;
    }

    let rows = node.end_position().row - node.start_position().row;
    if rows <= 5 {
        return;
    }

    let mut qc = QueryCursor::new();
    if qc
        .matches(query, node, ctx.source.as_bytes())
        .next()
        .is_none()
    {
        out.push(Violation {
            row: node.start_position().row,
            message: "Function lacks explicit error handling (Result, match, try/catch).".into(),
            law: "LAW OF PARANOIA",
        });
    }
}

/// Checks for complexity metrics.
/// # Errors
/// Returns `Ok`.
pub fn check_metrics(
    ctx: &CheckContext,
    complexity_query: &Query,
    out: &mut Vec<Violation>,
) -> Result<()> {
    traverse_nodes(ctx, |node| {
        if node.kind().contains("function") || node.kind().contains("method") {
            validate_arity(node, ctx.config.max_function_args, out);
            validate_depth(node, ctx.config.max_nesting_depth, out);
            validate_complexity(
                node,
                ctx.source,
                complexity_query,
                ctx.config.max_cyclomatic_complexity,
                out,
            );
        }
        Ok(())
    })
}

fn validate_arity(node: Node, max: usize, out: &mut Vec<Violation>) {
    let args = metrics::count_arguments(node);
    if args > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!(
                "High Arity: Function takes {args} arguments (Max: {max}). Use a Struct."
            ),
            law: "LAW OF COMPLEXITY",
        });
    }
}

fn validate_depth(node: Node, max: usize, out: &mut Vec<Violation>) {
    let depth = metrics::calculate_max_depth(node);
    if depth > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!("Deep Nesting: Max depth is {depth} (Max: {max}). Extract logic."),
            law: "LAW OF COMPLEXITY",
        });
    }
}

fn validate_complexity(
    node: Node,
    source: &str,
    query: &Query,
    max: usize,
    out: &mut Vec<Violation>,
) {
    let score = metrics::calculate_complexity(node, source, query);
    if score > max {
        out.push(Violation {
            row: node.start_position().row,
            message: format!("High Complexity: Score is {score} (Max: {max}). Hard to test."),
            law: "LAW OF COMPLEXITY",
        });
    }
}

/// Checks for banned constructs.
/// # Errors
/// Returns `Ok`.
#[allow(clippy::unnecessary_wraps)]
pub fn check_banned(
    ctx: &CheckContext,
    banned_query: &Query,
    out: &mut Vec<Violation>,
) -> Result<()> {
    let mut cursor = QueryCursor::new();
    for m in cursor.matches(banned_query, ctx.root, ctx.source.as_bytes()) {
        let node = m.captures[0].node;
        out.push(Violation {
            row: node.start_position().row,
            message: "Explicit 'unwrap()' call detected. Use 'expect', 'unwrap_or', or '?'.".into(),
            law: "LAW OF PARANOIA",
        });
    }
    Ok(())
}

fn is_lifecycle(node: Node, source: &str) -> bool {
    if let Some(name_node) = node.child_by_field_name("name") {
        let name = name_node.utf8_text(source.as_bytes()).unwrap_or("");
        matches!(
            name,
            "new" | "default" | "init" | "__init__" | "constructor" | "render" | "main"
        )
    } else {
        false
    }
}

fn traverse_nodes<F>(ctx: &CheckContext, mut cb: F) -> Result<()>
where
    F: FnMut(Node) -> Result<()>,
{
    let mut cursor = ctx.root.walk();
    loop {
        cb(cursor.node())?;
        if !cursor.goto_first_child() {
            while !cursor.goto_next_sibling() {
                if !cursor.goto_parent() {
                    return Ok(());
                }
            }
        }
    }
}
</file>

<file path="src/config.rs">
// src/config.rs
use crate::error::Result;
use regex::Regex;
use serde::Deserialize;
use std::collections::HashMap;
use std::fs;
use std::path::Path;

#[derive(Debug, Clone, Deserialize)]
pub struct RuleConfig {
    #[serde(default = "default_max_tokens")]
    pub max_file_tokens: usize,
    #[serde(default = "default_max_complexity")]
    pub max_cyclomatic_complexity: usize,
    #[serde(default = "default_max_depth")]
    pub max_nesting_depth: usize,
    #[serde(default = "default_max_args")]
    pub max_function_args: usize,
    #[serde(default = "default_max_words")]
    pub max_function_words: usize,
    #[serde(default)]
    pub ignore_naming_on: Vec<String>,
}

impl Default for RuleConfig {
    fn default() -> Self {
        Self {
            max_file_tokens: default_max_tokens(),
            max_cyclomatic_complexity: default_max_complexity(),
            max_nesting_depth: default_max_depth(),
            max_function_args: default_max_args(),
            max_function_words: default_max_words(),
            ignore_naming_on: Vec::new(),
        }
    }
}

const fn default_max_tokens() -> usize {
    2000
}
const fn default_max_complexity() -> usize {
    10
}
const fn default_max_depth() -> usize {
    4
}
const fn default_max_args() -> usize {
    5
}
const fn default_max_words() -> usize {
    3
}

#[derive(Debug, Clone, Deserialize, Default)]
pub struct WardenToml {
    #[serde(default)]
    pub rules: RuleConfig,
    #[serde(default)]
    pub commands: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub enum GitMode {
    Auto,
    Yes,
    No,
}

#[derive(Debug, Clone)]
pub struct Config {
    pub git_mode: GitMode,
    pub include_patterns: Vec<Regex>,
    pub exclude_patterns: Vec<Regex>,
    pub code_only: bool,
    pub verbose: bool,
    pub rules: RuleConfig,
    pub commands: HashMap<String, String>,
}

impl Default for Config {
    fn default() -> Self {
        Self::new()
    }
}

impl Config {
    #[must_use]
    pub fn new() -> Self {
        Self {
            git_mode: GitMode::Auto,
            include_patterns: Vec::new(),
            exclude_patterns: Vec::new(),
            code_only: false,
            verbose: false,
            rules: RuleConfig::default(),
            commands: HashMap::new(),
        }
    }

    /// Validates config.
    /// # Errors
    /// Returns `Ok`.
    pub fn validate(&self) -> Result<()> {
        Ok(())
    }

    pub fn load_local_config(&mut self) {
        self.load_ignore_file();
        self.load_toml_config();
    }

    fn load_ignore_file(&mut self) {
        if let Ok(content) = fs::read_to_string(".wardenignore") {
            for line in content.lines() {
                self.process_ignore_line(line);
            }
        }
    }

    fn process_ignore_line(&mut self, line: &str) {
        let trimmed = line.trim();
        if trimmed.is_empty() || trimmed.starts_with('#') {
            return;
        }
        if let Ok(re) = Regex::new(trimmed) {
            self.exclude_patterns.push(re);
        }
    }

    fn load_toml_config(&mut self) {
        if Path::new("warden.toml").exists() {
            if let Ok(content) = fs::read_to_string("warden.toml") {
                self.parse_toml(&content);
            }
        }
    }

    fn parse_toml(&mut self, content: &str) {
        if let Ok(parsed) = toml::from_str::<WardenToml>(content) {
            self.rules = parsed.rules;
            self.commands = parsed.commands;
            if self.verbose {
                println!("ğŸ”§ Loaded warden.toml");
            }
        }
    }
}

pub const PRUNE_DIRS: &[&str] = &[
    ".git",
    ".svn",
    ".hg",
    "node_modules",
    "target",
    "dist",
    "build",
    "out",
    "gen",
    ".venv",
    "venv",
    ".tox",
    "__pycache__",
    "coverage",
    "vendor",
    "Cargo.lock",
    "package-lock.json",
    "pnpm-lock.yaml",
    "yarn.lock",
    "bun.lockb",
    "go.sum",
    "Gemfile.lock",
    "tests",
    "test",
    "spec",
    "docs",
    "examples",
    "fixtures",
];
pub const BIN_EXT_PATTERN: &str =
    r"(?i)\.(png|jpg|gif|svg|ico|webp|woff2?|ttf|pdf|mp4|zip|gz|tar|exe|dll|so|dylib|class|pyc)$";
pub const SECRET_PATTERN: &str =
    r"(?i)(^\.?env(\..*)?$|/\.?env(\..*)?$|(^|/)(id_rsa|id_ed25519|.*\.(pem|p12|key|pfx))$)";
pub const CODE_EXT_PATTERN: &str = r"(?i)\.(rs|go|py|js|jsx|ts|tsx|java|c|cpp|h|hpp|cs|php|rb|sh|sql|html|css|scss|json|toml|yaml|md)$";
pub const CODE_BARE_PATTERN: &str = r"(?i)(Makefile|Dockerfile|CMakeLists\.txt)$";
</file>

<file path="src/detection.rs">
// src/detection.rs
use crate::error::Result;
use std::collections::HashSet;
use std::fmt;
use std::path::Path;

#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy)]
pub enum BuildSystemType {
    Rust,
    Node,
    Python,
    Go,
    CMake,
    Conan,
}

impl fmt::Display for BuildSystemType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{self:?}")
    }
}

#[derive(Default)]
pub struct Detector;

impl Detector {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    /// Detects build systems.
    /// # Errors
    /// Returns `Ok`.
    pub fn detect_build_systems(
        &self,
        files: &[std::path::PathBuf],
    ) -> Result<Vec<BuildSystemType>> {
        let mut detected = HashSet::new();
        for file in files {
            check_file(file, &mut detected);
        }
        Ok(detected.into_iter().collect())
    }
}

fn check_file(path: &Path, set: &mut HashSet<BuildSystemType>) {
    if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
        if check_cmake(path, set) {
            return;
        }
        let _ = check_common(name, set);
    }
}

fn check_cmake(path: &Path, set: &mut HashSet<BuildSystemType>) -> bool {
    if path
        .extension()
        .is_some_and(|e| e.eq_ignore_ascii_case("cmake"))
    {
        set.insert(BuildSystemType::CMake);
        return true;
    }
    false
}

fn check_common(name: &str, set: &mut HashSet<BuildSystemType>) -> bool {
    match name {
        "Cargo.toml" => {
            set.insert(BuildSystemType::Rust);
            true
        }
        "package.json" => {
            set.insert(BuildSystemType::Node);
            true
        }
        "requirements.txt" | "pyproject.toml" | "Pipfile" => {
            set.insert(BuildSystemType::Python);
            true
        }
        "go.mod" => {
            set.insert(BuildSystemType::Go);
            true
        }
        "CMakeLists.txt" => {
            set.insert(BuildSystemType::CMake);
            true
        }
        "conanfile.txt" | "conanfile.py" => {
            set.insert(BuildSystemType::Conan);
            true
        }
        _ => false,
    }
}
</file>

<file path="src/enumerate.rs">
// warden:ignore
use crate::config::{Config, PRUNE_DIRS};
use crate::error::{Result, WardenError};
use std::path::PathBuf;
use std::process::Command;
use walkdir::WalkDir;

pub struct FileEnumerator {
    config: Config,
}

impl FileEnumerator {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Enumerates files based on configuration.
    ///
    /// # Errors
    ///
    /// Returns error if `git` fails in Git mode.
    pub fn enumerate(&self) -> Result<Vec<PathBuf>> {
        use crate::config::GitMode;

        match self.config.git_mode {
            GitMode::Yes => {
                if !Self::in_git_repo() {
                    return Err(WardenError::NotInGitRepo);
                }
                // Fixed: Self::filter_paths
                Ok(Self::filter_paths(Self::git_ls_files()?))
            }
            GitMode::No => Ok(self.walk_all_files()),
            GitMode::Auto => {
                if Self::in_git_repo() {
                    if let Ok(files) = Self::git_ls_files() {
                        // Fixed: Self::filter_paths
                        return Ok(Self::filter_paths(files));
                    }
                }
                Ok(self.walk_all_files())
            }
        }
    }

    // Fixed: Removed &self
    fn filter_paths(paths: Vec<PathBuf>) -> Vec<PathBuf> {
        paths
            .into_iter()
            .filter(|p| {
                for part in p.components() {
                    if let Some(s) = part.as_os_str().to_str() {
                        if PRUNE_DIRS.contains(&s) {
                            return false;
                        }
                    }
                }
                true
            })
            .collect()
    }

    fn in_git_repo() -> bool {
        let out = Command::new("git")
            .arg("rev-parse")
            .arg("--is-inside-work-tree")
            .output();

        matches!(out, Ok(o) if o.status.success())
    }

    fn git_ls_files() -> Result<Vec<PathBuf>> {
        let out = Command::new("git")
            .arg("ls-files")
            .arg("-z")
            .arg("--exclude-standard")
            .output()?;

        if !out.status.success() {
            return Err(WardenError::Other(format!(
                "git ls-files failed: exit {}",
                out.status
            )));
        }

        let mut paths = Vec::new();
        for chunk in out.stdout.split(|b| *b == 0) {
            if chunk.is_empty() {
                continue;
            }
            let s = String::from_utf8_lossy(chunk);
            paths.push(PathBuf::from(s.as_ref()));
        }
        Ok(paths)
    }

    fn walk_all_files(&self) -> Vec<PathBuf> {
        let mut paths = Vec::new();
        let mut errors = Vec::new();

        let walker = WalkDir::new(".").follow_links(false).into_iter();

        for item in walker.filter_entry(|e| {
            let name = e.file_name().to_string_lossy();
            // WalkDir filtering allows us to skip descending into "node_modules" entirely
            !PRUNE_DIRS.iter().any(|p| name == *p)
        }) {
            let entry = match item {
                Ok(e) => e,
                Err(e) => {
                    errors.push(format!("walkdir: {e}"));
                    continue;
                }
            };

            if entry.file_type().is_file() {
                let p = entry.path().strip_prefix(".").unwrap_or(entry.path());
                paths.push(p.to_path_buf());
            }
        }

        if !errors.is_empty() && self.config.verbose {
            eprintln!(
                "WARN: Encountered {} errors during file walk:",
                errors.len()
            );
            for (i, err) in errors.iter().take(5).enumerate() {
                eprintln!("  {}. {}", i + 1, err);
            }
        }

        paths
    }
}
</file>

<file path="src/error.rs">
// src/error.rs
use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum WardenError {
    #[error("I/O error: {source} (path: {path})")]
    Io {
        source: std::io::Error,
        path: PathBuf,
    },

    #[error("Not inside a Git repository")]
    NotInGitRepo,

    #[error("Regex error: {0}")]
    Regex(#[from] regex::Error),

    #[error("Generic error: {0}")]
    Other(String),
}

pub type Result<T> = std::result::Result<T, WardenError>;

// Allow `?` on std::io::Error by converting to WardenError::Io with unknown path.
impl From<std::io::Error> for WardenError {
    fn from(source: std::io::Error) -> Self {
        WardenError::Io {
            source,
            path: PathBuf::from("<unknown>"),
        }
    }
}

// Gracefully convert WalkDir errors
impl From<walkdir::Error> for WardenError {
    fn from(e: walkdir::Error) -> Self {
        WardenError::Other(e.to_string())
    }
}
</file>

<file path="src/filter.rs">
// src/filter.rs
use crate::config::{Config, BIN_EXT_PATTERN, CODE_BARE_PATTERN, CODE_EXT_PATTERN, SECRET_PATTERN};
use crate::error::Result;
use regex::Regex;
use std::path::Path;

pub struct FileFilter {
    config: Config,
    bin_ext_re: Regex,
    secret_re: Regex,
    code_ext_re: Option<Regex>,
    code_bare_re: Option<Regex>,
}

impl FileFilter {
    /// Creates a new filter.
    /// # Errors
    /// Returns error on invalid regex.
    pub fn new(config: &Config) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            bin_ext_re: Regex::new(BIN_EXT_PATTERN)?,
            secret_re: Regex::new(SECRET_PATTERN)?,
            code_ext_re: if config.code_only {
                Some(Regex::new(CODE_EXT_PATTERN)?)
            } else {
                None
            },
            code_bare_re: if config.code_only {
                Some(Regex::new(CODE_BARE_PATTERN)?)
            } else {
                None
            },
        })
    }

    #[must_use]
    pub fn filter(&self, files: Vec<std::path::PathBuf>) -> Vec<std::path::PathBuf> {
        files.into_iter().filter(|p| self.should_keep(p)).collect()
    }

    fn should_keep(&self, path: &Path) -> bool {
        let s = path.to_string_lossy().replace('\\', "/");
        if self.is_ignored(&s) {
            return false;
        }
        if self.config.code_only && !self.is_code(&s) {
            return false;
        }
        self.is_included(&s)
    }

    fn is_ignored(&self, path: &str) -> bool {
        if self.secret_re.is_match(path) {
            return true;
        }
        if self.bin_ext_re.is_match(path) {
            return true;
        }
        if self
            .config
            .exclude_patterns
            .iter()
            .any(|p| p.is_match(path))
        {
            return true;
        }
        false
    }

    fn is_included(&self, path: &str) -> bool {
        self.config.include_patterns.is_empty()
            || self
                .config
                .include_patterns
                .iter()
                .any(|p| p.is_match(path))
    }

    fn is_code(&self, path: &str) -> bool {
        match (&self.code_ext_re, &self.code_bare_re) {
            (Some(ext), Some(bare)) => ext.is_match(path) || bare.is_match(path),
            _ => true,
        }
    }
}
</file>

<file path="src/heuristics.rs">
// warden:ignore
use crate::config::{CODE_BARE_PATTERN, CODE_EXT_PATTERN};
use regex::Regex;
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::sync::LazyLock;

// --- Configuration Constants for Heuristics ---
const MIN_TEXT_ENTROPY: f64 = 3.5;
const MAX_TEXT_ENTROPY: f64 = 5.5;

const BUILD_SYSTEM_PAMPS: &[&str] = &[
    "find_package",
    "add_executable",
    "target_link_libraries",
    "cmake_minimum_required",
    "project(",
    "add-apt-repository",
    "conanfile.py",
    "dependency",
    "require",
    "include",
    "import",
    "version",
    "dependencies",
];

// Pre-compiled regexes for known code files
static CODE_EXT_RE: LazyLock<Regex> = LazyLock::new(|| Regex::new(CODE_EXT_PATTERN).unwrap());
static CODE_BARE_RE: LazyLock<Regex> = LazyLock::new(|| Regex::new(CODE_BARE_PATTERN).unwrap());

pub struct HeuristicFilter;

impl HeuristicFilter {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    #[must_use]
    pub fn filter(&self, files: Vec<std::path::PathBuf>) -> Vec<std::path::PathBuf> {
        files
            .into_iter()
            .filter(|path| Self::should_keep(path))
            .collect()
    }

    fn should_keep(path: &Path) -> bool {
        let path_str = path.to_string_lossy();

        if CODE_EXT_RE.is_match(&path_str) || CODE_BARE_RE.is_match(&path_str) {
            return true;
        }

        if let Ok(entropy) = calculate_entropy(path) {
            if !(MIN_TEXT_ENTROPY..=MAX_TEXT_ENTROPY).contains(&entropy) {
                return false;
            }
        } else {
            return false;
        }

        if let Ok(content) = fs::read_to_string(path) {
            let lower_content = content.to_lowercase();
            for pamp in BUILD_SYSTEM_PAMPS {
                if lower_content.contains(pamp) {
                    return true;
                }
            }
        }

        true
    }
}

impl Default for HeuristicFilter {
    fn default() -> Self {
        Self::new()
    }
}

fn calculate_entropy(path: &Path) -> std::io::Result<f64> {
    let bytes = fs::read(path)?;
    if bytes.is_empty() {
        return Ok(0.0);
    }

    let mut freq_map = HashMap::new();
    for &byte in &bytes {
        *freq_map.entry(byte).or_insert(0) += 1;
    }

    // Suppress cast precision loss for 64-bit length; entropy approximation is fine.
    #[allow(clippy::cast_precision_loss)]
    let len = bytes.len() as f64;

    let entropy = freq_map.values().fold(0.0, |acc, &count| {
        let probability = f64::from(count) / len;
        acc - probability * probability.log2()
    });

    Ok(entropy)
}
</file>

<file path="src/lib.rs">
// src/lib.rs
pub mod analysis;
pub mod checks;
pub mod clipboard;
pub mod config;
pub mod detection;
pub mod enumerate;
pub mod error;
pub mod filter;
pub mod heuristics;
pub mod metrics;
pub mod prompt;
pub mod reporting;
pub mod rules;
pub mod tokens;
pub mod tui;
pub mod types;
</file>

<file path="src/metrics.rs">
// warden:ignore
use tree_sitter::{Node, Query, QueryCursor};

/// Calculates the nesting depth of a node.
///
/// # Returns
/// The maximum depth of control structures within the node.
#[must_use]
pub fn calculate_max_depth(node: Node) -> usize {
    let mut max_depth = 0;
    let mut cursor = node.walk();

    // We start at 0 relative to function body
    for child in node.children(&mut cursor) {
        if child.kind().contains("block") || child.kind().contains("body") {
            max_depth = std::cmp::max(max_depth, walk_depth(child, 0));
        }
    }
    max_depth
}

fn walk_depth(node: Node, current: usize) -> usize {
    let mut max = current;
    let mut cursor = node.walk();

    for child in node.children(&mut cursor) {
        let kind = child.kind();
        // Uses matches! macro to reduce Cyclomatic Complexity score
        // (This replaces the massive if/else chain)
        if matches!(
            kind,
            "if_expression"
                | "match_expression"
                | "for_expression"
                | "while_expression"
                | "loop_expression"
                | "if_statement"
                | "for_statement"
                | "for_in_statement"
                | "while_statement"
                | "do_statement"
                | "switch_case"
                | "catch_clause"
                | "try_statement"
        ) {
            max = std::cmp::max(max, walk_depth(child, current + 1));
        } else {
            max = std::cmp::max(max, walk_depth(child, current));
        }
    }
    max
}

/// Calculates `McCabe` Cyclomatic Complexity.
#[must_use]
pub fn calculate_complexity(node: Node, source: &str, query: &Query) -> usize {
    let mut cursor = QueryCursor::new();
    // Base complexity is 1
    let mut complexity = 1;
    for _ in cursor.matches(query, node, source.as_bytes()) {
        complexity += 1;
    }
    complexity
}

/// Counts named arguments/parameters.
#[must_use]
pub fn count_arguments(node: Node) -> usize {
    let mut cursor = node.walk();
    for child in node.children(&mut cursor) {
        if child.kind().contains("parameter") || child.kind().contains("argument") {
            return child.named_child_count();
        }
    }
    0
}
</file>

<file path="src/prompt.rs">
// src/prompt.rs
use crate::config::RuleConfig;
use anyhow::Result;

pub struct PromptGenerator {
    config: RuleConfig,
}

impl PromptGenerator {
    #[must_use]
    pub fn new(config: RuleConfig) -> Self {
        Self { config }
    }

    /// Generates the full system prompt.
    ///
    /// # Errors
    /// Returns `Ok` on success.
    pub fn generate(&self) -> Result<String> {
        Ok(format!(
            r#"ğŸ›¡ï¸ SYSTEM MANDATE: THE WARDEN PROTOCOL
ROLE: High-Integrity Systems Architect (NASA/JPL Standard).
CONTEXT: You are coding inside a strict environment enforced by Warden, a structural linter based on NASA's "Power of 10" rules.

THE 3 LAWS (Non-Negotiable):

1. LAW OF ATOMICITY (Holzmann Rule 4)
   - Files: MUST be < {} tokens (~60-100 logical statements).
   - Rationale: Each file should be a logical unit, verifiable as a unit.
   - Action: Split immediately if larger. Separate VIEW (UI) from LOGIC (business logic).

2. LAW OF COMPLEXITY (Holzmann Rules 1 & 2)
   - Cyclomatic Complexity: MUST be â‰¤ {} per function.
   - Nesting Depth: MUST be â‰¤ {} levels.
   - Function Arguments: MUST be â‰¤ {} parameters.
   - Rationale: Simpler control flow = stronger analysis capabilities.
   - Action: Extract functions. Simplify branching. Use data structures over parameter lists.

3. LAW OF PARANOIA (Holzmann Rules 5, 7, & 10)
   - Error Handling: ALL functions return Result<T, E> or Option<T>. NO silent failures.
   - Banned Patterns: NO .unwrap() calls. NO .expect() in production code.
   - Validation: Check ALL inputs at entry point (Line 1 of function).
   - Rationale: Assertion density correlates with defect interception.

LANGUAGE SPECIFICS:
   - RUST: clippy::pedantic enforced. Use thiserror for errors.
   - TYPESCRIPT: Strict mode + @ts-check. NO 'any' type.
   - PYTHON: Type hints mandatory (def func(x: int) -> str).

OPERATIONAL PROTOCOL:
   1. Read: Understand the full context before generating code.
   2. Generate: Output COMPLETE, WHOLE files with proper headers.
   3. Verify: Ask "Does this violate the 3 Laws?" before submission.
   4. Iterate: If Warden rejects it, refactor and resubmit."#,
            self.config.max_file_tokens,
            self.config.max_cyclomatic_complexity,
            self.config.max_nesting_depth,
            self.config.max_function_args
        ))
    }

    /// Generates the short reminder text.
    ///
    /// # Errors
    /// Returns `Ok` on success.
    pub fn generate_reminder(&self) -> Result<String> {
        Ok(format!(
            r"
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›¡ï¸ REMINDER: WARDEN PROTOCOL CONSTRAINTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BEFORE SUBMITTING CODE, VERIFY:
â–¡ Files < {} tokens
â–¡ Cyclomatic complexity â‰¤ {} per function
â–¡ Nesting depth â‰¤ {} levels
â–¡ Function parameters â‰¤ {}
â–¡ No .unwrap() or .expect() calls
â–¡ All functions return Result<T, E> or Option<T>
â–¡ All inputs validated at function entry

If ANY constraint is violated, REFACTOR before submitting.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
            self.config.max_file_tokens,
            self.config.max_cyclomatic_complexity,
            self.config.max_nesting_depth,
            self.config.max_function_args
        ))
    }

    /// Wraps the prompt with the standard header.
    ///
    /// # Errors
    /// Returns `Ok` on success.
    pub fn wrap_header(&self) -> Result<String> {
        let body = self.generate()?;
        Ok(format!(
            r"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›¡ï¸ WARDEN PROTOCOL - AI SYSTEM PROMPT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{body}
"
        ))
    }
}
</file>

<file path="src/rules.rs">
use crate::analysis::Analyzer;
use crate::config::Config;
use crate::tokens::Tokenizer;
use crate::types::{FileReport, ScanReport, Violation};
use rayon::prelude::*;
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::LazyLock;
use std::time::Instant;

static ANALYZER: LazyLock<Analyzer> = LazyLock::new(Analyzer::new);

pub struct RuleEngine {
    config: Config,
}

impl RuleEngine {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Scans a list of files and returns a structured report.
    ///
    /// # Errors
    ///
    /// Returns error if Rayon thread pool fails (unlikely).
    #[must_use]
    pub fn scan(&self, files: Vec<PathBuf>) -> ScanReport {
        let start = Instant::now();

        // Parallel scan
        let results: Vec<FileReport> = files
            .into_par_iter()
            .filter_map(|path| self.analyze_file(&path))
            .collect();

        let total_tokens = results.iter().map(|f| f.token_count).sum();
        let total_violations = results.iter().map(|f| f.violations.len()).sum();

        ScanReport {
            files: results,
            total_tokens,
            total_violations,
            duration_ms: start.elapsed().as_millis(),
        }
    }

    fn analyze_file(&self, path: &Path) -> Option<FileReport> {
        let content = fs::read_to_string(path).ok()?;

        if content.contains("// warden:ignore") || content.contains("# warden:ignore") {
            return None;
        }

        let filename = path.to_string_lossy();
        let mut violations = Vec::new();
        let token_count = Tokenizer::count(&content);

        // 1. Law of Atomicity
        if token_count > self.config.rules.max_file_tokens {
            violations.push(Violation {
                row: 0,
                message: format!(
                    "File size is {token_count} tokens (Limit: {})",
                    self.config.rules.max_file_tokens
                ),
                law: "LAW OF ATOMICITY",
            });
        }

        // 2. AST Analysis
        if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
            let mut analysis_violations =
                ANALYZER.analyze(ext, &filename, &content, &self.config.rules);
            violations.append(&mut analysis_violations);
        }

        Some(FileReport {
            path: path.to_path_buf(),
            token_count,
            complexity_score: 0, // Future: aggregate function complexity here
            violations,
        })
    }
}
</file>

<file path="src/tokens.rs">
use std::sync::LazyLock;
use tiktoken_rs::CoreBPE;

// We use cl100k_base (GPT-4/3.5 turbo encoding) as the standard
static BPE: LazyLock<CoreBPE> =
    LazyLock::new(|| tiktoken_rs::cl100k_base().expect("Failed to load cl100k_base dictionary"));

pub struct Tokenizer;

impl Tokenizer {
    #[must_use]
    pub fn count(text: &str) -> usize {
        // EncodeOrdinary is faster as it ignores special tokens, which is fine for code
        BPE.encode_ordinary(text).len()
    }

    /// Returns true if the file exceeds the token limit
    #[must_use]
    pub fn exceeds_limit(text: &str, limit: usize) -> bool {
        Self::count(text) > limit
    }
}
</file>

<file path="src/tui/mod.rs">
pub mod state;
pub mod view;
</file>

<file path="src/tui/state.rs">
// src/tui/state.rs
use crate::types::{FileReport, ScanReport};
use anyhow::Result;
use crossterm::event::{self, Event, KeyCode};
use std::time::Duration;

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum SortMode {
    Path,
    Tokens,
    Violations,
}

pub struct App {
    pub report: ScanReport,
    pub view_indices: Vec<usize>,
    pub selected_index: usize,
    pub running: bool,
    pub sort_mode: SortMode,
    pub only_violations: bool,
}

impl App {
    #[must_use]
    pub fn new(report: ScanReport) -> Self {
        let mut app = Self {
            report,
            view_indices: Vec::new(),
            selected_index: 0,
            running: true,
            sort_mode: SortMode::Path,
            only_violations: false,
        };
        app.update_view();
        app
    }

    fn update_view(&mut self) {
        let mut indices: Vec<usize> = self
            .report
            .files
            .iter()
            .enumerate()
            .filter(|(_, f)| !self.only_violations || !f.is_clean())
            .map(|(i, _)| i)
            .collect();

        self.sort_indices(&mut indices);
        self.view_indices = indices;
        self.clamp_selection();
    }

    fn sort_indices(&self, indices: &mut [usize]) {
        let files = &self.report.files;
        indices.sort_by(|&a, &b| {
            let f1 = &files[a];
            let f2 = &files[b];
            match self.sort_mode {
                SortMode::Path => f1.path.cmp(&f2.path),
                SortMode::Tokens => f2.token_count.cmp(&f1.token_count),
                SortMode::Violations => f2.violations.len().cmp(&f1.violations.len()),
            }
        });
    }

    fn clamp_selection(&mut self) {
        if self.view_indices.is_empty() {
            self.selected_index = 0;
        } else if self.selected_index >= self.view_indices.len() {
            self.selected_index = self.view_indices.len() - 1;
        }
    }

    /// Runs TUI loop.
    /// # Errors
    /// Returns error on IO failure.
    pub fn run<B: ratatui::backend::Backend>(
        &mut self,
        terminal: &mut ratatui::Terminal<B>,
    ) -> Result<()> {
        while self.running {
            terminal.draw(|f| crate::tui::view::draw(f, self).unwrap_or(()))?;

            if event::poll(Duration::from_millis(100))? {
                if let Event::Key(key) = event::read()? {
                    self.handle_input(key.code);
                }
            }
        }
        Ok(())
    }

    fn handle_input(&mut self, code: KeyCode) {
        match code {
            KeyCode::Char('q') | KeyCode::Esc => self.running = false,
            KeyCode::Up | KeyCode::Char('k') => self.move_up(),
            KeyCode::Down | KeyCode::Char('j') => self.move_down(),
            KeyCode::Char('s') => self.cycle_sort(),
            KeyCode::Char('f') => self.toggle_filter(),
            _ => {}
        }
    }

    fn move_up(&mut self) {
        if self.selected_index > 0 {
            self.selected_index -= 1;
        }
    }

    fn move_down(&mut self) {
        if !self.view_indices.is_empty() && self.selected_index < self.view_indices.len() - 1 {
            self.selected_index += 1;
        }
    }

    fn cycle_sort(&mut self) {
        self.sort_mode = match self.sort_mode {
            SortMode::Path => SortMode::Tokens,
            SortMode::Tokens => SortMode::Violations,
            SortMode::Violations => SortMode::Path,
        };
        self.update_view();
    }

    fn toggle_filter(&mut self) {
        self.only_violations = !self.only_violations;
        self.update_view();
    }

    #[must_use]
    pub fn get_selected_file(&self) -> Option<&FileReport> {
        if let Some(&real_index) = self.view_indices.get(self.selected_index) {
            self.report.files.get(real_index)
        } else {
            None
        }
    }
}
</file>

<file path="src/tui/view/components.rs">
// src/tui/view/components.rs
use crate::tui::state::App;
use crate::types::FileReport;
use anyhow::Result;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Gauge, List, ListItem, Paragraph};
use ratatui::Frame;

/// Draws file list.
/// # Errors
/// Returns `Ok`.
pub fn draw_file_list(f: &mut Frame, app: &App, area: Rect) -> Result<()> {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" ğŸ“‚ File List ");
    let items = build_list_items(app);

    let list = List::new(items).block(block).highlight_style(
        Style::default()
            .bg(Color::DarkGray)
            .add_modifier(Modifier::BOLD),
    );

    let mut state = ratatui::widgets::ListState::default();
    state.select(Some(app.selected_index));
    f.render_stateful_widget(list, area, &mut state);
    Ok(())
}

fn build_list_items(app: &App) -> Vec<ListItem<'_>> {
    app.view_indices
        .iter()
        .map(|&idx| {
            let file = &app.report.files[idx];
            create_list_item(file)
        })
        .collect()
}

fn create_list_item(file: &FileReport) -> ListItem<'_> {
    let name = file.path.to_string_lossy();
    let is_clean = file.is_clean();
    let (color, icon) = if !is_clean {
        (Color::Red, "!")
    } else if file.token_count > 1000 {
        (Color::Yellow, "âœ“")
    } else {
        (Color::Green, "âœ“")
    };

    let bars = (file.token_count / 200).clamp(0, 10);
    let bar_vis = "I".repeat(bars);

    let content = Line::from(vec![
        Span::styled(
            format!("{icon} "),
            Style::default().fg(color).add_modifier(Modifier::BOLD),
        ),
        Span::raw(format!("{name:<30} ")),
        Span::styled(
            format!("{bar_vis:<10}"),
            Style::default().fg(Color::DarkGray),
        ),
    ]);
    ListItem::new(content)
}

/// Draws inspector.
/// # Errors
/// Returns `Ok`.
#[allow(clippy::cast_precision_loss)]
pub fn draw_inspector(f: &mut Frame, app: &App, area: Rect) -> Result<()> {
    let block = Block::default()
        .borders(Borders::ALL)
        .title(" ğŸ•µï¸ Inspector ");
    let inner = block.inner(area);
    f.render_widget(block, area);

    if let Some(file) = app.get_selected_file() {
        let layout = Layout::default()
            .direction(Direction::Vertical)
            .constraints(
                [
                    Constraint::Length(2),
                    Constraint::Length(6),
                    Constraint::Min(5),
                ]
                .as_ref(),
            )
            .split(inner);

        draw_header(f, file, layout[0]);
        draw_stats(f, file, layout[1]);
        draw_issues(f, file, layout[2]);
    } else {
        f.render_widget(
            Paragraph::new("No file selected").alignment(Alignment::Center),
            inner,
        );
    }
    Ok(())
}

fn draw_header(f: &mut Frame, file: &FileReport, area: Rect) {
    let header = Paragraph::new(Line::from(vec![
        Span::styled("TARGET: ", Style::default().fg(Color::DarkGray)),
        Span::styled(
            file.path.to_string_lossy(),
            Style::default().add_modifier(Modifier::BOLD),
        ),
    ]));
    f.render_widget(header, area);
}

#[allow(clippy::cast_precision_loss)]
fn draw_stats(f: &mut Frame, file: &FileReport, area: Rect) {
    let chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(50), Constraint::Percentage(50)].as_ref())
        .split(area);

    let t_ratio = (file.token_count as f64 / 2000.0).clamp(0.0, 1.0);
    let t_gauge = Gauge::default()
        .block(Block::default().borders(Borders::NONE).title("Size"))
        .gauge_style(Style::default().fg(if t_ratio > 0.8 {
            Color::Red
        } else {
            Color::Green
        }))
        .ratio(t_ratio)
        .label(format!("{} toks", file.token_count));
    f.render_widget(t_gauge, chunks[0]);

    let v_count = file.violations.len();
    let v_ratio = (v_count as f64 / 5.0).clamp(0.0, 1.0);
    let v_gauge = Gauge::default()
        .block(Block::default().borders(Borders::NONE).title("Issues"))
        .gauge_style(Style::default().fg(if v_count > 0 {
            Color::Red
        } else {
            Color::Green
        }))
        .ratio(v_ratio)
        .label(format!("{v_count} Found"));
    f.render_widget(v_gauge, chunks[1]);
}

fn draw_issues(f: &mut Frame, file: &FileReport, area: Rect) {
    if file.is_clean() {
        let p = Paragraph::new("âœ¨ Clean.")
            .style(Style::default().fg(Color::Green))
            .alignment(Alignment::Center);
        f.render_widget(p, area);
        return;
    }

    let items: Vec<ListItem> = file
        .violations
        .iter()
        .map(|v| {
            let header = Line::from(vec![
                Span::styled(
                    format!("[{}] ", v.law),
                    Style::default().fg(Color::Red).add_modifier(Modifier::BOLD),
                ),
                Span::raw(format!("Line {}", v.row + 1)),
            ]);
            let msg = Line::from(Span::styled(
                format!("  â””â”€ {}", v.message),
                Style::default().fg(Color::White),
            ));
            ListItem::new(vec![header, msg, Line::from("")])
        })
        .collect();

    let list = List::new(items).block(Block::default().borders(Borders::TOP).title(" Violations "));
    f.render_widget(list, area);
}
</file>

<file path="src/tui/view/layout.rs">
// src/tui/view/layout.rs
use crate::tui::state::{App, SortMode};
use crate::tui::view::components;
use anyhow::Result;
use ratatui::layout::{Alignment, Constraint, Direction, Layout, Rect};
use ratatui::style::{Color, Modifier, Style};
use ratatui::text::{Line, Span};
use ratatui::widgets::{Block, Borders, Paragraph};
use ratatui::Frame;

/// Renders dashboard.
/// # Errors
/// Returns `Ok`.
pub fn render_dashboard(f: &mut Frame, app: &App, area: Rect) -> Result<()> {
    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints(
            [
                Constraint::Length(3),
                Constraint::Min(10),
                Constraint::Length(1),
            ]
            .as_ref(),
        )
        .split(area);

    draw_header(f, app, chunks[0]);
    draw_main(f, app, chunks[1])?;
    draw_footer(f, chunks[2]);
    Ok(())
}

#[allow(clippy::cast_precision_loss)]
fn draw_header(f: &mut Frame, app: &App, area: Rect) {
    let (clean_count, total) = count_stats(app);
    let health = if total > 0 {
        (clean_count as f64 / total as f64) * 100.0
    } else {
        100.0
    };
    let health_color = get_health_color(health);

    let info = build_info_string(app, total);
    let line = build_header_line(health, health_color, &info);

    f.render_widget(
        Paragraph::new(line)
            .block(Block::default().borders(Borders::ALL))
            .alignment(Alignment::Center),
        area,
    );
}

fn count_stats(app: &App) -> (usize, usize) {
    (
        app.report.files.iter().filter(|f| f.is_clean()).count(),
        app.report.files.len(),
    )
}

fn get_health_color(health: f64) -> Color {
    if health > 90.0 {
        Color::Green
    } else if health > 70.0 {
        Color::Yellow
    } else {
        Color::Red
    }
}

fn build_info_string(app: &App, total: usize) -> String {
    let sort_str = match app.sort_mode {
        SortMode::Path => "NAME",
        SortMode::Tokens => "SIZE",
        SortMode::Violations => "ERRORS",
    };
    let filter_str = if app.only_violations {
        " | FILTER: ERRORS"
    } else {
        ""
    };
    format!(" FILES: {total} | SORT: {sort_str}{filter_str} ")
}

fn build_header_line(health: f64, color: Color, info: &str) -> Line<'_> {
    Line::from(vec![
        Span::styled(
            " ğŸ›¡ï¸ WARDEN PROTOCOL ",
            Style::default()
                .fg(Color::Cyan)
                .add_modifier(Modifier::BOLD),
        ),
        Span::raw(" | "),
        Span::styled(format!("HEALTH: {health:.1}%"), Style::default().fg(color)),
        Span::raw(" |"),
        Span::raw(info),
    ])
}

fn draw_main(f: &mut Frame, app: &App, area: Rect) -> Result<()> {
    let chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([Constraint::Percentage(40), Constraint::Percentage(60)].as_ref())
        .split(area);

    components::draw_file_list(f, app, chunks[0])?;
    components::draw_inspector(f, app, chunks[1])?;
    Ok(())
}

fn draw_footer(f: &mut Frame, area: Rect) {
    let text = " [s] Sort Mode | [f] Filter Errors | [j/k] Navigate | [q] Quit ";
    f.render_widget(
        Paragraph::new(text).style(Style::default().fg(Color::DarkGray).bg(Color::Black)),
        area,
    );
}
</file>

<file path="src/tui/view/mod.rs">
// src/tui/view/mod.rs
pub mod components;
pub mod layout;

use crate::tui::state::App;
use anyhow::Result;
use ratatui::Frame;

/// Draws the application UI.
///
/// # Errors
/// Returns `Ok` on success.
pub fn draw(f: &mut Frame, app: &App) -> Result<()> {
    let area = f.area();
    layout::render_dashboard(f, app, area)?;
    Ok(())
}
</file>

<file path="src/types.rs">
use std::path::PathBuf;

#[derive(Debug, Clone)]
pub struct Violation {
    pub row: usize,
    pub message: String,
    pub law: &'static str,
}

#[derive(Debug, Clone)]
pub struct FileReport {
    pub path: PathBuf,
    pub token_count: usize,
    pub complexity_score: usize,
    pub violations: Vec<Violation>,
}

impl FileReport {
    #[must_use]
    pub fn is_clean(&self) -> bool {
        self.violations.is_empty()
    }
}

#[derive(Debug, Clone, Default)]
pub struct ScanReport {
    pub files: Vec<FileReport>,
    pub total_tokens: usize,
    pub total_violations: usize,
    pub duration_ms: u128,
}
</file>

<file path="warden.toml">
# warden.toml
[rules]
max_file_tokens = 2000
max_cyclomatic_complexity = 4
max_nesting_depth = 2
max_function_args = 5
max_function_words = 5
ignore_naming_on = ["tests", "spec"]

[commands]
check = "cargo clippy --all-targets -- -D warnings -D clippy::pedantic"
</file>


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
END CODEBASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ›¡ï¸ REMINDER: WARDEN PROTOCOL CONSTRAINTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BEFORE SUBMITTING CODE, VERIFY:
â–¡ Files < 2000 tokens
â–¡ Cyclomatic complexity â‰¤ 4 per function
â–¡ Nesting depth â‰¤ 2 levels
â–¡ Function parameters â‰¤ 5
â–¡ No .unwrap() or .expect() calls
â–¡ All functions return Result<T, E> or Option<T>
â–¡ All inputs validated at function entry

If ANY constraint is violated, REFACTOR before submitting.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
